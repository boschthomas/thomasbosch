% This is LLNCS.DEM the demonstration file of
% the LaTeX macro package from Springer-Verlag
% for Lecture Notes in Computer Science,
% version 2.4 for LaTeX2e as of 16. April 2010
%
\documentclass{llncs}

% allows for temporary adjustment of side margins
\usepackage{chngpage}

% just makes the table prettier (see \toprule, \bottomrule, etc. commands below)
\usepackage{booktabs}

\usepackage[utf8]{inputenc}
%\usepackage[font=small,skip=0pt]{caption}

% footnotes
\usepackage{scrextend}

% colors
\usepackage[usenames, dvipsnames]{color}

% underline
\usepackage{tikz}
\newcommand{\udensdot}[1]{%
    \tikz[baseline=(todotted.base)]{
        \node[inner sep=1pt,outer sep=0pt] (todotted) {#1};
        \draw[densely dotted] (todotted.south west) -- (todotted.south east);
    }%
}%

\newcommand{\uloosdot}[1]{%
    \tikz[baseline=(todotted.base)]{
        \node[inner sep=1pt,outer sep=0pt] (todotted) {#1};
        \draw[loosely dotted] (todotted.south west) -- (todotted.south east);
    }%
}%

\newcommand{\udash}[1]{%
    \tikz[baseline=(todotted.base)]{
        \node[inner sep=1pt,outer sep=0pt] (todotted) {#1};
        \draw[dashed] (todotted.south west) -- (todotted.south east);
    }%
}%

\newcommand{\udensdash}[1]{%
    \tikz[baseline=(todotted.base)]{
        \node[inner sep=1pt,outer sep=0pt] (todotted) {#1};
        \draw[densely dashed] (todotted.south west) -- (todotted.south east);
    }%
}%

\newcommand{\uloosdash}[1]{%
    \tikz[baseline=(todotted.base)]{
        \node[inner sep=1pt,outer sep=0pt] (todotted) {#1};
        \draw[loosely dashed] (todotted.south west) -- (todotted.south east);
    }%
}%

% URL handling
\usepackage{url}
\urlstyle{same}

%\usepackage{makeidx}  % allows for indexgeneration

%\usepackage{amsmath}
\usepackage{amsmath, amssymb}
\usepackage{mathabx}
\usepackage{caption} 
\captionsetup[table]{skip=10pt}

% monospace within text
\newcommand{\ms}[1]{\texttt{#1}}

% examples
\usepackage{fancyvrb}
\DefineVerbatimEnvironment{ex}{Verbatim}{numbers=left,numbersep=2mm,frame=single,fontsize=\scriptsize}

\usepackage{xspace}
% Einfache und doppelte Anfuehrungszeichen
\newcommand{\qs}{``} 
\newcommand{\qe}{''\xspace} 
\newcommand{\sqs}{`} 
\newcommand{\sqe}{'\xspace} 

% checkmark
\usepackage{tikz}
\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;} 

% Xs
\usepackage{pifont}

% Tabellenabstände kleiner
\setlength{\intextsep}{10pt} % Vertical space above & below [h] floats
\setlength{\textfloatsep}{10pt} % Vertical space below (above) [t] ([b]) floats
% \setlength{\abovecaptionskip}{0pt}
% \setlength{\belowcaptionskip}{0pt}

\usepackage{tabularx}
\newcommand{\hr}{\hline\noalign{\smallskip}} % für die horizontalen linien in tabellen

% Todos
\usepackage[colorinlistoftodos]{todonotes}
\newcommand{\ke}[1]{\todo[size=\small, color=red!40]{\textbf{Kai:} #1}}
\newcommand{\tb}[1]{\todo[size=\small, color=green!40]{\textbf{Thomas:} #1}}
\newcommand{\er}[1]{\todo[size=\small, color=blue!40]{\textbf{Erman:} #1}}

\newenvironment{table-1cols}{
  \scriptsize
  \sffamily
  \vspace{0.3cm}
  \begin{tabular}{l}
  \hline
  \textbf{Requirements} \\
  \hline

}{
  \hline
  \end{tabular}
  \linebreak
}

\newenvironment{table-2cols}{
  \scriptsize
  \sffamily
  \vspace{0.3cm}
  \begin{tabular}{l|l}
  \hline
  \textbf{Requirements} & \textbf{Covering DSCLs} \\
  \hline

}{
  \hline
  \end{tabular}
  \linebreak
}

\newenvironment{complexity}{
  %\scriptsize
  %\sffamily
  %\vspace{0.3cm}
  \begin{tabular}{l|l}
  \hline
  \textbf{Complexity Class} & \textbf{Complexity} \\
  \hline

}{
  \hline
  \end{tabular}
  \linebreak
}

\newenvironment{DL}{
  %\scriptsize
  %\sffamily
  \small
  \vspace{0cm}
	\begin{center}
  \begin{tabular}{c l}

}{
  \end{tabular}
	\end{center}
}


\newenvironment{evaluation}{
  %\scriptsize
  %\sffamily
  %\vspace{0.3cm}
  \begin{tabular}{l|c|c|c|c|c|c}
  \hline
  \textbf{Constraint Class} & \textbf{DSP} & \textbf{OWL2-DL} & \textbf{OWL2-QL} & \textbf{ReSh} & \textbf{ShEx} & \textbf{SPIN} \\
  \hline

}{
  \hline
  \end{tabular}
  \linebreak
}

\newenvironment{constraint-languages-complexity}{
  %\scriptsize
  %\sffamily
  %\vspace{0.3cm}
  \begin{tabular}{l|c|c|c|c|c|c}
  \hline
  \textbf{Complexity Class} & \textbf{DSP} & \textbf{OWL2-DL} & \textbf{OWL2-QL} & \textbf{ReSh} & \textbf{ShEx} & \textbf{SPIN} \\
  \hline

}{
  \hline
  \end{tabular}
  \linebreak
}

\newenvironment{user-fiendliness}{
  %\scriptsize
  %\sffamily
  %\vspace{0.3cm}
  \begin{tabular}{l|c|c|c|c|c}
  \hline
  \textbf{criterion} & \textbf{DSP} & \textbf{OWL2} & \textbf{ReSh} & \textbf{ShEx} & \textbf{SPIN} \\
  \hline

}{
  \hline
  \end{tabular}
  \linebreak
}

\setcounter{secnumdepth}{5}

% tables
\usepackage{array,graphicx}
\usepackage{booktabs}
\usepackage{pifont}
\newcommand*\rot{\rotatebox{90}}
\newcommand*\OK{\ding{51}}
\usepackage{booktabs}
\newcommand*\ON[0]{$\surd$}

\usepackage{tablefootnote}

\usepackage{float}

\usepackage{ntheorem}
\newtheorem{hyp}{Hypothesis}

\makeatletter
\newcounter{subhyp} 
\let\savedc@hyp\c@hyp
\newenvironment{subhyp}
 {%
  \setcounter{subhyp}{0}%
  \stepcounter{hyp}%
  \edef\saved@hyp{\thehyp}% Save the current value of hyp
  \let\c@hyp\c@subhyp     % Now hyp is subhyp
  \renewcommand{\thehyp}{\saved@hyp\alph{hyp}}%
 }
 {}
\newcommand{\normhyp}{%
  \let\c@hyp\savedc@hyp % revert to the old one
  \renewcommand\thehyp{\arabic{hyp}}%
} 
\makeatother

\usepackage{multirow}

\begin{document}
\renewcommand{\arraystretch}{1.3}
\title{Aspects of RDF Data Constraints in the Social, Behavioural and Economic Sciences}
\subtitle{}

\titlerunning{XXXXX}  % abbreviated title (for running head)
%                                     also used for the TOC unless
%                                     \toctitle is used
%
\author{Thomas Bosch\inst{1} \and Benjamin Zapilko\inst{1} \and Joachim Wackerow\inst{1} \and Kai Eckert\inst{2}}
%
\authorrunning{} % abbreviated author list (for running head)
%
%%%% list of authors for the TOC (use if author list has to be modified)
\institute{GESIS – Leibniz Institute for the Social Sciences, Germany\\
\email{\{firstname.lastname\}@gesis.org},\\ 
\and
Stuttgart Media University, Germany \\
\email{eckert@hdm-stuttgart.de} 
}

\maketitle              % typeset the title of the contribution

\begin{abstract}

For research institutes, data libraries, and data archives,
RDF data validation according to predefined constraints is a much sought-after feature, 
particularly as this is taken for granted in the XML world.
Based on our work in the DCMI RDF Application Profiles Task Group and in cooperation with the W3C Data Shapes Working Group, we identified and published by today 82 types of constraints that are required by various stakeholders for data applications.
In this paper, we formulate 246 constraints of 53 different types on six different vocabularies (Disco, QB, SKOS, PHDD, DCAT, XKOS) and classify them according to the complexity level of their type and their severity level. For 114 of these constraints, we evaluate the data quality of 15,694 data sets (4,26 billion triples) of research data for the social, behavioural, and economic (SBE) sciences obtained from 33 SPARQL endpoints. \ke{check numbers}
Based on the results, we formulate several hypotheses to direct the further development of constraint langugaes.


\keywords{Constraint Validation, Data Quality, RDF}
\end{abstract}

\section{Introduction}

%%% Findings aus dem Abstract
%%% (1) for well-established vocabularies, the explicitly defined constraints are almost completely satisfied which demonstrates that constraint formulation in general works. (2) A significant amount of 47\% of the violations refer to complex constraints that are not easily expressible in existing languages which confirms the necessity to provide suitable constraint languages.  (3) The percentage of severe constraint violations is very low, compared to about 2/3 of warning violations and 1/3 of informational violations, which implies that proper constraint languages can significantly improve the data quality beyond fundamental requirements.

The social, behavioural, and economic sciences (SBE) require high-quality data for their empirical research. For more than a decade, members of the \emph{SBE} community have been developing and using a
metadata standard, composed of almost twelve hundred metadata fields, known as the \emph{Data Documentation Initiative (DDI)},
an XML format to disseminate, manage,
and reuse data collected and archived for research \cite{Vardigan2008}. 
%DDI is heavily used by the CESSDA community of European national data archives, 
%the International Household Survey Network community (made up of more than 90 statistical agencies),
%and ICPSR - the largest SBE data archive in the US.

In XML, the definition of schemas containing data constraints and the validation of data according to these constraints is commonly used to ensure a certain level of data quality.

With the rise of the Web of Data, data professionals and institutions are very interested in having their data be discovered and used by publishing their data directly in RDF or at least publish accurate metadata about their data to facilitate data integration. Therefore, not only established vocabularies like SKOS are used; 
recently, members of the \emph{SBE} and Linked Data community developed with the \emph{DDI-RDF Discovery Vocabulary (Disco)}\footnote{\url{http://rdf-vocabulary.ddialliance.org/discovery.html}} a means to expose \emph{DDI} metadata as Linked Data. 

For constraint formulation and validation on RDF data, several languages exist or are currently developed, like \emph{Shape Expressions}, \emph{Resource Shapes} or \emph{Description Set Profiles}. \emph{OWL 2} is also used as a constraint language under a closed world assumption. With its direct support of validation via SPARQL, \emph{SPIN}%\footnote{\url{http://spinRDF.org/}}
is very popular and certainly plays an important role for future developments in this field. It is particularly interesting as a means to validate arbitrary constraint languages by mapping them to SPARQL \cite{BoschEckert2014-2}. Yet, there is no clear favorite and none of the languages is able to meet all requirements raised by data practitioners. Further research and development therefore is needed.

In 2013, the W3C organized the RDF Validation Workshop\footnote{\url{http://www.w3.org/2012/12/rdf-val/}}, 
where experts from industry, government, and academia discussed first use cases for RDF constraint formulation and RDF data validation.
In 2014, two working groups on RDF validation have been established to develop a language to express constraints on RDF data: 
the W3C RDF Data Shapes working group\footnote{\url{http://www.w3.org/2014/rds/charter}} and the DCMI RDF Application Profiles task group\footnote{\url{http://wiki.dublincore.org/index.php/RDF-Application-Profiles}} which among others bundles the requirements of data institutions of the cultural heritage and SBE sector and represents them in the W3C group. 

Within the DCMI task group, a collaboratively curated database of RDF validation requirements has been created which contains the findings of the working groups based on various case studies provided by data institutions \cite{BoschEckert2014}. It is publicly available and open for further contributions\footnote{Online at \url{http://purl.org/net/rdf-validation}}.

The database connects requirements to use cases, case studies and implementations and forms the basis of this paper. To gain a better understanding about the role of certain requirements for data quality and in order to direct the further development of constraint langugaes, we collected constraints for commonly used vocabularies in the SBE domain, either from the vocabularies themselves or from domain and data experts. All in all, this lead to TODO constraints on TODO vocabularies. We let the experts classify the constraints according to the severity if they are violated. Furthermore, we classified the type of each constraint (corresponding to a requirement) based on its complexity ranging from types commonly found in vocabulary specificatons (e.g., domain and range), over types that are simply stated using common constraint languages (e.g., cardinality restrictions) to complex types that involve complex data structures or need more sophisticated langugaes to be easily expressible (e.g., constraints on network topologies).\ke{Thomas: Ok so?}

As we do not want to base our conclusions on the evaluations of vocabularies and constraint definitions alone, we conducted a large-scale experiment and evaluated the data quality of 15,694 data sets (4,26 billion triples) of research data for the social, behavioural, and economic (SBE) sciences obtained from 33 SPARQL endpoints.

The \textbf{contribution} of this paper is the development of a system 
\begin{enumerate}
	\item to classify RDF constraints (according to their severity levels) to evaluate the quality of metadata and data which may be represented by any vocabulary and 
	\item to classify RDF constraint types (according to their complexity) which in most cases correspond to RDF validation requirements\footnote{For simplicity reasons, we use the terms \emph{constraint types} and \emph{constraints} instead of \emph{RDF constraint types} and \emph{RDF constraints} in the rest of the paper} (sections \ref{classification} - \ref{complex-constraint-types}).
\end{enumerate}
By defining a huge amount of constraints of the majority of the constraint types,   
we apply the developed classification system to several and different vocabularies from the \emph{SBE} domain to represent both metadata and data and therefore prove its generality.
A complex and complete real world running example from the \emph{SBE} domain serves to prove the claim that the developed classification system perfectly applies for diverse vocabularies.
%, i.e., that it does not matter which vocabularies are used to represent (meta)data and for which domains constraints are defined.
We describe why RDF validation is important for the \emph{SBE} community (section \ref{motivation}), 
how data in tabular format and metadata on person-level data sets, aggregated data sets, and thesauri are represented in RDF, 
how therefore reused vocabularies are interrelated (section \ref{rdf-representation}),
and how \emph{SBE} (meta)data is validated against constraints of different constraint types (sections \ref{classification} - \ref{complex-constraint-types}).
We evaluated the (meta)data quality of large real world data sets (more than 4.2 billion triples and 15 thousand data sets) from the \emph{SBE} domain represented by multiple vocabularies to get an understanding 
(1) which sets of constraint types (on different levels of complexity) and 
(2) which sets of constraints (associated with particular severity levels) encompass the constraints causing the most/fewest constraint violations (see section \ref{evaluation}).

In this paper, we discuss constraints on RDF data in general. Note that the data represented in RDF can be data in the sense of SBE sciences, but also metadata about published or unpublished data. We generally refer to both simply as RDF data and only distinguish between data and metadata in the dataset descriptions and in the case that it matters for the purpose of this paper.\tb{hab ich eingefügt, ist der absatz von dir...}

%We propose an extensible metric to measure the continuum of severity levels to indicate how serious the violation of given constraints is.
%Constraints are instantiated from constraint types in order to validate both metadata and data represented by any vocabulary. 
%As constraint types are used to define constraints on (meta)data expressed by any vocabulary, the proposed constraint classification can be applied generically, i.e. vocabulary-independent. 

\ke{in guter tradition unserer arbeitsgruppe würde ich mich als senior researcher um die einleitung kümmern, die gefällt mir so nicht}\tb{sehr gerne}

\section{Motivation}
\label{motivation}

The data most often used in research within the \emph{SBE} community is \emph{person-level data} (or more generally \emph{record-unit data}, i.e., data collected about individuals, businesses, and households) in form of responses to studies or taken from administrative registers
(such as hospital records, registers of births and deaths). 
The range of person-level data is very broad - 
including census, education, health data and business, social, and labor force surveys.  
This type of research data is
held within data archives or data libraries after it has been collected, so that it may be
reused by future researchers. 

By its nature, person-level data is highly confidential and access is often only permitted for qualified researchers who must apply for access. 
Researchers typically represent their results as aggregated data in form of multi-dimensional tables with only a few columns; so-called \emph{variables} such as \emph{sex} or \emph{age}.
%In performing their research, the detailed person-level
%data is aggregated into multi-dimensional tables which answer particular research questions.
Aggregated data, which answers particular research questions, is derived from person-level data by statistics on groups or aggregates such as frequencies and arithmetic means.
%The SDMX metadata standard – used as the basis for \emph{QB} – and DDI have traditionally made efforts to align their content. 
%Similarly, some of the developers of \emph{Disco} were also involved in the development of \emph{QB}, 
%allowing the RDF versions of these standards to retain that alignment.
%While \emph{Disco} and \emph{QB} provide terms for the description of data sets, 
%both on a different level of aggregation, 
%the \emph{Data Catalog Vocabulary (DCAT)}\footnote{\url{http://www.w3.org/TR/vocab-dcat/}} enables the representation of these data sets inside of data collections like repositories, catalogs, or archives. 
%The relationship between data collections and their contained data sets is useful, since such collections are a typical entry point when searching for data.
%Although, in most cases aggregated data is still published in form of PDFs, 
%it is more and more common to publish aggregated data as CSV files,
The purpose of publicly available aggregated data is to get a first overview and to gain an interest in further analyses on the underlying person-level data.
Aggregated data is more and more published in form of CSV files,
allowing to perform data calculations.\ke{abgeshen davon dass ich more and more nicht mehr sehen kann, ist der zusammenhang mit RDF daten und damit RDF providern im nächsten satz unklar}
%For these calculations, definitions of the columns are needed (e.g. is a given variable interpreted numerically or as a string).
%\emph{Physical Data Description (PHDD)}\footnote{\url{https://github.com/linked-statistics/physical-data-description}} is a vocabulary to represent data in tabular format in RDF.\tb{tabular data} 
%The data could be either represented in records with character-separated values (CSV) or in records with fixed length. 
Portals harvest metadata (as well as publicly available data) from multiple RDF data providers.
To ensure high quality, (meta)data must satisfy certain criteria - specified in terms of RDF constraints.\ke{verwirrend. wie gesagt, können wir nicht bei RDF data bleiben? was anderes behandelt ihr doch eh nicht und qb enthält doch keine metadaten.} 

For more detailed analyses, researchers refer to person-level data including additional variables needed to answer subsequent research questions
%Although not that common, the other direction is also possible, 
%i.e. researchers may use metadata on person-level data to search for aggregated data.
like the comparison of studies between countries.
A \emph{study} represents the process by which a data set was generated or collected.
Eurostat\footnote{\url{http://ec.europa.eu/eurostat}}, the statistical office of the European Union, provides research findings in form of aggregated data (downloadable as CSV files) and its metadata at European level that enable comparisons between countries.
%This way, researchers get promising findings (in form of published tables with a few columns).
%We use the availability of childcare services in European Union member states by year, duration, and child age leading to subsequent research questions as running example of this paper.
%The first step is to determine in which countries the unemployment rate is collected and which other variables of each country-specific study are theoretically comparable and can therefore be used to answer the underlying research question.
The variable \emph{formal childcare}\footnote{Aggregated data and its metadata is available at: \url{http://ec.europa.eu/eurostat/web/products-datasets/-/ilc_caindformal}} %(in contrast to childcare at home)
captures the measured availability of childcare services in percent over the population in European Union member states by 
the variables \emph{year}, \emph{duration} (in hours per week), \emph{age} of the child, and \emph{country}.
Variables are constructed out of values (of one or multiple datatypes) and/or code lists.
The variable \emph{age}, e.g., may be represented by values of the datatype \emph{xsd:nonNegativeInteger} or by a code list of age clusters (e.g., '0 to 10' and '11 to 20'). 

To determine if variables measuring \emph{age} 
- collected for different countries (\emph{$age_{DE}$}, \emph{$age_{UK}$}) - 
are comparable, diverse constraints are checked:
(1) variable definitions must be available, 
(2) for each code a human-readable label has to be specified,
(3) code lists must be structured properly, and
(4) code lists must either be identical or at least similar.
If a researcher only wants to get a first overview over comparable variables (use case 1), 
covering the first three constraints may be sufficient,
i.e., the violation of the first three constraints is more serious than the violation of the last constraint.
If the intention of the researcher is to perform more sophisticated comparisons (use case 2), however, the user may raise the severity level of the last constraint.
 
\ke{ich finde das immer noch recht verwirrend. Ist das mit den aggregierten Daten wichtig für dieses Paper? Wieso werden constraints nur gechecked wenn variablen auf vergleichbarkeit geprüft werden? das habt ihr doch gar nicht gemacht sondern ganz allgemein constraints für vokabulare aufgestellt. Ist QB und Disco nicht sowieso unabhängig von konkreten Variablen?}
 
%After validating the metadata according to these constraints, portals offer added values to their customers, e.g., by searching over and comparing metadata of multiple providers. 

%\textbf{Data validation.}
%Data providers and harvesters do not only offer metadata but also publicly available data on different level of aggregation.
%To ensure high data quality, they have to check provenance information and to analyze and therefore validate the data according to predefined constraints (e.g. 'are fundamental data fragments available?', and 'how does valid data look like?'). 
%They validate, e.g., if fundamental data parts are available and define how valid data should look like (using syntactic rules).
%Provenance (where does the data come from?) is an important aspect in evaluating data quality.
%As data searchers know exactly which data sources they trust and which are reasonable to meet their individual use cases, 
%RDF data validation can only be performed semi-automatically, i.e., an automatic approach serves as basis for intellectual decisions. 

\section{Related Work}
\label{related-work}

%\textbf{XML vs. RDF Validation.}
For data archives, research institutes, and data libraries,
RDF validation according to predefined constraints is a much sought-after feature, 
particularly as this is taken for granted in the XML world.
DDI-XML documents, e.g., are validated against diverse XSDs\footnote{\url{http://www.ddialliance.org/Specification/}}.
As certain constraints cannot be formulated and validated by XSDs, 
so-called secondary-level validation tools like \emph{Schematron}\footnote{\url{https://msdn.microsoft.com/en-us/library/aa468554.aspx}} have been introduced to overcome the limitations of XML validation.
\emph{Schematron} generates validation rules and validates XML documents according to them.
With RDF validation, one can overcome drawbacks when validating XML documents\footnote{\url{http://www.xmlmind.com/xmleditor/_distrib/doc/xmltool/xsd_structure_limitations.html}}.
It cannot be validated, e.g., if each code of a variable's code list is associated with a category (\emph{R-86}).
Additionally, it cannot be validated that if an element has a specific value, then certain child elements must be present (\emph{R-71}).  
A comprehensive comparison of XML and RDF validation, however, is not within the scope of this paper.

%validation or relational databases: types, relations

%\textbf{RDF Data Cube Vocabulary.}
A well-formed \emph{RDF Data Cube} is an a RDF graph describing one or more instances of \emph{qb:DataSet} for which each of the 22 integrity constraints\footnote{http://www.w3.org/TR/vocab-data-cube/\#wf}, defined within the \emph{QB} specification, passes.
Each integrity constraint is expressed as narrative prose and, where possible, a SPARQL ASK query or query template. 
If the ASK query is applied to an RDF graph then it will return true if that graph contains one or more \emph{QB} instances which violate the corresponding constraint \cite{CyganiakReynolds2014}.
Mader, Haslhofer, and Isaac investigated how to support
taxonomists in improving SKOS vocabularies by pointing out quality
issues that go beyond the integrity constraints defined in the SKOS specification \cite{MaderHaslhoferIsaac2012}.

%\section{Running Example}
%\label{running-example}
\section{Common Vocabularies in SBE Sciences}
\label{rdf-representation}
\tb{jeweils ein satz zu DCAT und XKOS}
%In this section, we describe how data in rectangular format (\emph{PHDD}) and metadata on person-level data sets (\emph{Disco}), aggregated data sets (\emph{QB}), thesauri (\emph{SKOS}), and statistical classifications (\emph{XKOS}) are represented in RDF\footnote{The complete running example in RDF is available at: \url{https://github.com/boschthomas/rdf-validation/tree/master/data/running-example}} and how therefore reused vocabularies are interrelated.

For our evaluation, we examined four different vocabularies commonly used in or developed for the SBE sciences which are briefly introduced in the following. For three of them, we analysed actual data according to constraint violations. For PHDD, there is not yet enough data available so we limit our examination to the vocabulary itself.\ke{Neuer Absatz von mir...}

%\textbf{Metadata on Aggregated Data.}
The \emph{RDF Data Cube Vocabulary (QB)}\footnote{http://www.w3.org/TR/vocab-data-cube/} is a W3C recommendation for representing metadata on \emph{data cubes}, i.e. multi-dimensional aggregated data, in RDF \cite{Cyganiak2010}. 
%\emph{QB} represents metadata on multi-dimensional aggregate data in two files: a \emph{qb:DataSet} and a \emph{qb:DataStructureDefinition}.
A \emph{qb:DataStructureDefinition} contains metadata of the data collection.
The variable \emph{formal childcare} is modelled as \emph{qb:measure}, since it stands for what has been measured in the data collection.
The variables \emph{year}, \emph{duration}, \emph{age}, and \emph{country} are \emph{qb:dimensions}.
Data values, i.e., the availability of childcare services in percent over the population, are collected in a \emph{qb:DataSet}. 
Each data value is represented inside a \emph{qb:Observation} which contains values for each dimension\footnote{The complete running example in RDF is available at: \url{https://github.com/boschthomas/rdf-validation/tree/master/data/running-example}}. 
%(e.g., the year in which \emph{formal childcare} has been determined).
\ke{Wieso springt das von einer sehr knappen Einführung direkt zu konkreten (beliebigen?) Variablen? Ich möchte wissen, wofür QB benutzt wird und gerne ein konkretes Beispiel sehen, um mir einen Data Cube vorstellen zu können.}

%\textbf{Rectangular Data.}
\emph{Physical Data Description (PHDD)}\footnote{\url{https://github.com/linked-statistics/physical-data-description}} is a vocabulary to represent data in tabular format in RDF enabling further aggregations and calculations. 
The data could be either represented in records with character-separated values (CSV) or fixed length. 
\emph{Eurostat} provides a CSV file, a two-dimensional table (\emph{phdd:Table}) about \emph{formal childcare} 
which is structured by a table structure (\emph{phdd:TableStructure}, \emph{phdd:Delimited})
including information about the character set (\emph{ASCII}), the variable delimiter (\emph{,}), the new line marker (\emph{CRLF}), and the first line where the data starts (\emph{2}).
The table structure is related to table columns (\emph{phdd:Column}) which are described by column descriptions (\emph{phdd:DelimitedColumnDescription}).
For the column containing the cell values in percent, the column position (\emph{5}), the recommended data type (\emph{xsd:nonNegativeInteger}), and the storage format (\emph{TINYINT}) is stated. 
%The RDFication enables further aggregations and calculations, e.g., in order to compare \emph{formal childcare} between Northern and Southern Europe or between otherwise grouped countries.
% macht mehr sinn wenn Tabelle in mehreren Dimensionen vorhanden ist ode wenn Klassifikationen wie ISCO verwendet werden 

%\textbf{Metadata on Person-Level Data.}
For more detailed analyses we refer to the metadata on person-level data collected for the series \emph{EU-SILC (European Union Statistics on Income and Living Conditions)}\footnote{\url{http://www.gesis.org/missy/eu/metadata/EU-SILC}}. 
%published by the \emph{Microdata Information System (MISSY)}\footnote{\url{http://www.gesis.org/missy/eu/missy-home}}.
Where data collection is cyclic, data sets may be released as \emph{series}, 
where each cycle produces one or more data sets. 
%\emph{Missy} is an online service platform that provides systematically structured metadata for official statistics on European person-level data sets. 
%This includes data documentation at the study and variable level as well as documentation materials, tools and further information. 
\ke{Wieso jetzt wieder qb?}\tb{ich möchte zeigen wie aggregierte daten und personenbezogene daten in RDF verbunden sind. Weglassen?}
Aggregated (qb:DataSet) and underlying person-level data sets (\emph{disco:LogicalDataSet}) are connected by \emph{prov:wasDerivedFrom}. 
The aggregated variable \emph{formal childcare} is calculated on the basis of six person-level variables 
(e.g., \emph{Education at pre-school}\footnote{\url{http://www.gesis.org/missy/eu/metadata/EU-SILC/2011/Cross-sectional/original#2011-Cross-sectional-RL010}})
%(2) Education at compulsory school,
%(3) Child care at centre-based services,
%(4) Child care at day-care centre,
%(5) Child care by a professional child-minder, and
%(6) Child care by grand-parents, household members, relatives, friends, neighbours.
for which detailed metadata is given 
%(definitions, descriptions, theoretical concepts, questions variables are based on, code lists, frequencies, descriptive statistics, countries, year of data collection, and classifications) 
(e.g., code lists)
enabling researchers to replicate the results shown in aggregated data tables.
The \emph{DDI-RDF Discovery Vocabulary (Disco)} is a vocabulary to represent metadata on person-level data in RDF.
The series (\emph{disco:StudyGroup}) \emph{EU-SILC} contains one study (\emph{disco:Study}) for each year (\emph{dcterms:temporal}) of data collection.   
\emph{dcterms:spatial} points to the countries for which the data has been collected.
The study \emph{EU-SILC 2011} contains eight person-level data sets (\emph{disco:LogicalDataSet})
including person-level variables (\emph{disco:Variable}) like the six ones needed to calculate the aggregated variable \emph{formal childcare}.
%Metadata on person-level data enables researchers to investigate further research questions based on promising findings of other researchers in form of aggregated data.
%One common research question is ,e.g., the comparison of variables like 
%\emph{formal childcare} between countries, for which the variable is collected within the context of an individual study, and other European or non European countries (e.g. OSCE).

%\textbf{Organizations, Hierarchies, and Classifications.}
%\textbf{Reusing SKOS.}
The \emph{Simple Knowledge Organization System (SKOS)} is reused multiple times to build \emph{SBE} vocabularies.
%Variables are constructed out of values and/or (un)ordered code lists.
The codes of the variable \emph{Education at pre-school} (number of education hours per week) are modeled as \emph{skos:Concepts} and 
a \emph{skos:OrderedCollection} organizes them in a particular order within a \emph{skos:memberList}.
A variable may be associated with a theoretical concept (\emph{skos:Concept}). 
\emph{skos:narrower} builds the hierarchy of theoretical concepts within a \emph{skos:ConceptScheme} of a series.
The variable \emph{Education at pre-school} is assigned to the theoretical concept \emph{Child Care} which is the narrower concept of \emph{Education} - one of the top concepts of the series \emph{EU-SILC}.
Controlled vocabularies (\emph{skos:ConceptScheme}), serving as extension and reuse mechanism,
organize types (\emph{skos:Concept}) of descriptive statistics (\emph{disco:SummaryStatistics}) like minimum, maximum, and arithmetic mean.
%\emph{XKOS}\footnote{\url{https://github.com/linked-statistics/xkos}} is a SKOS extension to describe formal statistical classifications like the International Standard Classification of Occupations (\emph{ISCO}). 
%and the Statistical Classification of Economic Activities in the European Community \emph{NACE}.

%\textbf{Searching for (Meta)data.}
%\emph{DCAT} enables to represent aggregated and person-level data sets inside of data collections like portals, repositories, catalogs, and archives
%which serve as typical entry points when searching for data.
%Users search for aggregated and person-level data records (\emph{dcat:CatalogRecord}) inside data catalogs (\emph{dcat:Catalog}). 
%As search differs depending on the users’ information need,
%users may only search for records' metadata (e.g., \emph{dcterms:title}, \emph{dcterms:description}), 
%or may formulate more sophisticated queries on aggregated and person-level data sets (\emph{dcat:Dataset}) or their
%distributions (\emph{dcat:Distribution}) which are part of the records. 
%Often, users search for data sets covering particular topics (\emph{dcat:keyword}, \emph{dcat:theme}), time periods (\emph{dcterms:temporal}),  or  locations (\emph{dcterms:spatial}), 
%or for certain formats in which the data distribution is available (\emph{dcterms:format}). 

\section{Classification of Constraint Types and Constraints}
\label{classification}

Bosch et al. identified 76 requirements to formulate RDF constraints (e.g. \emph{R-75: minimum qualified cardinality restrictions}); each of them corresponding to an RDF constraint type\footnote{Constraint types and constraints are uniquely identified by alphanumeric technical identifiers like \emph{R-71-CONDITIONAL-PROPERTIES}}\cite{BoschNolleAcarEckert2015}. 
We published a technical report\footnote{Available at: \url{http://arxiv.org/abs/1501.03933}} in which we explain each requirement/constraint type in detail and give examples for each expressed by different constraint languages.
The knowledge representation formalism \emph{Description logics (DL)}, with its  well-studied theoretical properties, provides the foundational basis for each constraint type.
Therefore, this technical report contains mappings to \emph{DL} to logically underpin each requirement and to determine which \emph{DL} constructs are needed to express each constraint type \cite{BoschNolleAcarEckert2015}.
We classified both RDF constraint types and RDF constraints to gain better insights into the quality of RDF data with respect to this classification, independent of the used vocabulary.
%Constraints are instantiated from constraint types in order to validate both metadata and data represented by any vocabulary; thus, the proposed classification system is vocabulary-independent and therefore applicable generically.
We recently published a technical report\footnote{\label{technical-report-1}Available at: \url{http://arxiv.org/abs/1504.04479}} (serving as first appendix of this paper) in which we describe 246 constraints of 53 distinct constraint types on six vocabularies \cite{BoschZapilkoWackerowEckert2015}.

\subsection{Classification of RDF Constraint Types}

According to the complexity of constraint types, the complete set of \emph{constraint types} encompasses three disjoint \emph{sets of constraint types}:
\begin{enumerate}
	\item \textbf{\emph{Vocabulary Constraint Types}}
	\item \textbf{\emph{Simple Constraint Types}}
	\item \textbf{\emph{Complex Constraint Types}}
\end{enumerate}

The modeling languages \emph{RDF}, \emph{RDFS}, and \emph{OWL} are typically used to define vocabularies.
\emph{Vocabulary constraint types}
denotes the set of constraint types whose constraints 
can be extracted completely automatically out of the formal specifications of vocabularies.
As vocabularies have been specified using \emph{RDF}, \emph{RDFS}, and \emph{OWL},
\emph{vocabulary constraints} ensure that the data is consistent with the intended syntaxes, semantics, and integrity of vocabularies' data models.
%The {\em existential quantification} (\emph{R-86}) 
%\ms{Study $\equiv$ $\exists$ product.LogicalDataSet} (\emph{DL}), e.g., restricts studies to contain at least one data set which is expressible in OWL 2:
\emph{Minimum qualified cardinality restrictions} (\emph{R-74}), e.g., guarantee that individuals of given classes are connected by particular properties to at least n different individuals/literals of certain classes or data ranges.
This way, it is expressible in \emph{OWL 2} that a \emph{phdd:TableStructure} has (\emph{phdd:column}) at least one \emph{phdd:Column}:
\ke{Ich dachte, das kommt aus dem Vokabular. Dann muss das heißen: For example, in PHDD, a minimum qualified cardinality constraint can be obtained from the OWL definition of this restriction class, ensuring that a \emph{phdd:TableStructure} has (\emph{phdd:column}) at least one \emph{phdd:Column}:}
\begin{ex}
[   a owl:Restriction ; rdfs:subClassOf TableStructure ;
    owl:minQualifiedCardinality 1 ;
    owl:onProperty column ;
    owl:onClass Column ] .
\end{ex}

\emph{Simple} and \emph{complex constraints} are in contrast to \emph{vocabulary constraints} not explicitly defined within formal specifications of vocabularies.
\emph{Simple} and \emph{complex constraints} are defined according to textual descriptions of the intended semantics of vocabularies.  
\emph{Simple constraint types} is the set of constraint types whose constraints can be easily defined without much effort in addition to \ke{einfügen: the already defined (auf den Lesefluss achten, das ist eine sehr pragmatische klassifikation und muss nicht übertrieben formal und trocken geschrieben sein)} \emph{vocabulary constraints}.
{\emph{Data property facets} (\emph{R-46}) is an example of a \emph{simple constraint type} which enables to declare frequently needed facets for data properties in order to validate input against simple conditions including min/max values, regular expressions, and string length.
The abstract of series/studies, e.g., should have a minimum length.

\emph{Complex constraint types} encompass constraint types for which the definition of constraints is rather complex and cannot be derived from vocabulary definitions.
For assessing the quality of thesauri, e.g., we concentrate on the graph-based structure and apply graph- and network-analysis techniques.
An example of such constraints of the constraint type \emph{structure} is that 
a thesaurus should not contain many orphan concepts, i.e., concepts without any associative or hierarchical relations, lacking context information valuable for search. 
\emph{Complex constraints} show the importance to develop constraint languages enabling to describe more complex constraints.\ke{der satz macht so keinen sinn... aber er ist ein weiteres gutes beispiel, warum du viel mehr auf die formulierungen achten musst. Ich weiß genau was du meinst, du hast auch recht, aber hier steht trotzdem ein sinnloser satz, so wie er formuliert ist.} 

\ke{ich würde da auch für simple und vor allem für complex konkrete beispiele in code einfügen}

\subsection{Classification of RDF Constraints}

\ke{da muss mindestens ein überleitender satz rein, der noch mal den unterschied von constraint types und types aufgreift und erklärt, dass es jetzt um konkrete constraints auf den vokabularen geht, deren severity eben vom konkreten kontext abhängt und nicht allgemein über den type bestimmt werden kann.}

\emph{SBE} experts determined the default \textbf{\emph{severity level}} (\emph{R-158}) for each of the 246 constraints to indicate how serious the violation of the constraint is.
We propose an extensible metric to measure the continuum of severity levels ranging from \emph{informational} to \emph{error}.\ke{oder etwas weniger anmaßend: we use the commonly accepted classification of log messages in software development and distingush informational, warning and error.}
According to the default severity level of constraints, the complete set of constraints encompasses three disjoint \emph{sets of constraints}:\ke{auch dieser satz macht keinen sinn für mich, ebensowenig wie die extrem erhellende auflistung, die gleich folgt... mein vorschlag von eben kann das komplett ersetzen.}
\begin{itemize}
	\item \textbf{\emph{informational constraints}}: constraints with severity level \emph{informational}
	\item \textbf{\emph{warning constraints}}: constraints with severity level \emph{warning}
	\item \textbf{\emph{error constraints}}: constraints with severity level \emph{error}
\end{itemize}
Violations of \emph{informational constraints} point to desirable but not necessary data improvements to achieve RDF representations which are ideal in terms of syntax and semantics of used vocabularies. 
Data not conforming to \emph{warning} and \emph{error constraints} is syntactically and/or semantically not correctly represented.
Data not conforming to \emph{warning constraints} but to \emph{error constraints} could be processed further.
Data not corresponding to \emph{error constraints}, however, cannot be processed further after validation. 
As the purpose of \emph{vocabulary constraints} is to ensure explicitly stated semantics of vocabularies, 
their default severity levels are in most cases very strong (\emph{error}) and 
in average stronger than the severity levels of \emph{simple} and \emph{complex constraints}.
As a consequence, violating many \emph{vocabulary constraints} is an indicator for bad (meta)data quality\footnote{For simplicity reasons, we only assign severity levels to \emph{vocabulary constraints} in this paper in case they differ from \emph{error constraints}.}.
\ke{den ganzen letzten absatz finde ich unübersichtlich. Kann man das nicht klarer sagen: Error sind syntaktische oder semantische fehler, die zu einem abbruch der datenverarbeitung führen sollten. warnings sind syntaktische oder semantische fehler, die aber normalerweise nicht zu einem abbruch führen müssen. verletzungen von informational constraints weisen auf potentielle verbesserungen in den daten hin, werden aber nicht als fehler angesehen (der satz war in ordnung). die fussnote ist doch vollkommen fehl am platz, oder? es werden ja nicht alle constraints aufgelistet und wie ihr das implementiert habt und ob ihr euch arbeit erspart habt weil ihr default severity bei bestimmten constraint types vorgebt, das interessiert doch keinen.}

Although, we provide default severity levels for each constraint, validation environments should enable users to adapt the severity levels of constraints according to their individual needs.
Validation environments should enable users to select which constraints to validate against depending on their individual use cases.
For some use cases, 
validating \emph{vocabulary constraints} may be more important than validating \emph{simple} or \emph{complex constraints}.
For other use cases,
validating \emph{error constraints} may be sufficient without taking \emph{warning} and \emph{informational constraints} into account.
We evaluated the (meta)data quality of large real world data sets represented by multiple and different vocabularies to get an understanding  
(1) which sets of constraint types (on different levels of complexity) and 
(2) which sets of constraints (associated with particular severity levels) encompass the constraints causing the most/fewest constraint violations (see section \ref{evaluation}).

%
%- different sets of constraints for different use cases / use case specific constraints
%
%- validation against minimal set of requirements / constraints
%
%-----

%further ideas:
%
%RDF validation scenarios require the closed-world assumption (CWA) (i.e., a statement is inferred to be false if it cannot be proved to be true).

%use-case specific constraints, e.g., DCAT: searching for metadata
%\tb{Thomas: ToDo}

\section{Vocabulary Constraint Types}
\label{vocabulary-constraint-types}

The constraint type \emph{vocabulary} guarantees that users do not invent new or use deprecated terms of vocabularies.
%Out-dated classes and properties of previous vocabulary versions can be marked as deprecated.
%The constraint types \emph{context-specific valid classes and properties} (\emph{R-209; R-210}) can be used to specify which classes and properties are valid in which context - here a given vocabulary version.
\emph{Value is valid for datatype} (\emph{R-223}) constraints serve to make sure that all literal values are valid with regard to their datatypes - as stated in the vocabularies.
Thus, it is checked that all date values (e.g., {{\em dcterms:date}) are actually of the datatype \emph{xsd:date} and that \emph{xsd:nonNegativeInteger} values (e.g. \emph{disco:frequency}) are not negative.
Depending on property datatypes, two different literal values have
a specific ordering with respect to operators like \textless  (\emph{R-43: literal value comparison}).
Start dates (\emph{disco:startDate}), e.g., must be before (\emph{\textless}) end dates (\emph{disco:endDate}).

\begin{center}
\begin{DL} 
$\mathcal{K}=\{$ 
	&\ms{isStructuredBy $\sqsubseteq$ $\neg$ column}, \\
	&\ms{TableDescription $\sqcap$ ColumnDescription $\sqsubseteq$ $\perp$}, \\
	&\ms{CategoryStatistics $\equiv$} \\
	&\ms{$\forall$ computationBase.\{valid,invalid\} $\sqcap$ langString}
 \}\\ 
\end{DL}
\end{center}
\tb{K einführen}
All properties, not having the same domain and range types, are defined to be pairwise disjoint
(\emph{R-9: disjoint properties}), i.e., no individual \emph{x} can be connected to an individual/literal \emph{y} by disjoint properties (e.g., \emph{phdd:isStructuredBy} and \emph{phdd:column}).
All \emph{PHDD} classes (e.g., \emph{phdd:TableDescription}, \emph{phdd:ColumnDescription}) are pairwise disjoint (\emph{R-7: disjoint classes}),
i.e., individuals cannot be instances of multiple disjoint classes.
It is a common requirement to narrow down the value space of properties by an exhaustive enumeration of valid values (\emph{R-30/37: allowed values}). 
%\emph{Allowed values} (\emph{R-30, R-37}) for properties can be IRIs (matching one or multiple patterns), any literals, allowed literals (e.g. 'red' 'blue' 'green'), and typed literals of one or multiple type(s) (e.g. \emph{xsd:string}). 
\emph{disco:CategoryStatistics}, e.g., can only have \emph{disco:computationBase} relationships to the values \emph{valid} and \emph{invalid} of the datatype \emph{rdf:langString}.
Validation should \emph{exploit sub-super relations} in vocabularies (\emph{R-224}).
If \emph{dcterms:coverage} and one of its sub properties (\emph{dcterms:spatial}, \emph{dcterms:temporal}) are given,
it is checked that \emph{dcterms:coverage} is not redundant with its sub-properties 
which may indicate when the data is verbose/redundant or expressed at a too general level.

-----

\emph{Property domain} (\emph{R-25, R-26}) and \emph{range} (\emph{R-28, R-35}) constraints restrict domains and ranges of properties.
Only \emph{phdd:Tables}, e.g., can have \emph{phdd:isStructuredBy} relationships and
\emph{xkos:belongsTo} relationships can only point to \emph{skos:Concepts}:
\begin{DL}
$\exists$ isStructuredBy.$\top$ $\sqsubseteq$ Table \\
$\top$ $\sqsubseteq$ $\forall$ belongsTo.Concept
\end{DL}

A \emph{universal quantification} (\emph{R-91}) contains all those individuals that are connected by a property only to individuals/literals of particular classes  or data ranges.
Only \emph{dcat:Catalogs}, e.g., can have \emph{dcat:dataset} relationships to \emph{dcat:Datasets}:
\begin{DL}
Catalog $\sqsubseteq$ $\forall$ dataset.Dataset
\end{DL}

%\textbf{Existential Quantifications.}
\emph{Existential quantifications} (\emph{R-86}) enforce that instances of given classes must have some property relation to individuals/literals of certain types.
Variables, e.g., should have a relation to a theoretical concept ($\mathcal{SL}_{0}$).
%The variables \emph{number of school years} and \emph{highest educational degree}, e.g., are associated with the theoretical concept \emph{education}. 
The variable \emph{Education at pre-school} is associated with the theoretical concept \emph{Child Care}. 
The default severity level of the constraint is weak, as in most cases research can be continued without having information about the theoretical concept of a variable.
\begin{DL}
Variable $\equiv$ $\exists$ concept.Concept
\end{DL}
%If a study, e.g., does not contain any data set ($\mathcal{SL}_{2}$), the actual description of the data is missing which may indicate that it is very hard or unlikely to get access to the data.
%If metadata on data files, including the actual data, is missing (especially case and variable quantities; $\mathcal{SL}_{1}$), the description of the data sets and the study is not sufficient.
%Case quantity measures how many cases are collected for a study.
%High case and variable quantities are indicators for high statistical quality and comprehensiveness of the underlying study ($\mathcal{SL}_{1}$).

%\textbf{Cardinality Restrictions on Properties.}
%An \emph{existential quantification} (\emph{R-86}) contains all those individuals that are connected by a property to individuals/literals of given classes or data ranges.
%Every \emph{qb:SliceKey}, e.g., must be associated with (\emph{qb:sliceKey}) a \emph{qb:DataStructureDefinition} (\ms{SliceKey $\sqsubseteq$ $\exists$ sliceKey$^{-}$.DataStructureDefinition}).
\emph{Minimum/maximum/exact qualified cardinality restrictions} (\emph{R-74, R-75, R-76}) contain all those individuals that are connected by a property to at least/at most/exactly n different individuals/literals of particular classes or data ranges.
A \emph{phdd:TableStructure} has (\emph{phdd:column}) at least one \emph{phdd:Column},
a \emph{disco:Variable} has at most one \emph{disco:concept} relationship to a theoretical concept (\emph{skos:Concept}), and a \emph{qb:DataSet} is structured by (\emph{qb:structure}) exactly one \emph{qb:DataStructureDefinition}.
\begin{DL}
TableStructure $\sqsubseteq$ $\geq$1 column.Column \\
Variable $\sqsubseteq$ $\leq$1 concept.Concept \\
DataSet $\sqsubseteq$ $\geq$1 structure.DSD $\sqcap$
$\leq$1 structure.DSD
\end{DL}

%\textbf{Validation and Reasoning.}
Some constraint types enable performing reasoning prior to validation which may resolve or cause constraint violations.
With \emph{subsumption} (\emph{R-100}), one can state that \emph{xkos:ClassificationLevel} is a sub-class of \emph{skos:Collection}, i.e., each \emph{xkos:ClassificationLevel} must also be part of the \emph{skos:Collection} class extension.
With \emph{sub properties} (\emph{R-54, R-64}), one can state that \emph{disco:fundedBy} is a sub-property of \emph{dcterms:contributor} - i.e., if a study is funded by an organization, then this organization contributed to this study.
\begin{DL}
ClassificationLevel $\sqsubseteq$ Collection \\
fundedBy $\sqsubseteq$ contributor
\end{DL} 

%\section{Basic Constraint Types}
%\label{vocabulary-constraint-types}
%
%As \emph{RDFS} and \emph{OWL} are typically used to define vocabularies, \emph{RDFS} and \emph{OWL} reasoning may be performed prior to validation. 
%Reasoning and validation are indeed very closely related. 
%\emph{Reasoning} is the process of determining what follows from what has been stated.
%%Both should be possible: (1) validation with reasoning and (2) validation without reasoning. 
%We divide the whole set of \emph{basic constraint types} ($\mathcal{CT}_{B}$) into two disjoint sets to investigate the affect of reasoning to the validation process: 
%\begin{enumerate}
	%\item $\mathcal{C}_B ^{\mathcal{R}}$ corresponds to axioms in \emph{OWL 2} and denotes the set of constraint types which enable performing reasoning prior to validation, especially when not all the knowledge is explicit (section \ref{basic-constraint-types-with-reasoning}).  
  %\item $\overline{\mathcal{C}_B ^{\mathcal{R}}}$  denotes the set of constraint types for which reasoning cannot be done or does not improve the result in any obvious sense (section \ref{basic-constraint-types-without-reasoning}).
%\end{enumerate}
%
%\subsection{Basic Constraint Types with Reasoning}
%\label{basic-constraint-types-with-reasoning}
%
%Validation environments should enable users to decide if they wish to perform reasoning prior to validation.
%Reasoning as an optional pre validation step is beneficial for RDF validation as 
%(1) it may resolve constraint violations and  
%(2) it may cause useful constraint violations.
%A \emph{universal quantification} (\emph{R-91}) contains all those individuals that are connected by a property only to individuals/literals of particular classes  or data ranges.
%%Only \emph{dcat:Catalogs}, e.g., can have \emph{dcat:dataset} relationships to \emph{dcat:Datasets}:
%%\begin{DL}
%%Catalog $\sqsubseteq$ $\forall$ dataset.Dataset
%%\end{DL}
%Consider the following DL knowledge base $\mathcal{K}$\footnote{A knowledge base is a collection of formal statements which corresponds to \emph{facts} or what is known explicitly. For simplicity reasons, we only write namespace prefixes in DL statements to avoid ambiguities.}:
%
%\begin{center}
%\begin{DL} 
%$\mathcal{K}=\{$ &\ms{LogicalDataSet $\sqsubseteq$ dcat:DataSet},\\
 %&\ms{LogicalDataSet $\sqsubseteq \forall$ aggregation.qb:DataSet},\\
 %%&\ms{dcat:DataSet $\equiv$ $\exists$ dcat:distribution . dcat:Distribution},\\
 %&\ms{LogicalDataSet( logical-data-set )},\\
 %&\ms{aggregation( logical-data-set, aggregated-data-set )}
 %\}\\ 
%\end{DL}
%\end{center}
%
%As we know that only person-level data sets (\emph{disco:LogicalDataSet}) can derive (\emph{disco:aggregation}) aggregated data sets (\emph{qb:DataSet}), 
%{\em logical-data-set} is a \emph{disco:LogicalDataSet}, 
%and \emph{aggregated-data-set} is derived from \emph{logical-data-set},
%we conclude that \emph{aggregated-data-set} must be a \emph{qb:DataSet}.
%As \emph{aggregated-data-set} is not explicitly defined to be a \emph{qb:DataSet}, however, a constraint violation is raised.
%If we perform reasoning prior to validation, the constraint violation is resolved, as the implicit triple \ms{qb:DataSet(aggregated-data-set)} is inferred. 
%
%Reasoning may also cause constraint violations which are needed to enhance data quality.
%With \emph{subsumption} (\emph{R-100}), one can state that \emph{disco:LogicalDataSet} is a sub-class of \emph{dcat:DataSet}, 
%i.e., each person-level data set is also a catalog data set.
%Thus, constraints on catalog data sets are also validated for person-level data sets;
%e.g., the \emph{existential quantification} below restricting that person-level data sets must have a distribution: 
%
%\begin{center}
%\begin{DL} 
%$\mathcal{K}=\{$ 
 %&\ms{DataSet $\equiv$ $\exists$ distribution.Distribution},\\
%&\ms{Variable $\equiv$ $\exists$ concept.Concept}
 %\}\\ 
%\end{DL}
%\end{center}
%
%We extend $\mathcal{K}$ by \emph{existential quantifications} (\emph{R-86}) enforcing that instances of given classes must have some property relation to individuals/literals of certain types.
%Variables, e.g., should have a relation to a theoretical concept ($\mathcal{SL}_{0}$).
%%The variables \emph{number of school years} and \emph{highest educational degree}, e.g., are associated with the theoretical concept \emph{education}. 
%The variable \emph{Education at pre-school} is associated with the theoretical concept \emph{Child Care}. 
%The default severity level of the constraint is weak, as in most cases research can be continued without having information about the theoretical concept of a variable.
%%If a study, e.g., does not contain any data set ($\mathcal{SL}_{2}$), the actual description of the data is missing which may indicate that it is very hard or unlikely to get access to the data.
%%If metadata on data files, including the actual data, is missing (especially case and variable quantities; $\mathcal{SL}_{1}$), the description of the data sets and the study is not sufficient.
%%Case quantity measures how many cases are collected for a study.
%%High case and variable quantities are indicators for high statistical quality and comprehensiveness of the underlying study ($\mathcal{SL}_{1}$).
%
%\begin{center}
%\begin{DL} 
%$\mathcal{K}=\{$ 
  %&\ms{fundedBy $\sqsubseteq$ contributor}
 %\}\\ 
%\end{DL}
%\end{center}
%
%By stating that \emph{disco:fundedBy} is a sub property of \emph{dcterms:contributor},
%the {\em sub property} (\emph{R-54, R-64}) above assures that if a series is funded by an organization, then the organization must also contribute to the series.
%In case the \emph{sub-property} is applied without reasoning and $\mathcal{K}$ contains the triple \ms{disco:fundedBy} \ms{(EU-SILC,} \ms{organization)},
%a constraint violation is thrown if $\mathcal{K}$ does not explicitly include the triple \ms{dcterms:contributor} \ms{(EU-SILC,} \ms{organization)}.
%If the \emph{sub property} is applied with reasoning, on the other side, the latter triple is derived which resolves the constraint violation.
%
%%\begin{DL}
%%fundedBy $\sqsubseteq$ contributor
%%\end{DL} 
%
%%\emph{Asymmetric object properties} (\emph{R-62}) restrict that if individual \emph{x} is connected by the object property \emph{OP} to individual \emph{y}, then \emph{y} cannot be connected by \emph{OP} to \emph{x}. 
%%Such constraints are defined for each object property for which a semantically equivalent object property pointing from the other direction would also be possible but is not defined within the vocabulary.
%%A \emph{disco:Variable}, e.g., may be based on (\emph{disco:basedOn}) a \emph{disco:RepresentedVariable}.
%%A \emph{disco:RepresentedVariable}, however, cannot be based on a \emph{disco:Variable} (\ms{$disco:basedOn \sqcap disco:basedOn^{-} \sqsubseteq \bot$}).
%
%%\emph{Property domain} (\emph{R-25, R-26}) and \emph{range} (\emph{R-28, R-35}) constraints restrict domains and ranges of properties.
%%Only \emph{phdd:Tables}, e.g., can have \emph{phdd:isStructuredBy} relationships and
%%\emph{xkos:belongsTo} relationships can only point to \emph{skos:Concepts}:
%%\begin{DL}
%%$\exists$ isStructuredBy.$\top$ $\sqsubseteq$ Table \\
%%$\top$ $\sqsubseteq$ $\forall$ belongsTo.Concept
%%\end{DL}
%
%%\begin{center}
%%\begin{DL} 
%%$\mathcal{K}=\{$ 
  %%&\ms{TableStructure $\sqsubseteq$ $\geq$1 column.Column}, \\
	%%&\ms{Variable $\sqsubseteq$ $\leq$1 concept.Concept}, \\
	%%&\ms{DataSet $\sqsubseteq$ $\geq$1 structure.DataStructureDefinition} \\ 
	%%&\ms{$\sqcap$ $\leq$1 structure.DataStructureDefinition}
 %%\}\\ 
%%\end{DL}
%%\end{center}
%
%%\textbf{Cardinality Restrictions on Properties.}
%%An \emph{existential quantification} (\emph{R-86}) contains all those individuals that are connected by a property to individuals/literals of given classes or data ranges.
%%Every \emph{qb:SliceKey}, e.g., must be associated with (\emph{qb:sliceKey}) a \emph{qb:DataStructureDefinition} (\ms{SliceKey $\sqsubseteq$ $\exists$ sliceKey$^{-}$.DataStructureDefinition}).
%%\emph{Minimum/maximum/exact qualified cardinality restrictions} (\emph{R-74, R-75, R-76}) contain all those individuals that are connected by a property to at least/at most/exactly n different individuals/literals of particular classes or data ranges.
%%A \emph{phdd:TableStructure} has (\emph{phdd:column}) at least one \emph{phdd:Column},
%%a \emph{disco:Variable} has at most one \emph{disco:concept} relationship to a theoretical concept (\emph{skos:Concept}), and a \emph{qb:DataSet} is structured by (\emph{qb:structure}) exactly one \emph{qb:DataStructureDefinition}.
%%DATA-CUBE-C-MINIMUM-QUALIFIED-CARDINALITY-RESTRICTIONS-
%%02: Unique data set (IC-1 [3]) - Every qb:Observation has (qb:dataSet) ex-
%%actly one associated qb:DataSet (Observation  ¥1 dataSet.DataSet [
%%¤1 dataSet.DataSet).
%%Severity level: ERROR
%%
%%(\emph{R-75: minimum qualified cardinality restrictions})
%%
%%DATA-CUBE-C-EXACT-QUALIFIED-CARDINALITY-RESTRICTIONS-
%%02: Unique DSD (IC-2 [3]) - Every qb:DataSet has (qb:structure) exactly one
%%associated qb:DataStructureDefinition (DataSet  ¥1 structure.DataStructureDefinition
%%[ ¤1 structure.DataStructureDefinition).
%%Severity level: ERROR
%%
%%(\emph{R-74: exact qualified cardinality restrictions})
%%\footnote{\emph{DATA-CUBE-C-EXACT-QUALIFIED-CARDINALITY-RESTRICTIONS-02}}.
%%\textbf{Language Tag Cardinality.}
%
%\subsection{Basic Constraint Types without Reasoning}
%\label{basic-constraint-types-without-reasoning}
%
%$\overline{\mathcal{C}_B ^{\mathcal{R}}}$  denotes the set of constraint types for which reasoning cannot be done or does not improve the result in any obvious sense.
%The constraint type \emph{vocabulary} guarantees that users do not invent new or use deprecated terms of vocabularies.
%%Out-dated classes and properties of previous vocabulary versions can be marked as deprecated.
%%The constraint types \emph{context-specific valid classes and properties} (\emph{R-209; R-210}) can be used to specify which classes and properties are valid in which context - here a given vocabulary version.
%\emph{Value is valid for datatype} (\emph{R-223}) constraints serve to make sure that all literal values are valid with regard to their datatypes - as stated in the vocabularies.
%Thus, it is checked that all date values (e.g., {{\em dcterms:date}, \em disco:startDate}, {\em disco:endDate}) are actually of the datatype \emph{xsd:date} and that \emph{xsd:nonNegativeInteger} values (e.g. \emph{disco:frequency}) are not negative.
%Depending on property datatypes, two different literal values have
%a specific ordering with respect to operators like \textless  (\emph{R-43: literal value comparison}).
%Start dates (\emph{disco:startDate}), e.g., must be before (\emph{\textless}) end dates (\emph{disco:endDate}).
%
%\begin{center}
%\begin{DL} 
%$\mathcal{K}=\{$ 
	%&\ms{isStructuredBy $\sqsubseteq$ $\neg$ column}, \\
	%&\ms{TableDescription $\sqcap$ ColumnDescription $\sqsubseteq$ $\perp$}, \\
	%&\ms{CategoryStatistics $\equiv$} \\
	%&\ms{$\forall$ computationBase.\{valid,invalid\} $\sqcap$ langString}
 %\}\\ 
%\end{DL}
%\end{center}
%
%All properties, not having the same domain and range types, are defined to be pairwise disjoint
%(\emph{R-9: disjoint properties}), i.e., no individual \emph{x} can be connected to an individual/literal \emph{y} by disjoint properties (e.g., \emph{phdd:isStructuredBy} and \emph{phdd:column}).
%All \emph{PHDD} classes (e.g., \emph{phdd:TableDescription}, \emph{phdd:ColumnDescription}) are pairwise disjoint (\emph{R-7: disjoint classes}),
%i.e., individuals cannot be instances of multiple disjoint classes.
%It is a common requirement to narrow down the value space of properties by an exhaustive enumeration of valid values (\emph{R-30/37: allowed values}). 
%%\emph{Allowed values} (\emph{R-30, R-37}) for properties can be IRIs (matching one or multiple patterns), any literals, allowed literals (e.g. 'red' 'blue' 'green'), and typed literals of one or multiple type(s) (e.g. \emph{xsd:string}). 
%\emph{disco:CategoryStatistics}, e.g., can only have \emph{disco:computationBase} relationships to the values \emph{valid} and \emph{invalid} of the datatype \emph{rdf:langString}.
%Validation should \emph{exploit sub-super relations} in vocabularies (\emph{R-224}).
%If \emph{dcterms:coverage} and one of its sub-properties (\emph{dcterms:spatial}, \emph{dcterms:temporal}) are given,
%it is checked that \emph{dcterms:coverage} is not redundant with its sub-properties 
%which may indicate when the data is verbose/redundant or expressed at a too general level.

\section{Simple Constraint Types}

$\mathcal{CT}_{S}$ is the set of constraint types whose constraints can be easily defined without much effort in addition to $\mathcal{CT}_{B}$ constraints.
For data properties, it may be desirable to restrict that values of predefined languages must be present for determined number of times (\emph{R-48/49: language tag cardinality}):
(1) It is checked if literal language tags are set. Some controlled vocabularies, e.g., contain literals in natural language, but without information what language has actually been used ($\mathcal{SL}_{1}$). 
(2) Language tags must conform to language standards ($\mathcal{SL}_{2}$). 
(3) Some thesaurus concepts are labeled in only one, others in multiple languages. 
It may be desirable to have each concept labeled in each of the languages that are also used on the other concepts,
as language coverage incompleteness for some concepts may indicate shortcomings of thesauri ($\mathcal{SL}_{0}$)
\cite{MaderHaslhoferIsaac2012}.

%\textbf{Validation and Reasoning.}
\emph{Default values} (\emph{R-31, R-38}) for objects/literals of given properties are inferred automatically when properties are not present in the data.
The value \emph{true} for the property {\em disco:isPublic} indicates that a {\em disco:LogicalDataSet} can be accessed by anyone.
Per default, however, access to data sets should be restricted (\emph{false}) ($\mathcal{SL}_{0}$).
Many properties are not necessarily required but \emph{recommended} within a particular context (\emph{R-72}).
The property {\em skos:notation}, e.g., is not mandatory for {\em disco:Variable}s, but recommended to represent variable names ($\mathcal{SL}_{0}$).
Percentage values are only valid when they are within the literal range of 0 and 100 (\emph{R-45: literal ranges}; $\mathcal{SL}_{2}$)
which is checked for \emph{disco:percentage} standing for the number of cases of a given code in relation to the total number of cases for a particular variable.

\begin{center}
\begin{DL} 
$\mathcal{K}=\{$ 
	&\ms{(funct identifier$\sp{\overline{\ }})$, identifier keyfor Resource}
 \}\\ 
\end{DL}
\end{center}

It is often useful to declare a given (data) property as the \emph{primary key} (\emph{R-226}) of a class, so that a system can enforce uniqueness and build URIs from user inputs and imported data. 
In \emph{Disco}, resources are uniquely identified by the property \emph{adms:identifier},
which is therefore inverse-functional,
i.e., for each \emph{rdfs:Resource x}, there can be at most one distinct resource \emph{y} such that \emph{y} is connected by \emph{adms:identifier$\sp{\overline{\ }}$} to \emph{x} ($\mathcal{SL}_{2}$).
%\begin{DL}
%$(\ms{funct identifier}\sp{\overline{\ }})$
%\end{DL} 
Keys, however, are even more general than \emph{inverse-functional properties} (\emph{R-58}),
as a key can be a data property, an object property, or a chain of properties \cite{Schneider2009}.
Thus and as there are different sorts of key, and as keys can lead to undecidability, 
\emph{DL} is extended with the construct \emph{keyfor} \cite{Lutz2005} which is implemented by the \emph{OWL 2} \emph{hasKey} construct.
%\emph{OWL 2} \emph{hasKey} implements \emph{keyfor} ($\mathcal{SL}_{2}$) and thus can be used to identify resources uniquely, to merge resources with identical key property values, and to recognize constraint violations.
%\begin{DL}
%identifier \ms{keyfor} Resource
%\end{DL} 

\section{Complex Constraint Types}
\label{complex-constraint-types}

%\textcolor{red}{
%In this sub-chapter, we assign default severity levels to and describe constraints of diverse $\mathcal{CT}_{C}$ constraint types 
%to ensure that the data is consistent with the intended syntax, semantics, and integrity of vocabularies' data models.
%}
%\textbf{Observations of Aggregated Data Sets.}
%- for each dimension there should be a description and code lists.
%- for each code list there should be a description.
%- there should be a relationship to the underlying person-level data.

$\mathcal{CT}_{C}$ denotes the set of constraint types for which the definition of constraints is rather complex and cannot be derived from vocabulary definitions.
\emph{Data model consistency} constraints ensure the integrity of
the data according to the intended semantics of vocabularies.
Every \emph{qb:Observation}, e.g., must have a value for each dimension
declared in its \emph{qb:DataStructureDefinition} ($\mathcal{SL}_{2}$)
and no two \emph{qb:Observations} in the same \emph{qb:DataSet}
can have the same value for all dimensions ($\mathcal{SL}_{1}$).
If a \emph{qb:DataSet} \emph{D} has a \emph{qb:Slice} \emph{S}, and \emph{S} has an
\emph{qb:Observation} \emph{O}, then the \emph{qb:DataSet} corresponding to \emph{O} must be \emph{D} ($\mathcal{SL}_{1}$).
%Relative frequencies of variable codes are calculated correctly, if the cumulative percentage (\emph{disco:cumulativePercentage}) of a given code exactly matches the cumulative percentage of the previous code
%plus the percentage value (\emph{disco:percentage}) of the current code ($\mathcal{SL}_{2}$).
{\em Mathematical Operations} (\emph{R-41, R-42}; e.g. date calculations and statistical computations like average, mean, and sum) are performed to ensure the integrity of data models.
The sum of percentage values of all variable codes, e.g., must exactly be 100 ($\mathcal{SL}_{2}$)
and the minimum absolute frequency of all variable codes do not have to be greater than the maximum ($\mathcal{SL}_{2}$).

%\textbf{Hierarchies and Ordering.}
%\textbf{Structure and Ordering.}
%For assessing the quality of \emph{SKOS} vocabularies, we concentrate on graph-based structures and apply graph- and network-analysis techniques (\emph{structure} constraints) like 
%(1) a thesaurus should provide entry points (top concepts) to the data to provide efficient access and guidance for human users,
%(2) concepts, internal to the tree, should not be indicated as top concepts, and
%(3) a thesaurus should not contain many orphan concepts 
%(concepts without any associative or hierarchical relations) lacking valuable context information for retrieval.
%, as, e.g., no hierarchical query expansion can be performed on search terms to find documents with more general content.) \cite{MaderHaslhoferIsaac2012}. 

%A very common research question is to compare variables of multiple studies or countries (constraint type: \emph{comparison}).
%To compare variables 
%(1) their code lists must be structured properly and
%(2) their code lists must either be identical or at least similar.
%If a researcher only wants to get a first overview over comparable variables (use case 1), 
%covering the first constraint may be sufficient.
%Thus, the severity level of the first constraint is stronger ($\mathcal{SL}_{2}$) than the one for the second constraint ($\mathcal{SL}_{0}$).
%If the intention of the researcher is to perform more detailed comparisons (use case 2), however, the violation of the second constraint is getting more serious
%and the user may raise its severity level.
%\textbf{Comparison.}
%A very common research question is to compare variables of multiple studies or countries (constraint type: \emph{comparison}).
%To compare variables, 
%(1) variables and (2) variable definitions must be present,
%(3) code lists must be structured properly,
%(4) for each code an associated category must be specified, and
%(5) code lists must either be identical or at least similar.
%If a researcher wants to get a first overview over comparable variables (use case 1), 
%covering the first three constraints may be sufficient.
%Thus, the severity level of the first three constraints is stronger ($\mathcal{SL}_{2}$) than for the last two constraints ($\mathcal{SL}_{1}$ and $\mathcal{SL}_{0}$).
%If the intention of the researcher is to perform more detailed comparisons (use case 2), however, the violation of the remaining two constraints is getting more serious.

%\textbf{Membership in Controlled Vocabularies.}
In many cases, resources must be \emph{members of controlled vocabularies} (\emph{R-32}).
If a dimension property, e.g., has a \emph{qb:codeList},
then the value of the dimension property on every \emph{qb:Observation} must be in the code list ($\mathcal{SL}_{2}$).
Summary statistics types like minimum, maximum, and arithmetic mean are maintained within a controlled vocabulary.  
Thus, summary statistics can only have \emph{disco:summaryStatisticType} relationships to \emph{skos:Concept}s which must be members of the controlled vocabulary \emph{ddicv:SummaryStatisticType}, a \emph{skos:ConceptScheme} ($\mathcal{SL}_{2}$).
Objects/literals can be declared to be ordered for given properties (\emph{R-121/217: ordering}).
Variables, questions, and codes, e.g., are typically organized in a particular order. 
If codes (\emph{skos:Concept}) should be ordered, they must be members (\emph{skos:memberList}) in an ordered collection (\emph{skos:OrderedCollection}), the variable's code list ($\mathcal{SL}_{0}$).

%\textbf{Constraints on Properties.}
It is useful to declare properties to be \emph{conditional} (\emph{R-71}), i.e., if particular properties exist (or do not exist), then other properties must also be present (or absent).
To get an overview over a series/study either an abstract, a title, an alternative title, or links to external descriptions should be provided. 
If an abstract and an external description are absent, however,  
a title or an alternative title should be given ($\mathcal{SL}_{1}$).
In case a variable is represented in form of a code list, codes may be associated with categories, i.e., human-readable labels ($\mathcal{SL}_{0}$).
The variable \emph{Education at pre-school}, e.g., is represented as ordered code list without any categories.
If a {\em skos:Concept} represents a code (having {\em skos:notation} and {\em skos:prefLabel} properties), 
then the property {\em disco:isValid} has to be stated indicating if the code stands for valid (\emph{true}) or missing (\emph{false}) cases ($\mathcal{SL}_{2}$).
\emph{Context-specific exclusive or of property groups} (\emph{R-11}) constraints
restrict individuals of given classes to have properties defined within exactly one of multiple property groups.
\emph{skos:Concept}s can have either \emph{skos:definition} (when interpreted as theoretical concepts) or \emph{skos:notation} and \emph{skos:prefLabel} properties (when interpreted as codes/categories), but not both ($\mathcal{SL}_{2}$).

%\textbf{Constraints on Literals.}

%\begin{center}
%\begin{DL}
%Concept $\sqsubseteq$ ($\neg$ D $\sqcap$ C) $\sqcup$ (D $\sqcap$ $\neg$ C), D $\equiv$ A $\sqcap$ B \\
%A $\sqsubseteq$ $\geq$ 1 notation.string $\sqcap$ $\leq$ 1 notation.string \\
%B $\sqsubseteq$ $\geq$ 1 prefLabel.string $\sqcap$ $\leq$ 1 prefLabel.string \\
%C $\sqsubseteq$ $\geq$ 1 definition.string $\sqcap$ $\leq$ 1 definition.string \\
%\end{DL}
%\end{center}

%\textbf{Searching for (Meta)data.}
%
%- DCAT
%
%\textbf{Data Integration.}
%use RDF validation for data integration

%\textbf{Series, Studies, Data Sets, and Data Files.}
%It is useful to declare properties to be \emph{conditional} (\emph{R-71}), i.e., if particular properties exist (or do not exist), then other properties must also be present (or absent).
%To get an overview over a series/study either an abstract, a title, an alternative title, or links to external descriptions should be provided. 
%If an abstract and an external description are absent, however,  
%a title or an alternative title should be given ($\mathcal{SL}_{1}$).
%For datatype properties, it should be possible to declare frequently needed \emph{facets} ({\emph{R-46}) to validate input against simple conditions including min/max values, regular expressions, and string length.
%The abstract of series/studies, e.g., should have a minimum length ($\mathcal{SL}_{1}$).
%\emph{Existential quantifications} (\emph{R-86}) enforce that instances of given classes must have some property relation to individuals of certain types.
%If a study, e.g., does not contain any data set ($\mathcal{SL}_{2}$), the actual description of the data is missing which may indicate that it is very hard or unlikely to get access to the data.
%If metadata on data files, including the actual data, is missing (especially case and variable quantities; $\mathcal{SL}_{1}$), the description of the data sets and the study is not sufficient.
%Case quantity measures how many cases are collected for a study.
%High case and variable quantities are indicators for high statistical quality and comprehensiveness of the underlying study ($\mathcal{SL}_{1}$).
%
%\textbf{Variables and Variable Comparison.}
%Variables should be represented (\emph{R-86}; $\mathcal{SL}_{1}$) either as (un)ordered code lists or as unions of datatypes.
%In case of a code list, associated categories (human-readable labels) may be stated (\emph{R-71}; $\mathcal{SL}_{0}$).
%The variable \emph{Education at pre-school}, e.g., is represented as ordered code list without any categories.
%If a {\em skos:Concept} represents a code (having {\em skos:notation} and {\em skos:prefLabel} properties), 
%then the property {\em disco:isValid} has to be stated indicating if the code stands for valid (\emph{true}) or missing (\emph{false}) cases (\emph{R-71}; $\mathcal{SL}_{2}$).
%Variables may have at least one relationship to a theoretical concept ({\emph{R-86}; $\mathcal{SL}_{0}$).
%%The variables \emph{number of school years} and \emph{highest educational degree}, e.g., are associated with the theoretical concept \emph{education}. 
%The variable \emph{Education at pre-school},.e.g, is associated with the theoretical concept \emph{Child Care}. 
%The default severity level of this constraint is weak, as in most cases research can be continued without associated theoretical concepts.
%A very common research question is to compare variables of multiple studies or countries (\emph{comparison}).
%To compare variables, 
%(1) variables and (2) variable definitions must be present,
%(3) code lists must be structured properly,
%(4) for each code an associated category (human-readable label) must be specified, and
%(5) code lists must either be identical or at least similar.
%If a researcher wants to get a first overview over comparable variables (use case 1), 
%covering the first three constraints may be sufficient for this purpose.
%Thus, the severity level of the first three constraints is stronger ($\mathcal{SL}_{2}$) than the severity level of the next two constraints ($\mathcal{SL}_{1}$ and $\mathcal{SL}_{0}$).
%If the intention of the researcher is to perform more detailed comparisons (use case 2), however, the violation of the remaining two constraints is getting more serious.
%
%\textbf{Descriptive Statistics.}
%The property \emph{disco:percentage} stands for the number of cases of a given code in relation to the total number of cases for a particular variable within a data set.
%Percentage values are only valid when they are within the \emph{literal range} of 0 and 100 (\emph{R-45}; $\mathcal{SL}_{2}$).
%{\em Mathematical Operations} (\emph{R-41, R-42}; e.g. date calculations and statistical computations like average, mean, and sum) are performed to ensure the integrity of data models.
%The sum of percentage values of all codes of a variable code list, e.g., must exactly be 100 ($\mathcal{SL}_{2}$)
%and the minimum of all variable codes do not have to be greater than the maximum ($\mathcal{SL}_{2}$).
%Codes (\emph{skos:Concept}) are ordered and therefore have fixed positions in an ordered collection (\emph{skos:OrderedCollection}) within variable representations.
%In order to check the correctness of relative frequencies' calculations, the cumulative percentage (\emph{disco:cumulativePercentage}) of the current code must exactly be the cumulative percentage of the previous code
%plus the percentage value (\emph{disco:percentage}) of the current code (\emph{data model consistency}; $\mathcal{SL}_{2}$).
%
%\textbf{Unique Identification.}
%It is often useful to declare a given (data) property as the \emph{primary key} (\emph{R-226}) of a class, so that a system can enforce uniqueness and also automatically build URIs from user inputs and imported data. 
%In \emph{Disco}, resources are uniquely identified by the property \emph{adms:identifier},
%which is therefore inverse-functional
%$(\ms{funct identifier}\sp{\overline{\ }})$,
%i.e. for each \emph{rdfs:Resource x}, there can be at most one distinct \emph{rdfs:Resource y} such that \emph{y} is connected by \emph{adms:identifier$\sp{\overline{\ }}$} to \emph{x} ($\mathcal{SL}_{2}$).
%Keys, however, are even more general than inverse-functional properties (\emph{R-58}),
%as a key can be a data, an object property, or a chain of properties \cite{Schneider2009}.
%For this generalization purposes, as there are different sorts of key, and as keys can lead to undecidability, 
%DL is extended with \emph{key boxes} and a special \emph{keyfor} construct (\ms{identifier \ms{keyfor} Resource}) \cite{Lutz2005}.
%OWL 2 \emph{hasKey} implements \emph{keyfor} ($\mathcal{SL}_{2}$) and thus can be used to identify resources uniquely, to merge resources with identical key property values, and to recognize constraint violations.
%%OWL 2 hasKey can be used to identify resources uniquely
%%
%%We used Protégé 5.
%%
%%example: owl:Thing owl:hasKey ( :hasSSN ) . :Peter :hasSSN "123-45-6789" .
%%:Peter_Griffin :hasSSN "123-45-6789" .
%%
%%We use the predefined Reasoner HermiT.
%%
%%:Peter and :Peter_Griffin are derived as identical from the reasoner as they have the same value for :hasSSN.
%%
%%hasKey can be used to merge resources and to recognize constraint violations.
%%
%%Alternative to OWL 2 hasKey: concise bounded description (proposal of Dan)
%%
%%action: explore CBD document solution might be using both approaches
%%
%%benefit: additional identification to URI / IRI benefit: compound properties can be used as key
%
%
%\textbf{Membership in Controlled Vocabularies.}
%In many cases, resources must be members of controlled vocabularies (\emph{R-32}).
%If a dimension property, e.g., has a \emph{qb:codeList},
%then the value of the dimension property on every \emph{qb:Observation} must be in the code list ($\mathcal{SL}_{2}$).
%Summary statistics types like minimum, maximum, and arithmetic mean are maintained within a controlled vocabulary.  
%Summary statistics can only have \emph{disco:summaryStatisticType} relationships to \emph{skos:Concept}s which must be members of the controlled vocabulary \emph{ddicv:SummaryStatisticType}, a \emph{skos:ConceptScheme} ($\mathcal{SL}_{2}$).
%
%%\begin{center}
%%\begin{DL}
%%SummaryStatistics $\sqsubseteq$ $\forall summaryStatisticType.A$ \\
%%$A \equiv Concept \sqcap \forall inScheme . B$ \\
%%$B \equiv ConceptScheme \sqcap \{SummaryStatisticType\}$
%%\end{DL}
%%\end{center}
%
%\textbf{Coverage.}
%Information about the temporal (\emph{dcterms:temporal}), the spatial (\emph{dcterms:spatial}), and the topical coverage (\emph{dcterms:subject}) of series, studies, data sets, and data files (\emph{R-86}; $\mathcal{SL}_{1}$) is of interest when performing frequently formulated queries 
	%(e.g. to search for all data sets of given years (temporal coverage) in which data is collected in certain countries (spatial coverage) about particular topics (topical coverage)).
%Depending on property datatypes,
%two different literal values have
%a specific ordering with respect to an operator like \textless (\emph{R-43: literal value comparison}).
%Start dates (\emph{disco:startDate}), e.g., must be before (\emph{\textless}) end dates (\emph{disco:endDate}) ($\mathcal{SL}_{2}$).
%
%%\textbf{Organizations, Hierarchies, Classifications, and Ordering.}
%\textbf{Hierarchies and Ordering.}
%SKOS is based on RDF, which is a graph-based data model. Therefore, we can concentrate on the vocabulary's graph-based structure for assessing the quality of SKOS vocabularies and apply graph- and network-analysis techniques (\emph{structure}) like 
%(1) a vocabulary should provide entry points (top concepts) to the data to provide efficient access and guidance for human users,
%(2) concepts, internal to the tree, should not be indicated as top concepts, and
%(3) a vocabulary should not contain many orphan concepts 
%(concepts without any associative or hierarchical relations) lacking valuable context information. A controlled vocabulary that contains many orphan concepts is less usable for search and retrieval use cases.
%%, as, e.g., no hierarchical query expansion can be performed on search terms to find documents with more general content.) \cite{MaderHaslhoferIsaac2012}. 
%Objects and literals can be \emph{ordered} (\emph{R-121, R-217}) for given properties.
%\emph{Disco }variables, questions, and codes/categories are typically organized in a particular order. 
%If a variable code list should be ordered, the variable representation should be of the type \emph{skos:OrderedCollection} containing multiple codes/categories (each represented as \emph{skos:Concept}) in a \emph{skos:memberList}. 
%
%\textbf{Reusability.}
%Within the context of \emph{Disco}, \emph{skos:Concept}s can have either \emph{skos:definition} (when interpreted as theoretical concepts) or \emph{skos:notation} and \emph{skos:prefLabel} properties (when interpreted as codes/categories), but not both ($\mathcal{SL}_{2}$).
%The constraint type \emph{context-specific exclusive or of property groups} (\emph{R-11})
%restricts individuals of given classes to have exactly one of multiple property groups.

\section{Implementation}
\label{implementation}

SPARQL is generally seen as the method of choice to validate RDF data according to certain constraints.
We use \emph{SPIN}, 
a SPARQL-based way to formulate and check constraints, as basis to develop a
validation environment (available at \url{http://purl.org/net/rdfval-demo})\footnote{Source code downloadable at: \url{https://github.com/boschthomas/rdf-validator}} to validate RDF data according to constraints expressed my arbitrary constraint languages like Shape Expressions,
%\footnote{\url{http://www.w3.org/Submission/shex-primer/}}
Resource Shapes, and the Web Ontology Language\footnote{SPIN mappings available at: \url{https://github.com/boschthomas/rdf-validation/tree/master/SPIN}} \cite{BoschEckert2014-2}.
The \emph{RDF Validator} also validates RDF data to ensure correct syntax, semantics, and integrity of diverse vocabularies such as \emph{Disco}, \emph{QB}, \emph{SKOS}, and \emph{PHDD}.
Although accessible within our validation tool, we provide all implemented constraints\footnote{\url{https://github.com/boschthomas/rdf-validation/tree/master/constraints}} in form of SPARQL CONSTRUCT queries.
For the subsequent evaluation, we implemented 213 constraints on \emph{Disco}, \emph{QB}, \emph{SKOS}, and \emph{PHDD} data sets.
The SPIN engine checks for each resource if it satisfies all constraints, which are associated with its assigned classes, and generates a result RDF graph containing information about all constraint violations.
There is one SPIN construct template for each constraint type and vocabulary-specific constraint\footnote{For a comprehensive description of the \emph{RDF Validator}, we refer to \cite{BoschEckert2014-2}}.
A SPIN construct template contains a SPARQL CONSTRUCT query which generates constraint violation triples indicating the subject and the properties causing constraint violations, and the reason why constraint violations have been raised.
A SPIN construct template creates constraint violation triples if all triple patterns within the SPARQL WHERE clause match.
\emph{Missy}\footnote{\url{http://www.gesis.org/missy/eu/missy-home}} provides comprehensive Linked Data services like diverse RDF exports of person-level metadata conforming to the \emph{Disco} vocabulary in form of multiple concrete syntaxes. 

\section{Evaluation}
\label{evaluation}

\subsection{Evaluation Setup}

	%\item First, we assigned each constraint type to exactly one of the disjoint sets of constraint types 
%in order to get an overview how many constraint types in relation to the total amount of constraint types are extractable from vocabularies ($\mathcal{CT}_{B}$), are easily definable ($\mathcal{CT}_{S}$), and are rather difficult to specify ($\mathcal{CT}_{C}$). 
In close collaboration with several \emph{SBE} domain experts, we defined 246 constraint 
of 53 different types on six vocabularies (\emph{Disco}, \emph{QB}, \emph{SKOS}, \emph{PHDD}, \emph{DCAT}, \emph{XKOS}) and classified them according to the complexity level of their type and their severity level. 
\ke{Alle vokabulare in section "vocabularies" eingeführt?}\tb{mache ich}
For 114 of these constraints, we evaluated the data quality of 15,694 data sets (4,26 billion triples) of \emph{SBE} research data on three common vocabularies in \emph{SBE} sciences (\emph{Disco}, \emph{QB}, \emph{SKOS}) obtained from 33 SPARQL endpoints.
We distinct two classes of vocabularies: 
(1) well-established vocabularies (e.g., \emph{QB}, \emph{SKOS}) which are widely adopted and accepted and 
(2) newly developed vocabularies (e.g., \emph{Disco} which will be published in 2015) which are either recently published or are still in the publication process.
%It is likely that such a vocabulary is still subject of constant change, that published data may not be consistent with its current version,
%and that early adopters did not properly understand its formal specification.
Based on this evaluation we formulate hypotheses.
Nevertheless, we have to be careful making general statements for all vocabularies 
as these hypotheses still have to be verified or falsified in terms of future work 
by evaluating the quality of data represented by additional well-established and newly developed vocabularies.
\ke{das ist zu früh finde um schon einzuschränken, erst mal ergebnisse, dann hypothesen, dann kritische auseinandersetzung}

We validated 
9,990 / 3,775,983,610 (\emph{QB}),
4,178 / 477,737,281 (\emph{SKOS}), and 
1,526 / 9,673,055 (\emph{Disco}) data sets / triples using the \emph{RDF Validator} in batch mode.
We validated, i.a., 
(1) \emph{QB} data sets published by the \emph{Australian Bureau of Statistics (ABS)},
the \emph{European Central Bank (ECB)}, and the
\emph{Organisation for Economic Co-operation and Development (OECD)},
(2) \emph{SKOS} thesauri like the \emph{AGROVOC Multilingual agricultural thesaurus},
the \emph{STW Thesaurus for Economics}, and the
\emph{Thesaurus for the Social Sciences (TheSoz)}, and
(3) \emph{Disco} data sets provided by the \emph{Microdata Information System (Missy)}, 
the \emph{DwB Discovery Portal}, the
\emph{Danish Data Archive (DDA)}, and the
\emph{Swedish National Data Service (SND)}.
We recently published a technical report\footnote{\label{technical-report-2}Available at: \url{http://arxiv.org/abs/1504.04478}} (serving as second appendix of this paper) 
in which we describe the evaluation in detail \cite{BoschZapilkoWackerowEckert2015-2}. 
As we evaluated nearly 10 thousand \emph{QB} data sets, we published the evaluation results for each data set in form of one document per SPARQL endpoint\footnote{Available at: \url{https://github.com/boschthomas/rdf-validation/tree/master/evaluation/data-sets/data-cube}}.

\subsection{Evaluation Results and Formulation of Hypotheses}

%DCAT: 11 constraints defined
%PHDD: 12
%XKOS: 10

%The majority (48 $\equiv$ 58.5\%) of the overall 82 $\mathcal{CT}$ constraint types are $\mathcal{CT}_{B}$ whose constraints can therefore be derived from vocabularies without any effort.
%Among $\mathcal{CT}_{B}$, two-thirds (34 $\equiv$ 70.8\%) are $\mathcal{C}_B ^{\mathcal{R}}$, i.e., constraint types for which reasoning may be performed prior to validation, and one third (14 $\equiv$ 29.2\%) are $\overline{\mathcal{C}_B ^{\mathcal{R}}}$, i.e., constraint types for which reasoning does not make any sense.
%A quarter (20 $\equiv$ 24.4\%) of all constraint types are $\mathcal{CT}_{S}$ and a sixth (14 $\equiv$ 17.1\%) are $\mathcal{CT}_{C}$.
%
%\begin{table}[H]
		%\scriptsize
    %\begin{center}
    %\begin{tabular}{@{}lcccc@{}}
%%           & \multicolumn{6}{c}{\textbf{Vocabularies}}
%%    \\  \cmidrule{2-7}
    %\\       \textbf{Criteria}
           %& \textbf{\emph{Disco}}
           %& \textbf{\emph{QB}}
					 %& \textbf{\emph{SKOS}}
					 %& \textbf{Total}
    %\\ \midrule
		%%\emph{Triples} & 9,673,055 & 3,775,983,610 & 477,737,281 & 4,263,393,946 \\
		%%\emph{Data Sets} & 1,526 & 9,990 & 4,178 & 15,694 \\
		%%\hline
		%\emph{CT} & 52 & 20 & 15 & 53 \\
		%\hline
		%$\mathcal{CT}_{C}$ & 9 (17.3\%) & 2 (10\%) & 2 (13.3\%) & 9 (17\%) \\
		%$\mathcal{CT}_{S}$ & 16 (30.8\%) & 3 (15\%) & 4 (26,7\%) & 16 (30.2\%) \\
		%$\mathcal{CT}_{B}$ & 27 (\textbf{51.9\%}) & 15 (\textbf{75\%}) & 9 (\textbf{60\%}) & 28 (\textbf{52.8\%}) \\
		%\hline
		%$\mathcal{C}_B ^{\mathcal{R}}$ & 18 (34.6\%) & 9 (45\%) & 4 (26.7\%) & 19 (35.9\%) \\
		%$\overline{\mathcal{C}_B ^{\mathcal{R}}}$ & 9 (17.3\%) & 6 (30\%) & 5 (33.3\%) & 9 (17\%) \\
    %\bottomrule
    %\end{tabular}
    %%\\ \emph{C (constraints), CT (constraint types)}
    %\caption{Evaluation - Constraint Types}
		%\label{tab:evaluation-constraint-types}
    %\end{center}
%\end{table}

%Table \ref{tab:evaluation-constraint-types} displays the evaluation of the metadata quality on real world data sets regarding constraint types.
Table \ref{tab:evaluation-constraint-violations} shows the results of the evaluation, more specifically the constraints and the constraint violations, which are caused by these constraints, in percent.
The constraints and their raised constraint violations are grouped by vocabulary, complexity level of their type, and their severity level.
The number of evaluated triples and data sets differs between the vocabularies
as we evaluated 3.8 billion \emph{QB}, 480 million \emph{SKOS}, and 10 million \emph{Disco} triples.
To be able to formulate hypotheses which apply for all vocabularies, 
we only use normalized relative values representing the percentage of constraints and constraint violations belonging to the respective class

%by building the arithmetic mean over the individual vocabularies' percentage values. 
%\tb{More than 80\% of the overall approx. 55 million constraint violations are caused by \emph{QB} constraints.}

\begin{table}[H]
		\scriptsize
    \begin{center}
    \begin{tabular}{@{}lcccccccc@{}}
    %       & \multicolumn{2}{c}{\textbf{Vocabularies}}
    %\\  \cmidrule{2-3}
		\hline
    \multirow{2}{*}{} &
      \multicolumn{2}{c}{\textbf{\emph{Disco}}} &
      \multicolumn{2}{c}{\textbf{\emph{QB}}} &
      \multicolumn{2}{c}{\textbf{\emph{SKOS}}} &
      \multicolumn{2}{c}{\textbf{\emph{Total}}} \\
    \textbf{} & C & CV & C & CV & C & CV & C & CV \\
    \hline
    %\\ \midrule
		 & 143 & 3,575,002 & 35 & 45,635,861 & 35 & 5,540,988 & 213 & 54,751,851 \\
		\hline
		\textbf{\emph{complex}} & 25.9\% & 18.3\% & 37.1\% & \textbf{100\%} & \textbf{37.1\%} & 21.4\% & 33.4\% & \textbf{46.6\%} \\
		\textbf{\emph{simple}} & 19.6\% & 15.7\% & 8.6\% & 0.0\% & \textbf{34.3\%} & \textbf{78.6\%} & 20.8\% & 31.4\% \\
		\textbf{\emph{vocabulary}} & \textbf{54.6\%} & \textbf{66.1\%} & \textbf{54.3\%} & 0.0\% & \textbf{28.6\%} & 0.0\% & \textbf{45.8\%} & 22.0\% \\
		%\hline
		%\emph{CV (}$\mathcal{C}_B ^{\mathcal{R}}$\emph{)} & 2,333,365 (65.3\%) & 1,777 (0\%) & 0 (0\%) & 2,335,142 (4.3\%) \\
		%\emph{CV (}$\overline{\mathcal{C}_B ^{\mathcal{R}}}$\emph{)} & 28,000 (0.8\%) & 0 (0\%) & 0 (0\%) & 28,000 (0.1\%) \\
		\hline
		\textbf{\emph{info}} & \textbf{52.5\%} & \textbf{52.6\%} & 11.4\% & 0.0\% & \textbf{60.0\%} & 41.2\% & \textbf{41.3\%} & 31.3\% \\
		\textbf{\emph{warning}} & 7.0\% & 29.4\% & 8.6\% & \textbf{99,8\%} & 14.3\% & \textbf{58.8\%} & 10.0\% & \textbf{62.7\%} \\
		\textbf{\emph{error}} & 40.6\% & 18\% & \textbf{80.0\%} & 0.3\% & 25.7\% & 0.0\% & \textbf{48.8\%} & 6.1\%\\
    \bottomrule
    \end{tabular}
    \\ \emph{C (constraints), CV (constraint violations)}
    \caption{Constraints and Constraint Violations}
		\label{tab:evaluation-constraint-violations}
    \end{center}
\end{table}

%More than the half of them are $\mathcal{CT}_{B}$, nearly a third $\mathcal{CT}_{S}$, and only a sixth $\mathcal{CT}_{C}$ constraint types.
%For \emph{Disco}, \emph{QB}, and \emph{SKOS}, more than 50\% of the instantiated constraint types are $\mathcal{CT}_{B}$ constraint types 
%(for \emph{QB} even three quarters).
%\emph{Existential quantifications} (\emph{R-86}, 32.4\%, \emph{Disco}), \emph{data model consistency} (31.4\%, \emph{QB}), and \emph{structure} (28.6\%, \emph{SKOS}) are the constraint types the most constraints are instantiated from.

\tb{Ich würde zu Beginn der Auswertung, nach den Tabellen, erklären, dass wir anhand der drei Vokabulare keine gesicherten allgemeinen Aussagen  machen können, aber dass wir im folgenden verschiedene Hypthesen aufstellen, die auf die Auswertung gestützt sind. / ann also erst die Fakten berichten, wie zum Beispiel bestimmte Prozentwerte / Und dann die Hypothese als Schluss, z..B. dass die Datenqualität bezüglich in den etablierten Vokabularen hinterlegter Constraints sehr gut ist, was darauf hindeutet, dass das hinterlegen von constraints sinnvoll ist und von den datenanbietern auch beachtet wird.
ganz eigentlich ist ja sogar nur das zweite unsere hypothese
die datenqualität in unserem sample IST gut, das ist auch keine hypothese / es muss halt klar werden, dass wir die hypothese im anschluss nicht beweisen, sondern dass wir sie lediglich postulieren, allerdings nicht komplett aus der luft sondern als interpretation unserer ergebnisse / Was zwar kein beweis ist wegen geringer zahl der vokabulare, aber eben auch nicht nix}

Almost 1/2 of all 213 constraints and more than 50\% of the \emph{Disco} and the \emph{QB} constraints are \emph{vocabulary constraints}.
The \emph{SKOS} constraints are equally distributed among the three disjoint sets of constraint types. 
\begin{hyp}
A significant amount of 46 \% of the constraints are directly extractable from formal specifications of well-established and newly defined vocabularies. 
\end{hyp} 

Nearly 1/2 of the violations are caused by \emph{complex constraints}, 1/3 by \emph{simple constraints}, and 1/5 by \emph{vocabulary constraints}.

\begin{hyp}
The fact that only 1/5 of all violations result from vocabulary constraints, 
even though, 46\% of all constraints are vocabulary constraints,
indicates good data quality in general (according to formal specifications of vocabularies).
\end{hyp}

2/3 of the \emph{Disco} violations result from \emph{vocabulary constraints},
almost only \emph{complex constraints} raised \emph{QB} violations, and 
nearly 80\% of the \emph{SKOS} violations are caused by \emph{simple constraints}.

\begin{hyp}
For well-established vocabularies, vocabulary constraints are almost completely satisfied\footnote{e.g. only 1.777 \emph{QB} violations} which indicates good data quality (according to formal specifications of vocabularies) and which demonstrates that constraint formulation in general works.
For newly defined vocabularies, however, 2/3 of all violations are raised by vocabulary constraints
which indicates good data quality (according to formal specifications of vocabularies).
\end{hyp}

It is likely that a newly developed vocabulary is still subject of constant change, 
that published data may not be consistent with its current version,
and that early adopters did not properly understand its formal specification.

\begin{hyp}
A significant amount of 47\% of the violations refer to complex constraints that are not easily expressible in existing languages which confirms the necessity to provide suitable constraint languages.
\end{hyp} 
\emph{Vocabulary} and \emph{simple constraints} can be expressed easily, concisely, and intuitively by either modeling languages (e.g., \emph{RDF}, \emph{RDFS}, \emph{OWL 2}) or constraint languages (e.g., \emph{ShEx}, \emph{ReSh}, \emph{SPIN}). 
This is not the case, however, for \emph{complex constraints} which in most cases still can only be expressed by plain \emph{SPARQL}. 
As almost 1/2 of all violations are caused by \emph{complex constraints}, 
data quality can be significantly improved when \emph{complex constraints} can also be expressed more easily, concisely, and intuitively.
Therefore, existing constraint languages have to be extended or new constraint languages have to be developed.

63\% of all violations are caused by \emph{warning constraints} which are only 10\% of all constraints. 
Only 6\% of the violations are caused by \emph{error constraints} which are 49\% of all constraints.

\begin{hyp}
In general, data quality is high (according to severity levels) since only 6\% of the violations are caused by error constraints which are 49\% of all defined constraints.
\end{hyp} 

\begin{hyp}
The percentage of severe violations is very low (6\%), compared to about 2/3 of warning violations and 1/3 of informational violations, which implies that proper constraint languages can significantly improve the data quality beyond fundamental requirements.
\end{hyp} 

\begin{table}[H]
		\scriptsize
    \begin{center}
    \begin{tabular}{@{}lcccccccc@{}}
    %       & \multicolumn{2}{c}{\textbf{Vocabularies}}
    %\\  \cmidrule{2-3}
		\hline
    \multirow{2}{*}{} &
      \multicolumn{2}{c}{\textbf{\emph{Disco}}} &
      \multicolumn{2}{c}{\textbf{\emph{QB}}} &
      \multicolumn{2}{c}{\textbf{\emph{SKOS}}} &
      \multicolumn{2}{c}{\textbf{\emph{Total}}} \\
    \textbf{} & C & CV & C & CV & C & CV & C & CV \\
    \hline
    %\\ \midrule
		 & 143 & 3,575,002 & 35 & 45,635,861 & 35 & 5,540,988 & 213 & 54,751,851 \\
		\hline
		\textbf{\emph{complex}} & 25.9\% & 18.3\% & 37.1\% & \textbf{100\%} & \textbf{37.1\%} & 21.4\% & 33.4\% & \textbf{46.6\%} \\
		\textbf{\emph{simple}} & 19.6\% & 15.7\% & 8.6\% & 0.0\% & \textbf{34.3\%} & \textbf{78.6\%} & 20.8\% & 31.4\% \\
		\textbf{\emph{vocabulary}} & \textbf{54.6\%} & \textbf{66.1\%} & \textbf{54.3\%} & 0.0\% & \textbf{28.6\%} & 0.0\% & \textbf{45.8\%} & 22.0\% \\
		%\hline
		%\emph{CV (}$\mathcal{C}_B ^{\mathcal{R}}$\emph{)} & 2,333,365 (65.3\%) & 1,777 (0\%) & 0 (0\%) & 2,335,142 (4.3\%) \\
		%\emph{CV (}$\overline{\mathcal{C}_B ^{\mathcal{R}}}$\emph{)} & 28,000 (0.8\%) & 0 (0\%) & 0 (0\%) & 28,000 (0.1\%) \\
		\hline
		\textbf{\emph{info}} & \textbf{52.5\%} & \textbf{52.6\%} & 11.4\% & 0.0\% & \textbf{60.0\%} & 41.2\% & \textbf{41.3\%} & 31.3\% \\
		\textbf{\emph{warning}} & 7.0\% & 29.4\% & 8.6\% & \textbf{99,8\%} & 14.3\% & \textbf{58.8\%} & 10.0\% & \textbf{62.7\%} \\
		\textbf{\emph{error}} & 40.6\% & 18\% & \textbf{80.0\%} & 0.3\% & 25.7\% & 0.0\% & \textbf{48.8\%} & 6.1\%\\
    \bottomrule
    \end{tabular}
    \\ \emph{C (constraints), CV (constraint violations)}
    \caption{Constraints and Constraint Violations}
		\label{tab:evaluation-constraint-violations}
    \end{center}
\end{table}

80\% of the \emph{QB} constraints are error constraints.
More than 50\% of the \emph{Disco} and \emph{SKOS} constraints, however, are informational constraints.
1/6 of the Disco violations are caused by error constraints.
Almost all \emph{QB} violations and 59\% of the \emph{SKOS} violations are caused by warning constraints.  
\begin{hyp}
For well-established vocabularies, data quality is high
as serious violations rarely appear.
For newly developed vocabularies, data quality is worse
as serious violations occur partially.
\end{hyp} 

%For the vocabularies \emph{Disco}, \emph{QB}, and \emph{SKOS}, we defined 213 constraints of 53 constraint types .

%\begin{table}[H]
		%\scriptsize
    %\begin{center}
    %\begin{tabular}{@{}lcccc@{}}
%%           & \multicolumn{6}{c}{\textbf{Vocabularies}}
%%    \\  \cmidrule{2-7}
    %\\       \textbf{Criteria}
           %& \textbf{\emph{Disco}}
           %& \textbf{\emph{QB}}
					 %& \textbf{\emph{SKOS}}
					 %& \textbf{Total}
    %\\ \midrule
		%%\emph{Triples} & 9,673,055 & 3,775,983,610 & 477,737,281 & 4,263,393,946 \\
		%%\emph{Data Sets} & 1,526 & 9,990 & 4,178 & 15,694 \\
		%%\hline
		%\emph{C} & 143 (67.1\%) & 35 (16.4\%) & 35 (16.4\%) & 213 \\
		%\hline
		%\emph{C (}$\mathcal{CT}_{C}$\emph{)} & 37 (25.9\%) & 13 (37.1\%) & 13 (\textbf{37.1\%}) & 63 (29.6\%) \\
		%\emph{C (}$\mathcal{CT}_{S}$\emph{)} & 28 (19.6\%) & 3 (8.6\%) & 12 (\textbf{34.3\%}) & 43 (20.2\%) \\
		%\emph{C (}$\mathcal{CT}_{B}$\emph{)} & 78 (\textbf{54.6\%}) & 19 (\textbf{54.3\%}) & 10 (\textbf{28.6\%}) & 107 (\textbf{50.2\%}) \\
		%%\hline
		%%\emph{C (}$\mathcal{C}_B ^{\mathcal{R}}$\emph{)} & 67 (46.9\%) & 13 (37.1\%) & 4 (11.4\%) & 84 (39.4\%) \\
		%%\emph{C (}$\overline{\mathcal{C}_B ^{\mathcal{R}}}$\emph{)} & 11 (7.7\%) & 6 (17.1\%) & 6 (17.1\%) & 23 (10.8\%) \\
		%\hline
		%\emph{C ($\mathcal{SL}_{0}$)} & 75 (\textbf{52.5\%}) & 4 (11.4\%) & 21 (\textbf{60\%}) & 100 (\textbf{46.9\%}) \\
		%\emph{C ($\mathcal{SL}_{1}$)} & 10 (7\%) & 3 (8.6\%) & 5 (14.3\%) & 18 (8.5\%) \\
		%\emph{C ($\mathcal{SL}_{2}$)} & 58 (40.6\%) & 28 (\textbf{80\%}) & 9 (25.7\%) & 95 (\textbf{44.6\%}) \\
    %\bottomrule
    %\end{tabular}
    %%\\ \emph{C (constraints), CT (constraint types)}
    %\caption{Evaluation - Constraints}
		%\label{tab:evaluation-constraints}
    %\end{center}
%\end{table}





%\begin{table}[H]
		%\scriptsize
    %\begin{center}
    %\begin{tabular}{@{}lcccccccc@{}}
%%           & \multicolumn{6}{c}{\textbf{Vocabularies}}
%%    \\  \cmidrule{2-7}
    %\\       \textbf{}
           %& \textbf{\emph{Disco}}
           %& \textbf{\emph{QB}}
					 %& \textbf{\emph{SKOS}}
					 %& \textbf{Total}
					 %& \textbf{\emph{Disco}}
           %& \textbf{\emph{QB}}
					 %& \textbf{\emph{SKOS}}
					 %& \textbf{Total}
    %\\ \midrule
		 %& 3,575,002 & 45,635,861 & 5,540,988 & 54,751,851 & & & &  \\
		%\hline
		%\textbf{\emph{complex}} & 18.3\% & \textbf{100\%} & 21.4\% & \textbf{86.7\%} & & & & \\
		%\textbf{\emph{simple}} & 15.7\% & 0.0\% & \textbf{78.6\%} & 9\% & & & & \\
		%\textbf{\emph{vocabulary}} & \textbf{66.1\%} & 0.0\% & 0\% & 4.3\% & & & & \\
		%%\hline
		%%\emph{CV (}$\mathcal{C}_B ^{\mathcal{R}}$\emph{)} & 2,333,365 (65.3\%) & 1,777 (0\%) & 0 (0\%) & 2,335,142 (4.3\%) \\
		%%\emph{CV (}$\overline{\mathcal{C}_B ^{\mathcal{R}}}$\emph{)} & 28,000 (0.8\%) & 0 (0\%) & 0 (0\%) & 28,000 (0.1\%) \\
		%\hline
		%\textbf{\emph{info}} & \textbf{52.6\%} & 0.0\% & 41.2\% & 7.6\% & & & & \\
		%\textbf{\emph{warning}} & 29.4\% & \textbf{99,8\%} & \textbf{58.8\%} & \textbf{91\%} & & & & \\
		%\textbf{\emph{error}} & 18\% & 0.3\% & 0.0\% & 1.4\% & & & & \\
    %\bottomrule
    %\end{tabular}
    %%\\ \emph{CV (constraint violations)}
    %\caption{Constraint Violations}
		%\label{tab:evaluation-constraint-violations}
    %\end{center}
%\end{table}

%The constraints responsible for the largest amounts of constraint violations are \emph{DISCO-C-LABELING-AND-DOCUMENTATION-06} and \emph{DISCO-C-COMPARISON-VARIABLES-02} (both 547,916; \emph{Disco}), \emph{DATA-CUBE-C-DATA-MODEL-CONSISTENCY-05} (45,514,102; \emph{QB}), and \emph{SKOS-C-LANGUAGE-TAG-CARDINALITY-01} (2,508,903; \emph{SKOS}).

Table \ref{tab:evaluation-complexity-severity} shows the relation between the complexity and the severity level of all 213 constraints.

\begin{table}[H]
		\scriptsize
    \begin{center}
    \begin{tabular}{@{}lcccc@{}}
%           & \multicolumn{6}{c}{\textbf{Vocabularies}}
%    \\  \cmidrule{2-7}
    \\       \textbf{}
           & \textbf{\emph{vocabulary}}
           & \textbf{\emph{simple}}
					 & \textbf{\emph{complex}}
    \\ \midrule
		%\emph{Triples} & 9,673,055 & 3,775,983,610 & 477,737,281 & 4,263,393,946 \\
		%\emph{Data Sets} & 1,526 & 9,990 & 4,178 & 15,694 \\
		%\hline
		\textbf{\emph{info}} & 38.7 \% & \textbf{76.2 \%} & \textbf{42.3 \%} \\
		\textbf{\emph{warning}} & 6.7 \% & 7.1 \% & 13.5 \% \\
		\textbf{\emph{error}} & \textbf{54.6 \%} & 16.7 \% & \textbf{44.2 \%} \\
    \bottomrule
    \end{tabular}
    %\\ \emph{C (constraints), CT (constraint types)}
    \caption{Complexity vs. Severity of Constraints}
		\label{tab:evaluation-complexity-severity}
    \end{center}
\end{table}

\begin{hyp}
More than the 1/2 of the vocabulary constraints are error constraints,
more than 3/4 of the simple constraints are informational constraints, and
complex constraints are either informational or error constraints. 
\end{hyp}

As vocabulary constraints are directly extractable from vocabularies and therefore ensure the syntax and semantics of vocabularies,
violations of vocabulary constraints are more severe than violations caused by simple constraints which are easily defined in addition to vocabulary constraints.
In many cases, complex constraints are also needed to express severe constraints which cannot be expressed by modeling languages.
The relation between complexity and severity shows the importance to develop constraint languages which are enable to express severe complex constraints. 

\section{Conclusion and Future Work}

\tb{ToDO}

%In this paper, we showed in form of a complete real world running example how to represent metadata on person-level data (\emph{Disco}), metadata on aggregated data (\emph{QB}), and data on both aggregation levels in a rectangular format (\emph{PHDD}) in RDF and how therefore used vocabularies are interrelated (\textbf{contribution 1}, section \ref{rdf-representation}).
%We explained why RDF validation is important in this context and how metadata on person-level data, aggregated data, thesauri, and statistical classifications as well as data on both aggregation levels is validated against constraints to ensure high (meta)data quality\footnote{The first appendix of this paper describing each constraint in detail is available at: \url{http://arxiv.org/abs/1504.04479} \cite{BoschZapilkoWackerowEckert2015}} (\textbf{contribution 2}, section \ref{rdf-validation}). 
%We distinguish two validation types:
%(1) \emph{Content-Driven Validation} $\mathcal{C}_{C}$ contains the set of constraints ensuring that the data is consistent with the intended syntax, semantics, and integrity of data models (section \ref{complex-constraint-types}).
%(2) \emph{Technology-Driven Validation} $\mathcal{C}_{T}$ includes the set of constraints which can be generated automatically out of data models, such as cardinality restrictions, universal and existential quantifications, domains, and ranges (section \ref{vocabulary-constraint-types}).
%We determined the default \emph{severity level} for each constraint to indicate how serious the violation of the constraint is
%and propose an extensible metric to measure the continuum of severity levels.

\footnote{The first appendix of this paper describing each constraint in detail is available at: \url{http://arxiv.org/abs/1504.04479} \cite{BoschZapilkoWackerowEckert2015}}

We implemented a validation environment (available at \url{http://purl.org/net/rdfval-demo}) to validate RDF data according to constraints expressed my arbitrary constraint languages and to ensure correct syntax and semantics of diverse vocabularies such as \emph{Disco}, \emph{QB}, \emph{SKOS}, and \emph{PHDD} (section \ref{implementation}).
We exhaustively evaluated the metadata quality of large real world aggregated (\emph{QB}), person-level (\emph{Disco}), and thesauri (\emph{SKOS}) data sets (more than 4.2 billion triples and 15 thousand data sets) by means of 213  constraints of the majority of the constraint types\footnote{The second appendix of this paper describing the evaluation in detail is available at: \url{http://arxiv.org/abs/1504.04478} \cite{BoschZapilkoWackerowEckert2015-2}.} (section \ref{evaluation}).

\tb{
die vokabular-beschreibungen klarer strukturieren. du nutzt plötzlich disco elemente, disco wird überhaupt nicht vernünftig eingeführt, obwohl es später untersucht wird
alles, was nicht der aussage des papers (neuer titel, btw) dient, solltest du weglassen
klare, einfache struktur: 1. grundidee (constraints identifizieren, validieren auf datensets, zu belastbaren aussagen kommen für die weitere entwicklung von constraint languages)
2. vorstellung der untersuchten vokabulare
3. beschreiben, wo die constraints herkommen, bzw. wie die generiert wurden. dabei auch auf die klassifikation eingehen, aber nur im hinblick auf die spätere auswertung, nicht als main contribution des papers
4. evaluation mit beschreibung der datasets in setup (das les ich mir gleich mal durch, das hast du ja schon neu geschrieben)
ja, und dann prägnante conclusion mit zusammenfassung der hypothesen, der bedutung für die weitere entwicklung von CLs und natürlich future work
}

\bibliography{../../literature/literature}{}
\bibliographystyle{plain}
\setcounter{tocdepth}{1}
%\listoftodos
\end{document}
