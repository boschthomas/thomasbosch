% This is LLNCS.DEM the demonstration file of
% the LaTeX macro package from Springer-Verlag
% for Lecture Notes in Computer Science,
% version 2.4 for LaTeX2e as of 16. April 2010
%
\documentclass{llncs}

% allows for temporary adjustment of side margins
\usepackage{chngpage}

% just makes the table prettier (see \toprule, \bottomrule, etc. commands below)
\usepackage{booktabs}

\usepackage[utf8]{inputenc}
%\usepackage[font=small,skip=0pt]{caption}

% footnotes
\usepackage{scrextend}

% colors
\usepackage[usenames, dvipsnames]{color}

% underline
\usepackage{tikz}
\newcommand{\udensdot}[1]{%
    \tikz[baseline=(todotted.base)]{
        \node[inner sep=1pt,outer sep=0pt] (todotted) {#1};
        \draw[densely dotted] (todotted.south west) -- (todotted.south east);
    }%
}%

\newcommand{\uloosdot}[1]{%
    \tikz[baseline=(todotted.base)]{
        \node[inner sep=1pt,outer sep=0pt] (todotted) {#1};
        \draw[loosely dotted] (todotted.south west) -- (todotted.south east);
    }%
}%

\newcommand{\udash}[1]{%
    \tikz[baseline=(todotted.base)]{
        \node[inner sep=1pt,outer sep=0pt] (todotted) {#1};
        \draw[dashed] (todotted.south west) -- (todotted.south east);
    }%
}%

\newcommand{\udensdash}[1]{%
    \tikz[baseline=(todotted.base)]{
        \node[inner sep=1pt,outer sep=0pt] (todotted) {#1};
        \draw[densely dashed] (todotted.south west) -- (todotted.south east);
    }%
}%

\newcommand{\uloosdash}[1]{%
    \tikz[baseline=(todotted.base)]{
        \node[inner sep=1pt,outer sep=0pt] (todotted) {#1};
        \draw[loosely dashed] (todotted.south west) -- (todotted.south east);
    }%
}%

% URL handling
\usepackage{url}
\urlstyle{same}

%\usepackage{makeidx}  % allows for indexgeneration

%\usepackage{amsmath}
\usepackage{amsmath, amssymb}
\usepackage{mathabx}
\usepackage{caption} 
\captionsetup[table]{skip=10pt}

% monospace within text
\newcommand{\ms}[1]{\texttt{#1}}

% examples
\usepackage{fancyvrb}
\DefineVerbatimEnvironment{ex}{Verbatim}{numbers=left,numbersep=2mm,frame=single,fontsize=\scriptsize}

\usepackage{xspace}
% Einfache und doppelte Anfuehrungszeichen
\newcommand{\qs}{``} 
\newcommand{\qe}{''\xspace} 
\newcommand{\sqs}{`} 
\newcommand{\sqe}{'\xspace} 

% checkmark
\usepackage{tikz}
\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;} 

% Xs
\usepackage{pifont}

% Tabellenabstände kleiner
\setlength{\intextsep}{10pt} % Vertical space above & below [h] floats
\setlength{\textfloatsep}{10pt} % Vertical space below (above) [t] ([b]) floats
% \setlength{\abovecaptionskip}{0pt}
% \setlength{\belowcaptionskip}{0pt}

\usepackage{tabularx}
\newcommand{\hr}{\hline\noalign{\smallskip}} % für die horizontalen linien in tabellen

% Todos
\usepackage[colorinlistoftodos]{todonotes}
\newcommand{\ke}[1]{\todo[size=\small, color=orange!40]{\textbf{Kai:} #1}}
\newcommand{\tb}[1]{\todo[size=\small, color=green!40]{\textbf{Thomas:} #1}}
\newcommand{\er}[1]{\todo[size=\small, color=red!40]{\textbf{Erman:} #1}}
\newcommand{\an}[1]{\todo[size=\small, color=blue!40]{\textbf{Andy:} #1}}

\newenvironment{table-1cols}{
  \scriptsize
  \sffamily
  \vspace{0.3cm}
  \begin{tabular}{l}
  \hline
  \textbf{Requirements} \\
  \hline

}{
  \hline
  \end{tabular}
  \linebreak
}

\newenvironment{table-2cols}{
  \scriptsize
  \sffamily
  \vspace{0.3cm}
  \begin{tabular}{l|l}
  \hline
  \textbf{Requirements} & \textbf{Covering DSCLs} \\
  \hline

}{
  \hline
  \end{tabular}
  \linebreak
}

\newenvironment{complexity}{
  %\scriptsize
  %\sffamily
  %\vspace{0.3cm}
  \begin{tabular}{l|l}
  \hline
  \textbf{Complexity Class} & \textbf{Complexity} \\
  \hline

}{
  \hline
  \end{tabular}
  \linebreak
}

\newenvironment{DL}{
  %\scriptsize
  %\sffamily
  \vspace{0cm}
  \begin{tabular}{r l}

}{
  \end{tabular}
  %\linebreak
}


\newenvironment{evaluation}{
  %\scriptsize
  %\sffamily
  %\vspace{0.3cm}
  \begin{tabular}{l|c|c|c|c|c|c}
  \hline
  \textbf{Constraint Class} & \textbf{DSP} & \textbf{OWL2-DL} & \textbf{OWL2-QL} & \textbf{ReSh} & \textbf{ShEx} & \textbf{SPIN} \\
  \hline

}{
  \hline
  \end{tabular}
  \linebreak
}

\newenvironment{constraint-languages-complexity}{
  %\scriptsize
  %\sffamily
  %\vspace{0.3cm}
  \begin{tabular}{l|c|c|c|c|c|c}
  \hline
  \textbf{Complexity Class} & \textbf{DSP} & \textbf{OWL2-DL} & \textbf{OWL2-QL} & \textbf{ReSh} & \textbf{ShEx} & \textbf{SPIN} \\
  \hline

}{
  \hline
  \end{tabular}
  \linebreak
}

\newenvironment{user-fiendliness}{
  %\scriptsize
  %\sffamily
  %\vspace{0.3cm}
  \begin{tabular}{l|c|c|c|c|c}
  \hline
  \textbf{criterion} & \textbf{DSP} & \textbf{OWL2} & \textbf{ReSh} & \textbf{ShEx} & \textbf{SPIN} \\
  \hline

}{
  \hline
  \end{tabular}
  \linebreak
}

\setcounter{secnumdepth}{5}

% tables
\usepackage{array,graphicx}
\usepackage{booktabs}
\usepackage{pifont}
\newcommand*\rot{\rotatebox{90}}
\newcommand*\OK{\ding{51}}
\usepackage{booktabs}
\newcommand*\ON[0]{$\surd$}

\usepackage{tablefootnote}

\usepackage{float}

\begin{document}
\renewcommand{\arraystretch}{1.3}
%
%
\title{RDF Validation of Metadata \\ on Person-Level and Aggregated Data}
\subtitle{}

\titlerunning{XXXXX}  % abbreviated title (for running head)
%                                     also used for the TOC unless
%                                     \toctitle is used
%
\author{Thomas Bosch\inst{1} \and Benjamin Zapilko\inst{1} \and Joachim Wackerow\inst{1} \and Kai Eckert\inst{2}}
%
\authorrunning{} % abbreviated author list (for running head)
%
%%%% list of authors for the TOC (use if author list has to be modified)
\institute{GESIS – Leibniz Institute for the Social Sciences, Germany\\
\email{\{firstname.lastname\}@gesis.org},\\ 
\and
University of Mannheim, Germany \\
\email{kai@informatik.uni-mannheim.de} 
}

\maketitle              % typeset the title of the contribution

\begin{abstract}
For research institutes, data libraries, and data archives,
RDF data validation according to predefined constraints is a much sought-after feature, 
particularly as this is taken for granted in the XML world.
The data most often used in research within the community around research data for the social, behavioural, and economic sciences (\emph{SBE}) is person-level data, i.e. data collected about individuals (households, businesses). 
%The DDI-RDF Discovery Vocabulary is used to describe and to discover this kind of research data.
While performing research, the detailed, often access restricted person-level
data is aggregated into less confidential publicly available multi-dimensional tables which answer particular research questions and whose purpose is to gain an interest in further more detailed analyses on the underlying person-level data.
To ensure high quality and trust, metadata and data must satisfy certain criteria - specified in terms of RDF constraints. 
From 2012 to 2015 together with other Linked Data community members and \emph{SBE} experts, we developed diverse vocabularies to represent \emph{SBE} metadata and data in RDF.

In this paper, we show how metadata and underlying data on different level of aggregation as well as collections of these data sets are represented in RDF and how therefore used vocabularies are interrelated.
We explain why RDF validation is important in this context and how (meta)data is validated against RDF constraints to ensure high quality and trust. 

\keywords{RDF Validation, RDF Constraints, DDI-RDF Discovery Vocabulary, Disco, RDF Data Cube Vocabulary, Linked Data, Semantic Web}
\end{abstract}

\section{Introduction}

%\tb{terminology: data in singular form / data set - NOT dataset / person-level data - NOT microdata / study - NOT survey}

For more than a decade, members of the community around research data for the social, behavioural, and economic (SBE) sciences have been developing and using a
metadata standard (composed of almost twelve hundred metadata fields) known as the \emph{Data Documentation Initiative (DDI)} \cite{Vardigan2008}.
DDI is an XML format designed for the purposes of supporting the dissemination, management,
and reuse of the data collected and archived for research purposes.  
DDI is heavily used by the CESSDA community of European national data archives, 
the International Household Survey Network community (made up of more than 90 statistical agencies),
and ICPSR - the largest SBE data archive in the US.
Increasingly, data professionals, national statistical institutes, data archives, data libraries, and government statisticians (e.g. \url{data.gov}, \url{data.gov.uk})
are very interested in having their data be discovered and used by providing their metadata on the web in form of RDF (e.g. metadata about unemployment rates or income).

Recently, members of the SBE and Linked Data community developed the \emph{DDI-RDF Discovery Vocabulary (Disco)}\footnote{\url{http://rdf-vocabulary.ddialliance.org/discovery.html}}, 
an effort to leverage the mature DDI metadata model for the purposes of exposing DDI metadata as resources within the Web of Linked Data. 
For data archives, research institutes, and data libraries,
RDF data validation according to predefined constraints is a much sought-after feature, 
particularly as this is taken for granted in the XML world (DDI-XML documents are validated against diverse XSDs\footnote{\url{http://www.ddialliance.org/Specification/}}).
Several approaches exist to meet this requirement, ranging from using \emph{OWL 2} as a constraint language to \emph{SPIN}\footnote{\url{http://spinRDF.org/}}, a SPARQL-based way to formulate and check constraints. 
There are also specific constraint languages like \emph{Shape Expressions}\footnote{\url{http://www.w3.org/Submission/shex-primer/}}, \emph{Resource Shapes}\footnote{\url{http://www.w3.org/Submission/shapes/}} or \emph{Description Set Profiles}\footnote{\url{http://dublincore.org/documents/2008/03/31/dc-dsp/}} that more or less explicitly address the aforementioned SBE community.
Bosch and Eckert\cite{BoschEckert2014-2} use SPIN as basis to define a
validation environment (\url{http://purl.org/net/rdfval-demo}) in which the validation of any constraint language\footnote{the only limitation is that constraint languages must be represented in RDF} can be implemented by representing them in SPARQL. 
The SPIN engine checks for each resource if it satisfies all constraints (associated with its assigned classes) and generates a result RDF graph containing information about all constraint violations.

\section{Motivation}

The data most often used in research within the SBE community is \emph{person-level data}, i.e. data collected about individuals 
(and sometimes also businesses and households) in the form of responses to studies or taken from administrative registers
(such as hospital records, registers of births and deaths). 
The range of person-level data is very broad (covering many different domains), 
including census, education, and health data as well as all types of business, social, and labor force surveys.  
Increasingly, this type of research data is
held within data archives or data libraries after it has been collected, so that it may be
reused by future researchers. 
In performing their research, the detailed person-level
data is aggregated into less confidential multi-dimensional tables which answer particular research questions.
Portals harvest metadata (as well as publicly available data) from multiple data providers in form of RDF.
To ensure high quality, the metadata must satisfy certain criteria - specified in terms of RDF constraints.  
After validating the metadata according to these constraints, portals offer added values to their customers, e.g. by searching over and comparing metadata of multiple providers. 

By its nature, person-level data is highly confidential, and access is often only permitted for qualified researchers who must apply for access. 
The purpose of publicly available aggregated data, on the other hand, is to get a first overview and to gain an interest in further analyses on the underlying person-level data.
Researchers typically represent their results as aggregated data in form of two-dimensional tables with only a few columns (so-called \emph{variables} such as \emph{sex} or \emph{age}).
The \emph{RDF Data Cube Vocabulary (QB)}\footnote{http://www.w3.org/TR/vocab-data-cube/} is a W3C recommendation for representing \emph{data cubes}, i.e. multi-dimensional aggregate data, in RDF \cite{Cyganiak2010}. 
Aggregate data is derived from person-level data by statistics on groups or aggregates such as counts, means, and frequencies.
The SDMX metadata standard\footnote{http://sdmx.org/} – used as the basis for \emph{QB} – and DDI have traditionally made efforts to align their content. 
Similarly, some of the developers of \emph{Disco} were also involved in the development of \emph{QB}, 
allowing the RDF versions of these standards to retain that alignment.
While \emph{Disco} and \emph{QB} provide terms for the description of data sets, 
both on a different level of aggregation, 
the \emph{Data Catalog Vocabulary (DCAT)}\footnote{\url{http://www.w3.org/TR/vocab-dcat/}} enables the representation of these data sets inside of data collections like repositories, catalogs, or archives. 
The relationship between data collections and their contained data sets is useful, since such collections are a typical entry point when searching for data.
Although, in most cases aggregated data is still published in form of PDFs, 
it is more and more common to publish aggregated data as CSV files,
allowing to perform first calculations (either using all variables or only a subset).
%For these calculations, definitions of the columns are needed (e.g. is a given variable interpreted numerically or as a string).
In 2014, SBE and Linked Data community members developed the \emph{Physical Data Description (PHDD)}\footnote{\url{https://github.com/linked-statistics/physical-data-description}} vocabulary to represent aggregated and person-level data in a rectangular format. 
The data could be either represented in records with character-separated values (CSV) or in records with fixed length. 

For more detailed analyses, researchers refer to person-level data from which aggregated data is derived from, 
as person-level data include additional variables needed for further research.
%Although not that common, the other direction is also possible, 
%i.e. researchers may use metadata on person-level data to search for aggregated data.
One very common example for detailed analyses on person-level data is the content-driven comparison of multiple studies.
Researchers get promising findings (in form of published tables with a few columns) within a metadata portal leading to subsequent research questions 
like 'How to compare the unemployment rate of different countries (e.g. Germany, UK, and France) in the last 10 years grouped by age?'.
The first step is to determine in which countries the unemployment rate is collected and which other variables of each country-specific study are theoretically comparable and can therefore be used to answer the underlying research question.
A \emph{study} represents the process by which a data set was generated or collected.
Variables are constructed out of values (of one or multiple datatypes) and/or code lists.
The variable \emph{age}, e.g., may be represented by values of the datatype \emph{xsd:nonNegativeInteger}, or by a code list including multiple age clusters (such as '0 to 10' and '11 to 20'). 
To determine if variables measuring \emph{age} 
- collected within multiple studies of different countries (\emph{$age_{DE}$}, \emph{$age_{UK}$}) - 
are comparable, both content-driven and technology-driven validation is performed either
within our developed validation environment or by matching algorithms. 
An example for a content-driven validation is to investigate if variables are represented in a compatible way,
i.e. are the variables' code lists theoretically comparable.
Technically, it can be validated (1) if variable definitions are available, (2) if code lists are properly structured, and (3) if for each code an associated category (a human-readable label) is specified.

\tb{maybe this paragraph should be moved to one of the subsequent sections and not be part of introduction}
Data providers and harvesters do not only offer metadata but also publicly available data on different level of detail.
To ensure high data quality and trust, they have to analyze and validate the data (are fundamental data fragments available?, how does valid data look like?). 
%They validate, e.g., if fundamental data parts are available and define how valid data should look like (using syntactic rules).
Provenance (where does the data come from?) is an important aspect in evaluating data quality.
As data searchers know exactly which data sources they trust and which are reasonable to meet their individual use cases, 
data validation can only be performed semi-automatically, i.e. an automatic approach serves as basis for intellectual decisions. 

This paper aims to address two main \textbf{audiences}: 
(1) metadata practitioners seeking for how to represent metadata on data sets on different aggregation levels and
(2) metadata providers and harvesters ensuring high quality metadata by validating metadata on highly complex RDF data sets.
In this paper, we show in form of a complete real world running example how to represent metadata on highly complex person-level data (\emph{Disco}), metadata on aggregated data (\emph{QB}), and aggregated as well as person-level data in a rectangular format (\emph{PHDD}) in RDF and how therefore used vocabularies are interrelated (\textbf{contribution 1}, section \ref{rdf-representation}).
We explain why RDF validation is important in this context and how (meta)data is validated against RDF constraints to ensure high quality (meta)data within this complex of data sets on different levels of aggregation (\textbf{contribution 2}, section \ref{rdf-validation}). 
The remainder of the paper is structured as follows.

%\section{Running Example}
%\label{running-example}
\section{Represent Metadata and Data in RDF}
\label{rdf-representation}

Eurostat\footnote{\url{http://ec.europa.eu/eurostat}} is the statistical office of the European Union. Its task is to provide statistics at European level that enable comparisons between countries and regions.
Eurostat provides publicly available European aggregated data (downloadable in machine-readable format like CSV files) and its metadata (only textual descriptions).
SBE researchers have a strong interest, e.g., in the availability of childcare services across European Union Member States.

\textbf{Metadata on Aggregated Data in RDF.}
The variable \emph{formal childcare}\footnote{\url{http://ec.europa.eu/eurostat/web/products-datasets/-/ilc_caindformal}} (in contrast to childcare at home)
captures the measured availability of childcare services in percent over the population.
The present data collection refers to data on formal childcare and other types of care by the variables \emph{year}, \emph{duration} (0 hours, 1 - 29 HPW; 30 HPW), \emph{age} of the child (0-2 years; 3 to admission age for compulsory school; admission age for compulsory school to 12) and \emph{country}.

\tb{ToDO for Ben: can you please insert how example is represented in Data Cube in this paragraph}

\emph{QB} is a vocabulary to represent metadata on multi-dimensional aggregate data.

data set

data structure definition

dimensions

measure

observations

\footnote{\textcolor{red}{The complete running example in RDF is available at: \url{XXXXX}}}

\textbf{Aggregated Data in RDF.}
As Eurostat provides aggregated data as CSV files, we can easily represent the two-dimensional table in RDF by means of \emph{PHDD}.
\emph{PHDD} is a vocabulary to represent aggregated and person-level data in a rectangular format. 
The data could be either represented in records with character-separated values (CSV) or in records with fixed length. 
The two-dimensional table about \emph{formal childcare} can be downloaded as CSV file. 
The table is represented as \emph{phdd:Table} and structured by a table structure (\emph{phdd:TableStructure}, \emph{phdd:Delimited}).
The running example table structure includes information about the character set (\emph{ASCII}), the variable delimiter (\emph{,}), the new line marker (\emph{CRLF}), and the first line where the data starts (\emph{2}).
The table structure relates to the table columns (\emph{phdd:Column}) which are described by column descriptions (\emph{phdd:DelimitedColumnDescription}).
For the column containing the cell values in percent, e.g., the CSV column position (\emph{5}), the recommended data type (\emph{xsd:nonNegativeInteger}), and the storage format (\emph{TINYINT}) is stated. 
The RDFication enables further aggregations and calculations, e.g., in order to compare \emph{formal childcare} between Northern and Southern Europe or between otherwise grouped countries.
% macht mehr sinn wenn Tabelle in mehreren Dimensionen vorhanden ist ode wenn Klassifikationen wie ISCO verwendet werden 

\textbf{Metadata on Person-Level Data in RDF.}
For a broader view of the data framework and more detailed analyses we refer to the metadata on person-level data collected for the series \emph{EU-SILC (European Union Statistics on Income and Living Conditions)}\footnote{\url{http://www.gesis.org/missy/eu/metadata/EU-SILC}} 
and publicly provided by the \emph{Microdata Information System (MISSY)}\footnote{\url{http://www.gesis.org/missy/eu/missy-home}}.
Where data collection is cyclic, data sets may be released as a \emph{series}, 
where each cycle of the data collection activity produces one or more data sets. 
\emph{Missy} is an online service platform that provides systematically structured metadata for official statistics on European person-level data sets. This includes data documentation at the study and variable level as well as documentation materials, tools and further information. 
Aggregated (qb:DataSet) and underlying person-level data sets (\emph{disco:LogicalDataSet}) are connected by \emph{prov:wasDerivedFrom}. 
The aggregated variable \emph{formal childcare} is calculated on the basis of six person-level variables like 
\emph{Education at pre-school}\footnote{\url{http://www.gesis.org/missy/eu/metadata/EU-SILC/2011/Cross-sectional/original#2011-Cross-sectional-RL010}}.
%(2) Education at compulsory school,
%(3) Child care at centre-based services,
%(4) Child care at day-care centre,
%(5) Child care by a professional child-minder, and
%(6) Child care by grand-parents, household members, relatives, friends, neighbours.
For each person-level variable detailed metadata is given (definitions and descriptions, theoretical concepts, underlying questions, code lists, frequencies and descriptive statistics, countries, year of data collection, classifications) enabling researchers to replicate the results shown in the aggregated data tables from Eurostat.
Metadata on person-level data is represented in RDF using the \emph{Disco} vocabulary.
The series (\emph{disco:StudyGroup}) \emph{EU-SILC} contains (\emph{disco:inGroup}) one study (\emph{disco:Study}) for each year (\emph{dcterms:temporal}) of data collection, e.g. \emph{EU-SILC 2011}.   
The property \emph{dcterms:spatial} points to the countries (\emph{dcterms:Location} resources which are the same as \emph{GeoNames} resources representing these countries) for which the data has been collected.
The study \emph{EU-SILC 2011} contains (\emph{disco:product}) eight person-level data sets (\emph{disco:LogicalDataSet}).
Data sets include (\emph{disco:variable}) person-level variables (\emph{disco:Variable}) like the six ones needed to calculate the aggregated variable \emph{formal childcare}.
Metadata on person-level data enables researchers to investigate further research questions based on promising findings of other researchers in form of aggregated data.
One common research question is ,e.g., the comparison of variables like 
\emph{formal childcare} between countries, for which the variable is collected within the context of an individual study, and other European or non European countries (e.g. OSCE).

\textbf{Organizations, Hierarchies, and Classifications.}
%\textbf{Reusing SKOS.}
The \emph{Simple Knowledge Organization System (SKOS)} is used multiple times within the context of aggregated and person-level (meta)data.
Variables, e.g., are constructed (\emph{disco:representation}) out of values (of one or multiple datatypes) and/or code lists.
The values of the variable \emph{Education at pre-school}, representing the number of education hours during a usual week, are expressed as \emph{skos:Concepts}. 
\emph{Disco} uses \emph{skos:OrderedCollection} to organize them in a particular order in a \emph{skos:memberList}. 
SKOS is also used to form hierarchies of \emph{SBE theoretical concepts} (e.g. Education) with which variables may be associated.
The compete hierarchy (\emph{skos:ConceptScheme}) of theoretical concepts (\emph{skos:Concepts}) of series is built using \emph{skos:narrower}.
The variable \emph{Education at pre-school} is assigned to the theoretical concept \emph{Child Care} which is a narrower concept of \emph{Education} - one of the top concept of the series \emph{EU-SILC}.
Controlled vocabularies serve as extension and reuse mechanism.
For \emph{Disco}, concepts (\emph{skos:Concepts}), organized within controlled vocabularies (\emph{skos:ConceptSchemes}), indicate types of descriptive statistics (\emph{\emph{disco:SummaryStatistics}}) like minimum, maximum, mean, and standard deviation.
From 2012 to 2015, SBE and Linked Data community members developed \emph{XKOS}\footnote{\url{https://github.com/linked-statistics/xkos}} - a SKOS extension to describe formal statistical classifications like the International Standard Classification of Occupations (\emph{ISCO}). 
%and the Statistical Classification of Economic Activities in the European Community \emph{NACE}.

\textbf{Searching for (Meta)data.}
\emph{DCAT} enables to represent aggregated and person-level data inside of data collections like portals, repositories, catalogs, or archives
serving as typical entry points when searching for data.
Users search for aggregated and person-level data records (\emph{dcat:CatalogRecord}) inside data catalogs (\emph{dcat:Catalog}). 
This search differs depending on the users’ information need. 
While it is possible to search for metadata provided inside such a record (e.g. \emph{dcterms:title}, \emph{dcterms:description}), 
users can also formulate more sophisticated queries on aggregated and person-level data sets (\emph{dcat:Dataset}) or their
distributions (\emph{dcat:Distribution}), which are part of the records. 
Users may want to search for data sets covering particular topical (\emph{dcat:keyword}, \emph{dcat:theme}), temporal (\emph{dcterms:temporal}),  or  spatial  coverages (\emph{dcterms:spatial}), 
or certain formats in which the data distribution is available (\emph{dcterms:format}). 

\section{RDF Validation of Metadata and Data}
\label{rdf-validation}

Bosch et al. identified in total 74 requirements to formulate RDF constraints; each of them corresponding to a constraint type. 
We published a technical report\footnote{Available at: \url{http://arxiv.org/abs/1501.03933}} in which we explain each requirement (constraint type) in detail and give examples for each (represented by different constraint languages).
The knowledge representation formalism \emph{Description logics (DL)}, with its  well-studied theoretical properties, provides the foundational basis for each constraint type.
Therefore, this technical report contains mappings to DL to logically underpin each requirement and to determine which DL constructs are needed to express each constraint type \cite{BoschNolleAcarEckert2015}.
We recently published a technical report\footnote{\textcolor{red}{\label{technical-report-1}Available at: \url{http://arxiv.org/abs/XXXXX}}} (serving as first appendix of this paper) in which we describe constraints to validate metadata on person-level, aggregated data, and thesauri.
We assign each constraint to constraint types corresponding to RDF validation requirements or to data model specific constraint types\footnote{Requirements/Constraint types and constraints are uniquely identified by alphanumeric technical identifiers like \emph{R-1}}
\cite{BoschZapilkoWackerowEckert2015}.

We distinguish two validation types:
(1) \emph{Content-Driven Validation} $\mathcal{C}_{C}$ contains the set of constraints ensuring that the data is consistent with the intended syntax, semantics, and integrity of given data models (section \ref{content-driven-validation}).
(2) \emph{Technology-Driven Validation} $\mathcal{C}_{T}$ includes the set of constraints which can be generated automatically out of data models, such as cardinality restrictions, universal and existential quantifications, domains, and ranges (section \ref{technology-driven-validation}).
We determined the default \emph{severity level} (corresponds to requirement \emph{R-158}) for each constraint to indicate how serious the violation of the constraint is.
We propose an extensible metric to measure the continuum of severity levels ranging from $\mathcal{SL}_{0}$ (informational) via $\mathcal{SL}_{1}$ (warning) to $\mathcal{SL}_{2}$ (error).
The violation of $\mathcal{SL}_{0}$ constraints does not mean that there are serious mistakes in either syntax or semantics. These constraints may point to possible data improvements in order to get ideal RDF representations. 
Data not conforming to $\mathcal{SL}_{1}$ and $\mathcal{SL}_{2}$ constraints is syntactically or semantically not correctly represented.
The difference between $\mathcal{SL}_{1}$ and $\mathcal{SL}_{2}$ constraints is that $\mathcal{SL}_{1}$ invalid data could be processed 
whereas $\mathcal{SL}_{1}$ invalid data cannot be processed further. 
Although we provide default severity levels for each constraint, users should be able to specify severity levels of constraints they need to validate for their individual use cases, i.e., users should be able to define use case specific severity levels for constraints.
In this section, we describe constraints and constraint types which are important to ensure (meta)data  quality on different aggregation levels. 
Furthermore, we associate constraints with default severity levels and assign them to validation types. 

%
%- different sets of constraints for different use cases / use case specific constraints
%
%- validation against minimal set of requirements / constraints
%
%-----

%further ideas:
%
%RDF validation scenarios require the closed-world assumption (CWA) (i.e., a statement is inferred to be false if it cannot be proved to be true).

%use-case specific constraints, e.g., DCAT: searching for metadata
%\tb{Thomas: ToDo}

\subsection{Content-Driven Validation}
\label{content-driven-validation}

\textbf{Observations of Aggregated Data Sets.}
The purpose of some constraints is to ensure the integrity of
the data according to intended data model semantics (\emph{data model consistency}).
Every \emph{qb:Observation}, e.g., must have a value for each dimension
declared in its associated \emph{qb:DataStructureDefinition} ($\mathcal{SL}_{2}$)
and no two \emph{qb:Observations} in the same \emph{qb:DataSet}
may have the same value for all dimensions ($\mathcal{SL}_{1}$).
If a \emph{qb:DataSet} \emph{D} has a \emph{qb:Slice} \emph{S}, and \emph{S} has an
\emph{qb:Observation} \emph{O}, then the \emph{qb:DataSet} corresponding to \emph{O} must be \emph{D} ($\mathcal{SL}_{1}$).

%- for each dimension there should be a description and code lists.
%
%- for each code list there should be a description.
%
%- there should be a relationship to the underlying person-level data.

\textbf{Series, Studies, Data Sets, and Data Files.}
It is useful to declare properties to be \emph{conditional} (\emph{R-71}), i.e., if particular properties exist (or do not exist), then other properties must also be present (or absent).
To get an overview over a series/study either an abstract, a title, an alternative title, or links to external descriptions should be provided. 
If an abstract and an external description are absent, 
a title or an alternative title has to be stated ($\mathcal{SL}_{1}$).
For datatype properties it should be possible to declare frequently needed \emph{facets} ({\emph{R-46}) to drive user interfaces and validate input against simple conditions including min/max values, regular expressions, and string length.
The abstract of series/studies, e.g., should have a minimum length (\emph{xsd:minLength}; $\mathcal{SL}_{1}$).
\emph{Existential quantifications} (\emph{R-86}) enforce that instances of given classes must have some property relation to individuals of certain types.
If a study, e.g., has no associated data sets ($\mathcal{SL}_{2}$), the actual description of the data is missing which may indicate that it is very hard or unlikely to get access to the data.
If there is no metadata on data files including the actual data of data sets (especially case and variable quantities; $\mathcal{SL}_{1}$), the description of the data sets and the underlying study is not sufficient.
The case quantity measures how many cases are collected for a study.
A high case and variable quantity is an indicator for high statistical quality and comprehensiveness of the conducted study ($\mathcal{SL}_{1}$).

\textbf{Variables and Variable Comparison.}
Each variable should have a variable representation (\emph{R-86}; $\mathcal{SL}_{1}$), which is either an (un)ordered code list or a union of datatypes.
In case of a code list, associated categories (human-readable labels) may be stated (\emph{R-71}; $\mathcal{SL}_{0}$).
The variable \emph{Education at pre-school} is represented as ordered code list without any categories.
If a {\em skos:Concept} stands for a code (having a {\em skos:notation} property) and a category (having a {\em skos:prefLabel} property), 
then the property {\em disco:isValid} has to be stated indicating if the code is valid (\emph{true}) or missing (\emph{false}) (\emph{R-71}; $\mathcal{SL}_{2}$).
Variables may have at least one relationship to a theoretical concept ({\emph{R-86}; $\mathcal{SL}_{0}$).
%The variables \emph{number of school years} and \emph{highest educational degree}, e.g., are associated with the theoretical concept \emph{education}. 
The variable \emph{Education at pre-school} is associated with the theoretical concept \emph{Child Care}. 
The default severity level of this constraint is weak, as in most cases research can be continued without associated theoretical concepts.
A very common research question is to compare variables of multiple studies or countries (\emph{comparison}).
To compare variables, 
(1) variables and (2) variable definitions must be present,
(3) code lists must be structured properly,
(4) for each code an associated category (human-readable label) must be specified, and
(5) code lists must either be identical or at least similar.
If a researcher wants to get a first overview over comparable variables (use case 1), 
covering the first three constraints may be sufficient for this purpose.
Thus, the severity level of the first three constraints is stronger ($\mathcal{SL}_{2}$) than the severity level of the next two constraints ($\mathcal{SL}_{1}$ and $\mathcal{SL}_{0}$).
If the intention of the researcher is to perform more detailed comparisons (use case 2), however, the violation of the remaining two constraints is getting more serious.

\textbf{Descriptive Statistics.}
The property \emph{disco:percentage} stands for the number of cases of a given code in relation to the total number of cases for a particular variable within a data set.
Percentage values are only valid when they are within the \emph{literal range} of 0 and 100 (\emph{R-45}; $\mathcal{SL}_{2}$).
{\em Mathematical Operations} (\emph{R-41, R-42}; e.g. date calculations and statistical computations like average, mean, and sum) are performed to ensure the integrity of data models.
The sum of percentage values of all codes of a variable code list, e.g., must exactly be 100 ($\mathcal{SL}_{2}$)
and the minimum of all variable codes do not have to be greater than the maximum ($\mathcal{SL}_{2}$).
Codes (\emph{skos:Concept}) are ordered and therefore have fixed positions in an ordered collection (\emph{skos:OrderedCollection}) within variable representations.
In order to check the correctness of relative frequencies' calculations, the cumulative percentage (\emph{disco:cumulativePercentage}) of the current code must exactly be the cumulative percentage of the previous code
plus the percentage value (\emph{disco:percentage}) of the current code (\emph{data model consistency}; $\mathcal{SL}_{2}$).

\textbf{Unique Identification.}
It is often useful to declare a given (data) property as the \emph{primary key} (\emph{R-226}) of a class, so that a system can enforce uniqueness and also automatically build URIs from user inputs and imported data. 
In \emph{Disco}, resources are uniquely identified by the property \emph{adms:identifier},
which is therefore inverse-functional
$(\ms{funct identifier}\sp{\overline{\ }})$,
i.e. for each \emph{rdfs:Resource x}, there can be at most one distinct \emph{rdfs:Resource y} such that \emph{y} is connected by \emph{adms:identifier$\sp{\overline{\ }}$} to \emph{x} ($\mathcal{SL}_{2}$).
Keys, however, are even more general than inverse-functional properties (\emph{R-58}),
as a key can be a data, an object property, or a chain of properties \cite{Schneider2009}.
For this generalization purposes, as there are different sorts of key, and as keys can lead to undecidability, 
DL is extended with \emph{key boxes} and a special \emph{keyfor} construct (\ms{identifier \ms{keyfor} Resource}) \cite{Lutz2005}.
OWL 2 \emph{hasKey} implements \emph{keyfor} ($\mathcal{SL}_{2}$) and thus can be used to identify resources uniquely, to merge resources with identical key property values, and to recognize constraint violations.
%OWL 2 hasKey can be used to identify resources uniquely
%
%We used Protégé 5.
%
%example: owl:Thing owl:hasKey ( :hasSSN ) . :Peter :hasSSN "123-45-6789" .
%:Peter_Griffin :hasSSN "123-45-6789" .
%
%We use the predefined Reasoner HermiT.
%
%:Peter and :Peter_Griffin are derived as identical from the reasoner as they have the same value for :hasSSN.
%
%hasKey can be used to merge resources and to recognize constraint violations.
%
%Alternative to OWL 2 hasKey: concise bounded description (proposal of Dan)
%
%action: explore CBD document solution might be using both approaches
%
%benefit: additional identification to URI / IRI benefit: compound properties can be used as key


\textbf{Membership in Controlled Vocabularies.}
In many cases, resources must be members of controlled vocabularies (\emph{R-32}).
If a dimension property, e.g., has a \emph{qb:codeList},
then the value of the dimension property on every \emph{qb:Observation} must be in the code list ($\mathcal{SL}_{2}$).
Summary statistics types like minimum, maximum, and arithmetic mean are maintained within a controlled vocabulary.  
Summary statistics can only have \emph{disco:summaryStatisticType} relationships to \emph{skos:Concept}s which must be members of the controlled vocabulary \emph{ddicv:SummaryStatisticType}, a \emph{skos:ConceptScheme} ($\mathcal{SL}_{2}$).

%\begin{center}
%\begin{DL}
%SummaryStatistics $\sqsubseteq$ $\forall summaryStatisticType.A$ \\
%$A \equiv Concept \sqcap \forall inScheme . B$ \\
%$B \equiv ConceptScheme \sqcap \{SummaryStatisticType\}$
%\end{DL}
%\end{center}

\textbf{Coverage.}
Information about the temporal (\emph{dcterms:temporal}), the spatial (\emph{dcterms:spatial}), and the topical coverage (\emph{dcterms:subject}) of series, studies, data sets, and data files (\emph{R-86}; $\mathcal{SL}_{1}$) is of interest when performing frequently formulated queries 
	(e.g. to search for all data sets of given years (temporal coverage) in which data is collected in certain countries (spatial coverage) about particular topics (topical coverage)).
Depending on property datatypes,
two different literal values have
a specific ordering with respect to an operator like \textless (\emph{R-43: literal value comparison}).
Start dates (\emph{disco:startDate}), e.g., must be before (\emph{\textless}) end dates (\emph{disco:endDate}) ($\mathcal{SL}_{2}$).
\emph{R-223} serves to make sure that all literal values are valid with regard to their datatypes ($\mathcal{SL}_{2}$).
Thus, all date values (e.g. {\em disco:startDate}, {\em disco:endDate}, {\em dcterms:date}) must be of the datatype \emph{xsd:date} and \emph{xsd:nonNegativeInteger} values (e.g. \emph{disco:frequency}) do not have to be negative.

%\textbf{Organizations, Hierarchies, Classifications, and Ordering.}
\textbf{Hierarchies and Ordering.}
SKOS is based on RDF, which is a graph-based data model. Therefore, we can concentrate on the vocabulary's graph-based structure for assessing the quality of SKOS vocabularies and apply graph- and network-analysis techniques (\emph{structure}) like 
(1) a vocabulary should provide entry points (top concepts) to the data to provide efficient access and guidance for human users,
(2) concepts, internal to the tree, should not be indicated as top concepts, and
(3) a vocabulary should not contain many orphan concepts 
(concepts without any associative or hierarchical relations) lacking valuable context information. A controlled vocabulary that contains many orphan concepts is less usable for search and retrieval use cases.
%, as, e.g., no hierarchical query expansion can be performed on search terms to find documents with more general content.) \cite{MaderHaslhoferIsaac2012}. 
Objects and literals can be \emph{ordered} (\emph{R-121, R-217}) for given properties.
\emph{Disco }variables, questions, and codes/categories are typically organized in a particular order. 
If a variable code list should be ordered, the variable representation should be of the type \emph{skos:OrderedCollection} containing multiple codes/categories (each represented as \emph{skos:Concept}) in a \emph{skos:memberList}. 

\textbf{Reusability.}
Within the context of \emph{Disco}, \emph{skos:Concept}s can have either \emph{skos:definition} (when interpreted as theoretical concepts) or \emph{skos:notation} and \emph{skos:prefLabel} properties (when interpreted as codes/categories), but not both ($\mathcal{SL}_{2}$).
The constraint type \emph{context-specific exclusive or of property groups} (\emph{R-11})
restricts individuals of given classes to have exactly one of multiple property groups.

%\begin{center}
%\begin{DL}
%Concept $\sqsubseteq$ ($\neg$ D $\sqcap$ C) $\sqcup$ (D $\sqcap$ $\neg$ C), D $\equiv$ A $\sqcap$ B \\
%A $\sqsubseteq$ $\geq$ 1 notation.string $\sqcap$ $\leq$ 1 notation.string \\
%B $\sqsubseteq$ $\geq$ 1 prefLabel.string $\sqcap$ $\leq$ 1 prefLabel.string \\
%C $\sqsubseteq$ $\geq$ 1 definition.string $\sqcap$ $\leq$ 1 definition.string \\
%\end{DL}
%\end{center}

%\textbf{Searching for (Meta)data.}
%
%- DCAT
%
%\textbf{Data Integration.}
%use RDF validation for data integration

\subsection{Technology-Driven Validation}
\label{technology-driven-validation}

\tb{use PHDD and XKOS here}

Constraints of some constraint types are directly and automatically derived from syntax and semantics of vocabularies' conceptual models.
As these constraints depend on data models' intended semantics, associated default severity levels are in most cases very strong ($\mathcal{SL}_{2}$).

\textbf{Vocabulary.}
One should not invent new or use deprecated terms of vocabularies (\emph{vocabulary}).
\emph{Property Domains} (\emph{R-25, R-26}) and \emph{Ranges} (\emph{R-28, R-35}) restrict domains and ranges of properties.
Only \emph{phdd:Tables}, e.g., can have \emph{phdd:isStructuredBy} relationships (\ms{$\exists$ isStructuredBy.$\top$ $\sqsubseteq$ Table}) and
\emph{xkos:belongsTo} relationships can only point to \emph{skos:Concept} instances (\ms{$\top$ $\sqsubseteq$ $\forall$ belongsTo.Concept}).
A \emph{universal quantification} (\emph{R-91}) contains all those individuals that are connected by a property only to individuals/literals of particular classes  or data ranges.
Only \emph{dcat:Catalogs}, e.g., can have \emph{dcat:dataset} relationships to \emph{dcat:Datasets} (\ms{Catalog $\sqsubseteq$ $\forall$ dataset.Dataset}).
Out-dated classes and properties of previous vocabulary versions can be marked as deprecated.
The constraint type \emph{context-specific valid classes and properties} (\emph{R-209; R-210}) can be used to specify which classes and properties are valid in which context - here a given vocabulary version.
Many properties are not necessarily required but \emph{recommended} within a particular context (\emph{R-72}).
The property {\em skos:notation}, e.g., is not mandatory for {\em disco:Variable}s, but recommended to represent variable names.

\textbf{Cardinality Restrictions.}
An \emph{existential quantification} (\emph{R-86}) contains all those individuals that are connected by a property to individuals/literals of given classes or data ranges.
Every \emph{qb:SliceKey}, e.g., must be associated with (\emph{qb:sliceKey}) a \emph{qb:DataStructureDefinition} (\ms{SliceKey $\sqsubseteq$ $\exists$ sliceKey$^{-}$.DataStructureDefinition}).
\emph{Minimum/maximum/exact qualified cardinality restrictions} (\emph{R-74, R-75, R-76}) contain all those individuals that are connected by a property to at least/at most/exactly n different individuals/literals of particular classes or data ranges.
A \emph{phdd:TableStructure}, e.g., has at least one \emph{phdd:column} relationship to a \emph{phdd:Column} (\ms{TableStructure $\sqsubseteq$ $\geq$1 column.Column}),
a \emph{disco:Variable} has at most one \emph{disco:concept} relationship to a theoretical concept (\emph{skos:Concept}) (\ms{Variable $\sqsubseteq$ $\leq$1 concept.Concept}), and a \emph{qb:DataSet} has (\emph{qb:structure}) exactly one associated \emph{qb:DataStructureDefinition} (\ms{DataSet $\sqsubseteq$ $\geq$1 structure.DataStructureDefinition $\sqcap$ $\leq$1 structure.DataStructureDefinition}).

%DATA-CUBE-C-MINIMUM-QUALIFIED-CARDINALITY-RESTRICTIONS-
%02: Unique data set (IC-1 [3]) - Every qb:Observation has (qb:dataSet) ex-
%actly one associated qb:DataSet (Observation  ¥1 dataSet.DataSet [
%¤1 dataSet.DataSet).
%Severity level: ERROR
%
%(\emph{R-75: minimum qualified cardinality restrictions})
%
%DATA-CUBE-C-EXACT-QUALIFIED-CARDINALITY-RESTRICTIONS-
%02: Unique DSD (IC-2 [3]) - Every qb:DataSet has (qb:structure) exactly one
%associated qb:DataStructureDefinition (DataSet  ¥1 structure.DataStructureDefinition
%[ ¤1 structure.DataStructureDefinition).
%Severity level: ERROR
%
%(\emph{R-74: exact qualified cardinality restrictions})

%\footnote{\emph{DATA-CUBE-C-EXACT-QUALIFIED-CARDINALITY-RESTRICTIONS-02}}.

\textbf{Language Tag Cardinality.}
For data properties, it may be desirable to restrict that values of predefined languages must be present for determined number of times (\emph{R-48, R-49}):
(1) Some controlled vocabularies contain literals in natural language, but without information what language has actually been used. 
(2) Language tags must conform to language standards. 
(3) Some thesaurus concepts are labeled in only one, others in multiple languages. It may be desirable to have each concept labeled in each of the languages that are also used on the other concepts. Although not always possible, incompleteness of language coverage for some concepts may indicate shortcomings of thesauri
\cite{MaderHaslhoferIsaac2012}.

\textbf{Disjointness and Allowed Values.}
All properties, not having the same domain and range classes, are defined to be pairwise \emph{disjoint}
(\emph{R-9: disjoint properties}), stating that no individual \emph{x} can be connected to an individual/literal \emph{y} by disjoint properties like \emph{phdd:isStructuredBy} and \emph{phdd:column} ($isStructuredBy \sqsubseteq \neg column$).
All \emph{Disco} classes are defined to be pairwise disjoint (\emph{R-7: disjoint classes}; e.g. \ms{Study $\sqcap$ Variable $\sqsubseteq$ $\perp$}),
i.e. no individual can be at the same time an instance of more than one disjoint class.
Furthermore, it is a common requirement to narrow down the value space of properties by an exhaustive enumeration of valid values.  
\emph{Allowed values} (\emph{R-30, R-37}) for properties can be IRIs (matching one or multiple patterns), any literals, allowed literals (e.g. 'red' 'blue' 'green'), and typed literals of one or multiple type(s) (e.g. \emph{xsd:string}) - 
\emph{disco:CategoryStatistics}, e.g., can only have \emph{disco:computationBase} relationships to the values \emph{valid} and \emph{invalid} of the datatype \emph{rdf:langString} (\ms{CategoryStatistics $\equiv$ $\forall$ computationBase.\{valid,invalid\} $\sqcap$ langString}).

\textbf{Validation and Reasoning.}
Constraints of some constraint types can be derived from vocabularies' data models for which reasoning can be performed prior to validation which enables to resolve possible constraint violations.
\emph{Subsumption} (\emph{R-100}) states that \emph{C1} is a sub-class of \emph{C2} - \emph{C1} is more specific than \emph{C2}, 
i.e., each \emph{C1} resource must also be part of the \emph{C2} class extension. 
All \emph{disco:Universe}s, e.g., must also be of the type \emph{skos:Concept} (\ms{Universe $\sqsubseteq$ Concept}).
\emph{Sub Properties} (\emph{R-54, R-64}) state that \emph{P1} is a sub-property of \emph{P2} - i.e., if individual \emph{x} is connected by \emph{P1} to individual/literal \emph{y}, then \emph{x} is also connected by \emph{P2} to \emph{y}. 
If a study is funded by (\emph{disco:fundedBy}) an organization, then this organization contributed (\emph{dcterms:contributor}) to this study (\ms{fundedBy $\sqsubseteq$ contributor}).
With \emph{Asymmetric object properties} (\emph{R-62}) it can be restricted that if individual \emph{x} is connected by the object property \emph{OP} to individual \emph{y}, then \emph{y} cannot be connected by \emph{OP} to \emph{x}. 
Such constraints are defined for each vocabulary's object property for which a semantically equivalent object property pointing from the other direction would also be possible but is not defined within the vocabulary.
A \emph{disco:Variable}, e.g., may be based on (\emph{disco:basedOn}) a \emph{disco:RepresentedVariable}.
A \emph{disco:RepresentedVariable}, however, cannot be based on a \emph{disco:Variable} (\ms{$basedOn \sqcap basedOn^{-} \sqsubseteq \bot$}).
\emph{Default values} (\emph{R-31, R-38}) for objects/literals of given prooperties are inferred automatically when properties are not stated.
The value \emph{true} for the property {\em disco:isPublic} ({\em xsd:boolean}) indicates that a data set ({\em disco:LogicalDataSet}) can be accessed by anyone.
Per default, however, access to data sets should be restricted (\emph{false}).

-----

It has to be ensured that a value is valid for its datatype (\emph{Value is Valid for Datatype}), e.g., 
that a date is really a date, or that a \emph{xsd:nonNegativeInteger} value is not negative. 
It is checked if all literals of {\em xsd:date} properties are really dates (e.g. {\em disco:startDate}, {\em disco:endDate}, {\em dcterms:date}).

-----

The validation of instances data (direct or indirect) exploits the sub-class or sub-property link in a given ontology (\emph{Use Sub-Super Relations in Validation}).
This validation can indicate when the data is verbose (redundant) or expressed at a too general level, and could thus be improved.
If \emph{dcterms:coverage} and one of its sub-properties (\emph{dcterms:spatial}, \emph{dcterms:temporal}) are present,
it is checked that \emph{dcterms:coverage} is not redundant with its sub-properties. 

-----

\section{Implementation}

SPARQL is generally seen as the method of choice to validate RDF data according to certain constraints.
We use \emph{SPIN}, 
a SPARQL-based way to formulate and check constraints, as basis to develop a
validation environment (available at \url{http://purl.org/net/rdfval-demo}) to validate RDF data according to constraints expressed my arbitrary constraint languages like Shape Expressions,
%\footnote{\url{http://www.w3.org/Submission/shex-primer/}}
Resource Shapes, and the Web Ontology Language\footnote{\textcolor{red}{SPIN mappings available at: \url{https://github.com/boschthomas/rdf-validation/SPIN}}} \cite{BoschEckert2014-2}.
The \emph{RDF Validator} also validates RDF data to ensure correct syntax, semantics, and integrity of diverse vocabularies such as \emph{Disco, QB, PHDD, SKOS, and XKOS}.
Although accessible within our validation tool, we provide all implemented constraints\footnote{\textcolor{red}{\url{https://github.com/boschthomas/rdf-validation/tree/master/constraints}}} in form of SPARQL CONSTRUCT queries.
For the subsequent evaluation, we implemented 212 constraints on \emph{Disco}, \emph{QB}, \emph{SKOS}, \emph{XKOS}, and \emph{PHDD} data sets.
The SPIN engine checks for each resource if it satisfies all constraints, which are associated with its assigned classes, and generates a result RDF graph containing information about all constraint violations.
There is one SPIN construct template for each constraint type and vocabulary-specific constraint\footnote{For a comprehensive description of the \emph{RDF Validator}, we refer to \cite{BoschEckert2014-2}}.
A SPIN construct template contains a SPARQL CONSTRUCT query which generates constraint violation triples indicating the subject and the properties causing constraint violations, and the reason why constraint violations have been raised.
A SPIN construct template creates constraint violation triples if all triple patterns within the SPARQL WHERE clause match.
\emph{Missy}\footnote{\url{http://www.gesis.org/missy/eu/missy-home}} provides comprehensive Linked Data services like diverse RDF exports of person-level metadata conforming to the \emph{Disco} vocabulary in form of multiple concrete syntaxes. 

\section{Evaluation}

We exhaustively evaluated the metadata quality of large real world aggregated (\emph{QB}), person-level (\emph{Disco}), and thesauri (\emph{SKOS}) data sets by means of both $\mathcal{C}_{C}$ and $\mathcal{C}_{T}$ constraints of the majority of the constraint types.
We validated 
9,990 / 3,775,983,610 (\emph{QB}),
4,178 / 477,737,281 (\emph{SKOS}), and 
1,526 / 9,673,055 (\emph{Disco}) data sets / triples using the \emph{RDF Validator} in batch mode.
That are more than 4.2 billion triples and 15 thousand data sets.
We validated, i.a., 
(1) \emph{QB} data sets published by the \emph{Australian Bureau of Statistics (ABS)},
the \emph{European Central Bank (ECB)}, and the
\emph{Organisation for Economic Co-operation and Development (OECD)},
(2) \emph{SKOS} thesauri like the \emph{AGROVOC Multilingual agricultural thesaurus},
the \emph{STW Thesaurus for Economics}, and the
\emph{Thesaurus for the Social Sciences (TheSoz)}, and
(3) \emph{Disco} data sets provided by the \emph{Microdata Information System (Missy)}, 
the \emph{DwB Discovery Portal}, the
\emph{Danish Data Archive (DDA)}, and the
\emph{Swedish National Data Service (SND)}.

We recently published a technical report\footnote{\label{technical-report-2}\textcolor{red}{Available at: \url{http://arxiv.org/abs/XXXXX}}} (serving as second appendix of this paper) 
in which we describe the comprehensive evaluation in detail \cite{BoschZapilkoWackerowEckert2015-2}. 
As we evaluated nearly 10 thousand \emph{QB} data sets, we published the evaluation results for each data set in form of one document per SPARQL endpoint\footnote{Available at: \url{https://github.com/boschthomas/rdf-validation/tree/master/evaluation/data-sets/data-cube}}.
The correctness of all constraints, i.e., the gold standard, has been proved by SBE domain experts.
Table \ref{tab:evaluation} shows the evaluation results.

\begin{table}[H]
		\scriptsize
    \begin{center}
    \begin{tabular}{@{}lcccc@{}}
%           & \multicolumn{6}{c}{\textbf{Vocabularies}}
%    \\  \cmidrule{2-7}
    \\       \textbf{Criteria\tablefootnote{\emph{C (constraints), CT (constraint types), CV (constraint violations)}}}
           & \textbf{\emph{Disco}}
           & \textbf{\emph{QB}}
					 & \textbf{\emph{SKOS}}
					 & \textbf{Total}
    \\ \midrule
		\emph{Triples} & 9,673,055 & 3,775,983,610 & 477,737,281 & 4,263,393,946 \\
		\emph{Data Sets} & 1,526 & 9,990 & 4,178 & 15,694 \\
		\hline
    \emph{CV} & 3,545,703 & 45,635,846 & 5,540,988 & 54,722,537\\
		\emph{CV ($\mathcal{SL}_{0}$)} & 2,437,922 (68.8\%) & 0 (0\%) & 2,281,740 (41.2\%) & 4,719,662 (8.6\%) \\
		\emph{CV ($\mathcal{SL}_{1}$)} & 473,574 (13.4\%) & 45,520,613 (99.75\%) & 3,259,248 (58.8\%) & 49,253,435 (90\%) \\
    \emph{CV ($\mathcal{SL}_{2}$)} & 634,207 (17.9\%) & 115,233 (0.25\%) & 0 (0\%) & 749,440 (1.4\%) \\
		\hline
		\emph{CT} & 52 (15\textbar 37)\tablefootnote{(\label{fnt:1}implemented \textbar not yet implemented)} & 20 (7\textbar 13)\footnotemark[\ref{fnt:1}] & 14 (4\textbar 10)\footnotemark[\ref{fnt:1}] & 53 \\
		\emph{CT} ($\mathcal{C}_{C}$) & 30 (57.7\%) & 5 (25\%) & 5 (35.7\%) & 30 (56.6\%) \\
		\emph{CT} ($\mathcal{C}_{T}$) & 22 (42.3\%) & 15 (75\%) & 9 (64.3\%) & 23 (43.4\%) \\
		\hline
		\emph{C} & 142 (77\textbar 65)\footnotemark[\ref{fnt:1}] & 35 (20\textbar 15)\footnotemark[\ref{fnt:1}] & 35 (17\textbar 18)\footnotemark[\ref{fnt:1}] & 212 \\
		\emph{C} ($\mathcal{C}_{C}$) & 72 (50.7\%) & 16 (45.7\%) & 21 (60\%) & 109 (51.4\%) \\
		\emph{C} ($\mathcal{C}_{T}$) & 70 (49.3\%) & 19 (54.3\%) & 14 (40\%) & 103 (48.6\%) \\
		\emph{C ($\mathcal{SL}_{0}$)} & 75 (52.8\%)& 4 (11.4\%) & 21 (60\%) & 100 (47.2\%) \\
		\emph{C ($\mathcal{SL}_{1}$)} & 9 (6.3\%)& 3 (8.6\%) & 5 (14.3\%) & 17 (8\%) \\
		\emph{C ($\mathcal{SL}_{2}$)} & 58 (40.8\%) & 28 (80\%) & 9 (25.7\%) & 95 (44.8\%) \\
    \bottomrule
    \end{tabular}
    \caption{Evaluation}
		\label{tab:evaluation}
    \end{center}
\end{table}

We identified 142 \emph{Disco} constraints ($\mathcal{C}_{C}$ and $\mathcal{C}_{T}$ constraints to the same extend) assigned to 52 distinct constraint types and implemented 77 of them to actually validate person-level data sets.  
For \emph{QB}, we specified more $\mathcal{C}_{T}$ (54\%) than $\mathcal{C}_{C}$ constraints; for \emph{SKOS}, however, more $\mathcal{C}_{C}$ constraints (60\%).
We instantiated more $\mathcal{C}_{C}$ (58\%) than $\mathcal{C}_{T}$ constraint types to define \emph{Disco} constraints; 
for \emph{QB} (75\%) and \emph{SKOS} (64\%), on the other side, more $\mathcal{C}_{T}$ constraint types. 
In total, we used 53 of overall 82 distinct constraint types (57\% of them are $\mathcal{C}_{C}$ constraint types) to define 212 constraints (equally $\mathcal{C}_{C}$ and $\mathcal{C}_{T}$ constraints).

For \emph{Disco} and \emph{SKOS}, more than the half of the constraints are associated with the weakest severity level $\mathcal{SL}_{0}$.
Within the context of \emph{QB}, 80\% of the constraints are classified as the most serious ones ($\mathcal{SL}_{2}$).
All in all, there are a little bit more $\mathcal{SL}_{0}$ then $\mathcal{SL}_{2}$ constraints, whereas $\mathcal{SL}_{1}$ constraints are negligible.
\emph{Existential quantifications} (32.4\%, \emph{Disco}), \emph{data model consistency} (31.4\%, \emph{QB}), and \emph{structure} (28.6\%, \emph{SKOS}) are the constraint types the most constraints are instantiated from.
By validating \emph{QB} data sets, we got the most constraint violations (more than 45 millions), followed by \emph{SKOS} and \emph{Disco} (with more than 5.5 and 3.5 millions) - consequently, almost 55 million constraint violations were raised during the evaluation which could be used to enhance the metadata quality of these data sets.  
Close to 70\% of all \emph{Disco} constraint violations are caused by violating $\mathcal{SL}_{0}$ constraints.
For \emph{QB} (nearly 100\%) and \emph{SKOS} (almost 60\%), the majority of the raised constraint violations are classified to be more serious ($\mathcal{SL}_{1}$).
80\% of all \emph{QB} constraints are $\mathcal{SL}_{2}$ constraints leading to less than 1\% of all \emph{QB} constraint violations.
Altogether, exactly 90\% of the constraint violations are assigned to the severity level $\mathcal{SL}_{1}$.
These findings are surprising as only 8\% of all defined constraints are $\mathcal{SL}_{1}$ constraints.
The constraints responsible for the largest numbers of constraint violations are \emph{DISCO-C-LABELING-AND-DOCUMENTATION-06} and \emph{DISCO-C-COMPARISON-VARIABLES-02} (both 547,916) (\emph{Disco}), \emph{DATA-CUBE-C-DATA-MODEL-CONSISTENCY-05} (45,514,102) (\emph{QB}), and \emph{SKOS-C-LANGUAGE-TAG-CARDINALITY-01} (2,508,903) (\emph{SKOS}).
We refer to the technical reports\footnotemark[\ref{technical-report-1}] \footnotemark[\ref{technical-report-2}] to get details about constraints on and the evaluation of \emph{XKOS} and \emph{PHDD} data sets.

\section{Related Work}

With RDF validation, one can overcome the drawbacks when validating XML documents.
Certain things cannot be validated using XSDs.
As a consequence, so-called secondary-level validation tools like Schematron have been introduced to overcome the limitations of XSDs.
Schematron generated validation rules and validates XML documents according to them.
It cannot be validated if each code of a variable's code list is associated with a category \tb{further explanation needed}.
It cannot be validated that if an element has a specific value, then certain child elements must be present.  

\tb{Achim, which kind of things cannot be validated using XSDs / we also need references}

\url{http://www.xmlmind.com/xmleditor/_distrib/doc/xmltool/xsd_structure_limitations.html}

\url{https://msdn.microsoft.com/en-us/library/aa468554.aspx}

\textbf{RDF Data Cube Vocabulary.}
A well-formed RDF Data Cube is an a RDF graph describing one or more instances of \emph{qb:DataSet} for which each of the 22 defined integrity constraints\footnote{http://www.w3.org/TR/vocab-data-cube/\#wf} passes.
Each integrity constraint is expressed as narrative prose and, where possible, a SPARQL ASK query or query template. 
If the ASK query is applied to an RDF graph then it will return true if that graph contains one or more \emph{QB} instances which violate the corresponding constraint \cite{CyganiakReynolds2014}.

\textbf{DCAT.}
\textcolor{blue}{are there already constraints defined for DCAT?}


\section{Conclusion and Future Work}

It is still ongoing work to identify and classify constraints on rectangular data (\emph{PHDD}) and statistical classifications (\emph{XKOS}) and to actually evaluate the quality of real world \emph{PHDD} and \emph{XKOS} data sets.

\bibliography{../../literature/literature}{}
\bibliographystyle{plain}
\setcounter{tocdepth}{1}
%\listoftodos
\end{document}
