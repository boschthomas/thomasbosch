
%% bare_conf.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_jrnl_compsoc.tex
%%*************************************************************************

% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices can trigger bugs that do  ***
% *** not appear when using other class files.                            ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
\documentclass[conference]{IEEEtran}
% Add the compsoc option for Computer Society conferences.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at: 
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found as epslatex.ps or
% epslatex.pdf at: http://www.ctan.org/tex-archive/info/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


%\usepackage{mdwmath}
%\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/mdwtools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/eqparbox/





% *** SUBFIGURE PACKAGES ***
%\usepackage[tight,footnotesize]{subfigure}
% subfigure.sty was written by Steven Douglas Cochran. This package makes it
% easy to put subfigures in your figures. e.g., "Figure 1a and 1b". For IEEE
% work, it is a good idea to load it with the tight package option to reduce
% the amount of white space around the subfigures. subfigure.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/obsolete/macros/latex/contrib/subfigure/
% subfigure.sty has been superceeded by subfig.sty.



%\usepackage[caption=false]{caption}
%\usepackage[font=footnotesize]{subfig}
% subfig.sty, also written by Steven Douglas Cochran, is the modern
% replacement for subfigure.sty. However, subfig.sty requires and
% automatically loads Axel Sommerfeldt's caption.sty which will override
% IEEEtran.cls handling of captions and this will result in nonIEEE style
% figure/table captions. To prevent this problem, be sure and preload
% caption.sty with its "caption=false" package option. This is will preserve
% IEEEtran.cls handing of captions. Version 1.3 (2005/06/28) and later 
% (recommended due to many improvements over 1.2) of subfig.sty supports
% the caption=false option directly:
%\usepackage[caption=false,font=footnotesize]{subfig}
%
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/
% The latest version and documentation of caption.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/caption/




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/



%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Documentation is contained in the stfloats.sty comments as well as in the
% presfull.pdf file. Do not use the stfloats baselinefloat ability as IEEE
% does not allow \baselineskip to stretch. Authors submitting work to the
% IEEE should note that IEEE rarely uses double column equations and
% that authors should try to avoid such use. Do not be tempted to use the
% cuted.sty or midfloat.sty packages (also by Sigitas Tolusis) as IEEE does
% not format its papers in such ways.





% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/misc/
% Read the url.sty source comments for usage information. Basically,
% \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}







% allows for temporary adjustment of side margins
\usepackage{chngpage}

% just makes the table prettier (see \toprule, \bottomrule, etc. commands below)
\usepackage{booktabs}

\usepackage[utf8]{inputenc}
%\usepackage[font=small,skip=0pt]{caption}

% footnotes
\usepackage{scrextend}

% colors
\usepackage[usenames, dvipsnames]{color}

% underline
\usepackage{tikz}
\newcommand{\udensdot}[1]{%
    \tikz[baseline=(todotted.base)]{
        \node[inner sep=1pt,outer sep=0pt] (todotted) {#1};
        \draw[densely dotted] (todotted.south west) -- (todotted.south east);
    }%
}%

\newcommand{\uloosdot}[1]{%
    \tikz[baseline=(todotted.base)]{
        \node[inner sep=1pt,outer sep=0pt] (todotted) {#1};
        \draw[loosely dotted] (todotted.south west) -- (todotted.south east);
    }%
}%

\newcommand{\udash}[1]{%
    \tikz[baseline=(todotted.base)]{
        \node[inner sep=1pt,outer sep=0pt] (todotted) {#1};
        \draw[dashed] (todotted.south west) -- (todotted.south east);
    }%
}%

\newcommand{\udensdash}[1]{%
    \tikz[baseline=(todotted.base)]{
        \node[inner sep=1pt,outer sep=0pt] (todotted) {#1};
        \draw[densely dashed] (todotted.south west) -- (todotted.south east);
    }%
}%

\newcommand{\uloosdash}[1]{%
    \tikz[baseline=(todotted.base)]{
        \node[inner sep=1pt,outer sep=0pt] (todotted) {#1};
        \draw[loosely dashed] (todotted.south west) -- (todotted.south east);
    }%
}%

% URL handling
\usepackage{url}
\urlstyle{same}

%\usepackage{makeidx}  % allows for indexgeneration

%\usepackage{amsmath}
\usepackage{amsmath, amssymb}
\usepackage{mathabx}
\usepackage{caption} 
\captionsetup[table]{skip=10pt}

% monospace within text
\newcommand{\ms}[1]{\texttt{#1}}

% examples
\usepackage{fancyvrb}
\DefineVerbatimEnvironment{ex}{Verbatim}{numbers=left,numbersep=2mm,frame=single,fontsize=\scriptsize}

\usepackage{xspace}
% Einfache und doppelte Anfuehrungszeichen
\newcommand{\qs}{``} 
\newcommand{\qe}{''\xspace} 
\newcommand{\sqs}{`} 
\newcommand{\sqe}{'\xspace} 

% checkmark
\usepackage{tikz}
\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;} 

% Xs
\usepackage{pifont}

% Tabellenabstände kleiner
\setlength{\intextsep}{10pt} % Vertical space above & below [h] floats
\setlength{\textfloatsep}{10pt} % Vertical space below (above) [t] ([b]) floats
% \setlength{\abovecaptionskip}{0pt}
% \setlength{\belowcaptionskip}{0pt}

\usepackage{tabularx}
\newcommand{\hr}{\hline\noalign{\smallskip}} % für die horizontalen linien in tabellen

% Todos
\usepackage[colorinlistoftodos]{todonotes}
\newcommand{\ke}[1]{\todo[size=\small, color=red!40]{\textbf{Kai:} #1}}
\newcommand{\tb}[1]{\todo[size=\small, color=green!40]{\textbf{Thomas:} #1}}
\newcommand{\bz}[1]{\todo[size=\small, color=blue!40]{\textbf{Ben:} #1}}

\newenvironment{table-1cols}{
  \scriptsize
  \sffamily
  \vspace{0.3cm}
  \begin{tabular}{l}
  \hline
  \textbf{Requirements} \\
  \hline

}{
  \hline
  \end{tabular}
  \linebreak
}

\newenvironment{table-2cols}{
  \scriptsize
  \sffamily
  \vspace{0.3cm}
  \begin{tabular}{l|l}
  \hline
  \textbf{Requirements} & \textbf{Covering DSCLs} \\
  \hline

}{
  \hline
  \end{tabular}
  \linebreak
}

\newenvironment{complexity}{
  %\scriptsize
  %\sffamily
  %\vspace{0.3cm}
  \begin{tabular}{l|l}
  \hline
  \textbf{Complexity Class} & \textbf{Complexity} \\
  \hline

}{
  \hline
  \end{tabular}
  \linebreak
}

\newenvironment{DL}{
  %\scriptsize
  %\sffamily
  \small
  \vspace{0cm}
	\begin{center}
  \begin{tabular}{c l}

}{
  \end{tabular}
	\end{center}
}


\newenvironment{evaluation}{
  %\scriptsize
  %\sffamily
  %\vspace{0.3cm}
  \begin{tabular}{l|c|c|c|c|c|c}
  \hline
  \textbf{Constraint Class} & \textbf{DSP} & \textbf{OWL2-DL} & \textbf{OWL2-QL} & \textbf{ReSh} & \textbf{ShEx} & \textbf{SPIN} \\
  \hline

}{
  \hline
  \end{tabular}
  \linebreak
}

\newenvironment{constraint-languages-complexity}{
  %\scriptsize
  %\sffamily
  %\vspace{0.3cm}
  \begin{tabular}{l|c|c|c|c|c|c}
  \hline
  \textbf{Complexity Class} & \textbf{DSP} & \textbf{OWL2-DL} & \textbf{OWL2-QL} & \textbf{ReSh} & \textbf{ShEx} & \textbf{SPIN} \\
  \hline

}{
  \hline
  \end{tabular}
  \linebreak
}

\newenvironment{user-fiendliness}{
  %\scriptsize
  %\sffamily
  %\vspace{0.3cm}
  \begin{tabular}{l|c|c|c|c|c}
  \hline
  \textbf{criterion} & \textbf{DSP} & \textbf{OWL2} & \textbf{ReSh} & \textbf{ShEx} & \textbf{SPIN} \\
  \hline

}{
  \hline
  \end{tabular}
  \linebreak
}

\setcounter{secnumdepth}{5}

% tables
\usepackage{array,graphicx}
\usepackage{booktabs}
\usepackage{pifont}
\newcommand*\rot{\rotatebox{90}}
\newcommand*\OK{\ding{51}}
\usepackage{booktabs}
\newcommand*\ON[0]{$\surd$}

\usepackage{tablefootnote}

\usepackage{float}

\usepackage{ntheorem}
\newtheorem{hyp}{Finding}

\makeatletter
\newcounter{subhyp} 
\let\savedc@hyp\c@hyp
\newenvironment{subhyp}
 {%
  \setcounter{subhyp}{0}%
  \stepcounter{hyp}%
  \edef\saved@hyp{\thehyp}% Save the current value of hyp
  \let\c@hyp\c@subhyp     % Now hyp is subhyp
  \renewcommand{\thehyp}{\saved@hyp\alph{hyp}}%
 }
 {}
\newcommand{\normhyp}{%
  \let\c@hyp\savedc@hyp % revert to the old one
  \renewcommand\thehyp{\arabic{hyp}}%
} 
\makeatother

\usepackage{multirow}

\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
%\title{Aspects of RDF Data Constraints in the Social, behavioral, and Economic Sciences}
\title{Validating RDF Data Quality using Constraints to Direct the Development of Constraint Languages}

% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
%\author{\IEEEauthorblockN{Thomas Bosch}
%\IEEEauthorblockA{School of Electrical and\\Computer Engineering\\
%Georgia Institute of Technology\\
%Atlanta, Georgia 30332--0250\\
%Email: http://www.michaelshell.org/contact.html}
%\and
%\IEEEauthorblockN{Homer Simpson}
%\IEEEauthorblockA{Twentieth Century Fox\\
%Springfield, USA\\
%Email: homer@thesimpsons.com}
%\and
%\IEEEauthorblockN{James Kirk\\ and Montgomery Scott}
%\IEEEauthorblockA{Starfleet Academy\\
%San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212\\
%Fax: (888) 555--1212}}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
\author{\IEEEauthorblockN{Thomas Hartmann\IEEEauthorrefmark{1},
Benjamin Zapilko\IEEEauthorrefmark{1},
Joachim Wackerow\IEEEauthorrefmark{1}, 
Kai Eckert\IEEEauthorrefmark{2}} %and
%Eldon Tyrell\IEEEauthorrefmark{4}}
\IEEEauthorblockA{\IEEEauthorrefmark{1}GESIS - Leibniz Institute for the Social Sciences, Mannheim, Germany\\
Email: \{firstname.lastname\}@gesis.org}
\IEEEauthorblockA{\IEEEauthorrefmark{2}Stuttgart Media University, Stuttgart, Germany\\
Email: eckert@hdm-stuttgart.de}}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}




% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle


\begin{abstract}
%\boldmath
For research institutes, data libraries, and data archives,
RDF data validation according to predefined constraints is a much sought-after feature, 
particularly as this is taken for granted in the XML world.
Based on our work in the DCMI RDF Application Profiles Task Group and in cooperation with the W3C Data Shapes Working Group, we identified and published by today 81 types of constraints that are required by various stakeholders for data applications.
In this paper, in collaboration with several domain experts we formulate 115 constraints on three different vocabularies (DDI-RDF, QB, and SKOS) and classify them according to (1) the severity of an occurring violation and (2) the complexity of the constraint expression in common constraint languages. We evaluate the data quality of 15,694 data sets (4.26 billion triples) of research data for the social, behavioral, and economic sciences obtained from 33 SPARQL endpoints.
Based on the results, we formulate several findings to direct the further development of constraint languages.
\end{abstract}
% IEEEtran.cls defaults to using nonbold math in the Abstract.
% This preserves the distinction between vectors and scalars. However,
% if the conference you are submitting to favors bold math in the abstract,
% then you can use LaTeX's standard command \boldmath at the very start
% of the abstract to achieve this. Many IEEE journals/conferences frown on
% math in the abstract anyway.

% no keywords




% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
% no \IEEEPARstart


%\hfill mds
 
%\hfill January 11, 2007

%%% Findings aus dem Abstract
%%% (1) for well-established vocabularies, the explicitly defined constraints are almost completely satisfied which demonstrates that constraint formulation in general works. (2) A significant amount of 47\% of the violations refer to complex constraints that are not easily expressible in existing languages which confirms the necessity to provide suitable constraint languages.  (3) The percentage of severe constraint violations is very low, compared to about 2/3 of warning violations and 1/3 of informational violations, which implies that proper constraint languages can significantly improve the data quality beyond fundamental requirements.

For constraint formulation and RDF data validation, several languages exist or are currently developed. \emph{Shape Expressions (ShEx)}, \emph{Resource Shapes (ReSh)}, \emph{Description Set Profiles (DSP)}, \emph{OWL 2}, the \emph{SPARQL Inferencing Notation (SPIN)}, and \emph{SPARQL} are the six most promising and widely used constraint languages. OWL 2 is used as a constraint language under the closed-world and unique name assumptions. The W3C currently develops \emph{SHACL}, an RDF vocabulary for describing RDF graph structures. With its direct support of validation via SPARQL, SPIN is very popular and certainly plays an important role for future developments in this field. It is particularly interesting as a means to validate arbitrary constraint languages by mapping them to SPARQL \cite{BoschEckert2014-2}. Yet, there is no clear favorite and none of the languages is able to meet all requirements raised by data practitioners. Further research and development therefore is needed.

In 2013, the W3C organized the RDF Validation Workshop,\footnote{\url{http://www.w3.org/2012/12/rdf-val/}} 
where experts from industry, government, and academia discussed first use cases for constraint formulation and RDF data validation.
In 2014, two working groups on RDF validation have been established to develop a language to express constraints on RDF data: 
the \emph{W3C RDF Data Shapes Working Group}\footnote{\url{http://www.w3.org/2014/rds/charter}} (33 participants of 19 organizations) and the \emph{DCMI RDF Application Profiles Task Group}\footnote{\url{http://wiki.dublincore.org/index.php/RDF-Application-Profiles}} (29 people of 22 organizations) which among others bundles the requirements of data institutions of the cultural heritage sector and the \emph{social, behavioral, and economic (SBE)} sciences and represents them in the W3C group. 

Within the DCMI task group, a collaboratively curated database of RDF validation requirements\footnote{Online available at: \url{http://purl.org/net/rdf-validation}} has been created which contains the findings of the working groups based on various case studies provided by data institutions \cite{BoschEckert2014}. It is publicly available and open for further contributions.
The database connects requirements to use cases, case studies, and implementations and forms the basis of this paper. 
We distinguish 81 requirements to formulate constraints on RDF data; 
each of them corresponding to a constraint type.

In this paper, we collected constraints for commonly used vocabularies in the SBE domain (see Section \ref{sbe-vocabularies}), either from the vocabularies themselves or from domain and data experts, in order to gain a better understanding about the role of certain requirements for data quality and to direct the further development of constraint languages. All in all, this lead to 115 constraints on three vocabularies.
We let the experts classify the constraints according to the severity of their violation. Furthermore, we classified each constraint type based on whether it is expressible by RDFS/OWL, common high-level constraint languages, or only by plain SPARQL (see Chapter~\ref{classification}).

As we do not want to base our conclusions on the evaluation of vocabularies and constraint definitions alone, we conducted a large-scale experiment.
For all these 115 constraints, we evaluated the data quality of 15,694 data sets (4.26 billion triples) of SBE research data on three common vocabularies in SBE sciences (DDI-RDF, QB, SKOS) obtained from 33 SPARQL endpoints.
%evaluated the data quality of 15,694 data sets (4.26 billion triples) of research data for the social, behavioral, and economic (SBE) sciences obtained from 33 SPARQL endpoints.
Based on the evaluation results,
we formulate several findings to direct the further development of constraint languages.
To make valid general statements for all vocabularies, however,
these findings still have to be verified or falsified
by evaluating the quality of data represented by more than three vocabularies (Section \ref{evaluation}).
%The \textbf{contribution} of this paper is the development of a system 
%\begin{enumerate}
	%\item to classify RDF constraints (according to their severity levels) to evaluate the quality of metadata and data which may be represented by any vocabulary and 
	%\item to classify RDF constraint types (according to their complexity) which in most cases correspond to RDF validation requirements\footnote{For simplicity reasons, we use the terms \emph{constraint types} and \emph{constraints} instead of \emph{RDF constraint types} and \emph{RDF constraints} in the rest of the paper} (sections \ref{classification} - \ref{SPARQL-based-constraint-types}).
%\end{enumerate}
%By defining a huge amount of constraints of the majority of the constraint types,   
%we apply the developed classification system to several and different vocabularies from the SBE domain to represent both metadata and data and therefore prove its generality.
%A complex and complete real world running example from the SBE domain serves to prove the claim that the developed classification system perfectly applies for diverse vocabularies.
%%, i.e., that it does not matter which vocabularies are used to represent (meta)data and for which domains constraints are defined.
%We describe why RDF validation is important for the SBE community (section \ref{motivation}), 
%how data in tabular format and metadata on unit-record data sets, aggregated data sets, and thesauri are represented in RDF, 
%how therefore reused vocabularies are interrelated (section \ref{rdf-representation}),
%and how SBE (meta)data is validated against constraints of different constraint types (sections \ref{classification} - \ref{SPARQL-based-constraint-types}).
%We evaluated the (meta)data quality of large real world data sets (more than 4.2 billion triples and 15 thousand data sets) from the SBE domain represented by multiple vocabularies to get an understanding 
%(1) which sets of constraint types (on different levels of complexity) and 
%(2) which sets of constraints (associated with particular severity levels) encompass the constraints causing the most/fewest constraint violations (see section \ref{evaluation}).

In this paper, we discuss constraints on RDF data in general. Note that the data represented in RDF can be data in the sense of SBE sciences, but also metadata about published or unpublished data. We generally refer to both simply as RDF data and only distinguish between data and metadata in the data set descriptions. 
%and in the case that it matters for the purpose of this paper.

%We propose an extensible metric to measure the continuum of severity levels to indicate how serious the violation of given constraints is.
%Constraints are instantiated from constraint types in order to validate both metadata and data represented by any vocabulary. 
%As constraint types are used to define constraints on (meta)data expressed by any vocabulary, the proposed constraint classification can be applied generically, i.e. vocabulary-independent. 

%\section{Running Example}
%\label{running-example}
\section{Common Vocabularies in SBE Sciences}
\label{sbe-vocabularies}

We took all well-established and newly developed SBE vocabularies into account and defined constraints for three vocabularies commonly used in the SBE sciences which are briefly introduced in the following. We analyzed actual data according to constraint violations, as for these vocabularies large data sets are already published.

SBE sciences require high-quality data for their empirical research. For more than a decade, members of the SBE community have been developing and using a
metadata standard, composed of almost twelve hundred metadata fields, known as the \emph{Data Documentation Initiative (DDI)}, \footnote{\url{http://www.ddialliance.org/Specification/}}
an XML format to disseminate, manage,
and reuse data collected and archived for research. 
%\cite{Vardigan-2008}. 
In XML, the definition of schemas containing constraints and the validation of data according to these constraints is commonly used to ensure a certain level of data quality.
With the rise of the Web of Data, data professionals and institutions are very interested in publishing their data directly in RDF or at least publish accurate metadata about their data to facilitate discovery and reuse. Therefore, not only established vocabularies like SKOS are used; 
recently, members of the SBE and Linked Data community developed with the \emph{DDI-RDF Discovery Vocabulary (DDI-RDF)}\footnote{\url{http://rdf-vocabulary.ddialliance.org/discovery.html}} a means to expose \emph{DDI} metadata as Linked Data. 

The data most often used in research within SBE sciences is \emph{unit-record data}, i.e., data collected about individuals, businesses, and households, in form of responses to studies or taken from administrative registers such as hospital records or registers of births and deaths. A \emph{study} represents the process by which a data set was generated or collected. The range of unit-record data is very broad - including census, education, health data and business, social, and labor force surveys. This type of research data is held within data archives or data libraries after it has been collected, so that it may be reused by future researchers. By its nature, unit-record data is highly confidential and access is often only permitted for qualified researchers who must apply for access. Researchers typically represent their results as aggregated data in form of multi-dimensional tables with only a few columns: so-called \emph{variables} such as \emph{sex} or \emph{age}. Aggregated data, which answers particular research questions, is derived from unit-record data by statistics on groups or aggregates such as frequencies and arithmetic means. The purpose of publicly available aggregated data is to get a first overview and to gain an interest in further analyses of the underlying unit-record data. For more detailed analyses, researchers refer to unit-record data including additional variables needed to answer subsequent research questions. 

\emph{Formal childcare} is an example of an aggregated variable which captures the measured availability of childcare services in percent over the population in European Union member states by 
the dimensions \emph{year}, \emph{duration}, \emph{age} of the child, and \emph{country}.
Variables are constructed out of values (of one or multiple datatypes) and/or code lists.
The variable \emph{age}, e.g., may be represented by values of the datatype \emph{xsd:nonNegativeInteger} or by a code list of age clusters (e.g., '0 to 10' and '11 to 20'). 
The \emph{RDF Data Cube Vocabulary (QB)}\footnote{http://www.w3.org/TR/vocab-data-cube/} is a W3C recommendation for representing \emph{data cubes}, i.e., multi-dimensional aggregated data, in RDF \cite{Cyganiak2010}. 
%QB represents metadata on multi-dimensional aggregate data in two files: a \emph{qb:DataSet} and a \emph{qb:DataStructureDefinition}.
A \emph{qb:DataStructureDefinition} contains metadata of the data collection.
The variable \emph{formal childcare} is modeled as \emph{qb:measure}, since it stands for what has been measured in the data collection.
\emph{Year}, \emph{duration}, \emph{age}, and \emph{country} are \emph{qb:dimensions}.
Data values, i.e., the availability of childcare services in percent over the population, are collected in a \emph{qb:DataSet}. 
Each data value is represented inside a \emph{qb:Observation} which contains values for each dimension. 
%(e.g., the year in which \emph{formal childcare} has been determined).
%\ke{Wieso springt das von einer sehr knappen Einführung direkt zu konkreten (beliebigen?) Variablen? Ich möchte wissen, wofür QB benutzt wird und gerne ein konkretes %Beispiel sehen, um mir einen Data Cube vorstellen zu können.}\tb{hmm, ich halte das beispiel für ein knappes und verständliches beipsiel wie eine tabelle in qb dargestellt wird / es ist auch der link enthalten zu der tabelle von eurostat}

For more detailed analyses we refer to the underlying unit-record data. The aggregated variable \emph{formal childcare} is calculated on the basis of six unit-record variables (i.a., \emph{Education at pre-school}) for which detailed metadata is given (i.a., code lists) enabling researchers to replicate the results shown in aggregated data tables.
\emph{DDI-RDF} is used to represent metadata on unit-record data in RDF.
%The series (\emph{DDI-RDF:StudyGroup}) \emph{EU-SILC} contains one study (\emph{DDI-RDF:Study}) for each year (\emph{dcterms:temporal}) of data collection.   
%\emph{dcterms:spatial} points to the countries for which the data has been collected.
The study (\emph{disco:Study}) for which the unit-record data has been collected 
%\emph{EU-SILC 2011} 
contains eight data sets (\emph{disco:LogicalDataSet})
including variables (\emph{disco:Variable}) like the six ones needed to calculate the variable \emph{formal childcare}.
%Metadata on unit-record data enables researchers to investigate further research questions based on promising findings of other researchers in form of aggregated data.
%One common research question is ,e.g., the comparison of variables like 
%\emph{formal childcare} between countries, for which the variable is collected within the context of an individual study, and other European or non European countries (e.g. OSCE).

The \emph{Simple Knowledge Organization System (SKOS)} is reused to a large extend to build SBE vocabularies.
The codes of the variable \emph{Education at pre-school} are modeled as \emph{skos:Concepts} and 
a \emph{skos:OrderedCollection} organizes them in a particular order within a \emph{skos:memberList}.
A variable may be associated with a theoretical concept (\emph{skos:Concept}) and \emph{skos:narrower} builds the hierarchy of theoretical concepts within a \emph{skos:ConceptScheme} of a study.
The variable \emph{Education at pre-school} is assigned to the theoretical concept \emph{Child Care} which is a narrower concept of the top concept \emph{Education}.
Controlled vocabularies (\emph{skos:ConceptScheme}), serving as extension and reuse mechanism,
organize types (\emph{skos:Concept}) of descriptive statistics (\emph{disco:SummaryStatistics}) like minimum, maximum, and arithmetic mean.





%% ---------------
%% Backup of 'Common Vocabularies in SBE Sciences'
%
%
%\section{Common Vocabularies in SBE Sciences}
%\label{rdf-representation}
%
%SBE sciences require high-quality data for their empirical research. For more than a decade, members of the SBE community have been developing and using a
%metadata standard, composed of almost twelve hundred metadata fields, known as the \emph{Data Documentation Initiative (DDI)},
%an XML format to disseminate, manage,
%and reuse data collected and archived for research \cite{Vardigan2008}. 
%In XML, the definition of schemas containing data constraints and the validation of data according to these constraints is commonly used to ensure a certain level of data quality.
%With the rise of the Web of Data, data professionals and institutions are very interested in having their data be discovered and used by publishing their data directly in RDF or at least publish accurate metadata about their data to facilitate data integration. Therefore, not only established vocabularies like SKOS are used; 
%recently, members of the SBE and Linked Data community developed with the \emph{DDI-RDF Discovery Vocabulary (DDI-RDF)}\footnote{\url{http://rdf-vocabulary.ddialliance.org/discovery.html}} a means to expose \emph{DDI} metadata as Linked Data. 
%
%The data most often used in research within SBE sciences is \emph{unit-record data}, i.e., data collected about individuals, businesses, and households, in form of responses to studies or taken from administrative registers
%such as hospital records, registers of births and deaths. 
%The range of unit-record data is very broad - 
%including census, education, health data and business, social, and labor force surveys.  
%This type of research data is
%held within data archives or data libraries after it has been collected, so that it may be
%reused by future researchers. 
%
%By its nature, unit-record data is highly confidential and access is often only permitted for qualified researchers who must apply for access. 
%Researchers typically represent their results as aggregated data in form of multi-dimensional tables with only a few columns; so-called \emph{variables} such as \emph{sex} or \emph{age}.
%Aggregated data, which answers particular research questions, is derived from unit-record data by statistics on groups or aggregates such as frequencies and arithmetic means.
%The purpose of publicly available aggregated data is to get a first overview and to gain an interest in further analyses on the underlying unit-record data.
%Aggregated data is published in form of CSV files,
%allowing to perform calculations on the data.
%
%For more detailed analyses, researchers refer to unit-record data including additional variables needed to answer subsequent research questions
%like the comparison of studies between countries.
%A \emph{study} represents the process by which a data set was generated or collected.
%Eurostat\footnote{\url{http://ec.europa.eu/eurostat}}, the statistical office of the European Union, provides research findings in form of aggregated data (downloadable as CSV files) and its metadata at European level that enable comparisons between countries.
%The variable \emph{formal childcare}\footnote{Aggregated data and its metadata is available at: \url{http://ec.europa.eu/eurostat/web/products-datasets/-/ilc_caindformal}} %(in contrast to childcare at home)
%captures the measured availability of childcare services in percent over the population in European Union member states by 
%the variables \emph{year}, \emph{duration} (in hours per week), \emph{age} of the child, and \emph{country}.
%Variables are constructed out of values (of one or multiple datatypes) and/or code lists.
%The variable \emph{age}, e.g., may be represented by values of the datatype \emph{xsd:nonNegativeInteger} or by a code list of age clusters (e.g., '0 to 10' and '11 to 20'). 
%
%A very important and representative RDF validation case study within SBE sciences is the comparison of variables between data collections of different countries.
%Several vocabulary-specific constraints on RDF data are checked for each data collection
%to determine if variables measuring \emph{age} 
%- collected for different countries (\emph{$age_{DE}$}, \emph{$age_{UK}$}) - 
%are comparable:
%(1) variable definitions must be available, 
%(2) for each code a human-readable label has to be specified,
%(3) code lists must be structured properly, and
%(4) code lists must either be identical or at least similar.
%If a researcher only wants to get a first overview over the comparability of variables (use case 1), 
%covering the first three constraints may be sufficient,
%i.e., the violation of the first three constraints is more serious than the violation of the last constraint.
%If the intention of the researcher is to perform more sophisticated comparisons (use case 2), however, the user may raise the severity level of the last constraint.
%
%We took all well-established and newly developed SBE vocabularies into account and defined constraints for six vocabularies commonly used in or developed for the SBE sciences which are briefly introduced in the following.
%For three of them, we analyzed actual data according to constraint violations, as for these vocabularies large data sets are already published.
%
%The \emph{RDF Data Cube Vocabulary (QB)}\footnote{http://www.w3.org/TR/vocab-data-cube/} is a W3C recommendation for representing \emph{data cubes}, i.e. multi-dimensional aggregated data, in RDF \cite{Cyganiak2010}. 
%A \emph{qb:DataStructureDefinition} contains metadata of the data collection.
%The variable \emph{formal childcare}, e.g., is modeled as \emph{qb:measure}, since it stands for what has been measured in the data collection.
%The variables \emph{year}, \emph{duration}, \emph{age}, and \emph{country} are \emph{qb:dimensions}.
%Data values, i.e., the availability of childcare services in percent over the population, are collected in a \emph{qb:DataSet}. 
%Each data value is represented inside a \emph{qb:Observation} which contains values for each dimension\footnote{The complete running example in RDF is available at: \url{https://github.com/boschthomas/rdf-validation/tree/master/data/running-example}}. 
%
%\emph{Physical Data Description (PHDD)}\footnote{\url{https://github.com/linked-statistics/physical-data-description}} is a vocabulary to represent data in tabular format in RDF enabling further aggregations and calculations. 
%The data could be either represented in records with character-separated values (CSV) or fixed length. 
%
%For more detailed analyses we refer to the unit-record data collected for the series \emph{EU-SILC (European Union Statistics on Income and Living Conditions)}\footnote{\url{http://www.gesis.org/missy/eu/metadata/EU-SILC}}. 
%Where data collection is cyclic, data sets may be released as \emph{series}, 
%where each cycle produces one or more data sets. 
%The aggregated variable \emph{formal childcare} is calculated on the basis of six unit-record variables 
%(e.g., \emph{Education at pre-school})
%for which detailed metadata is given 
%(e.g., code lists)
%enabling researchers to replicate the results shown in aggregated data tables.
%The \emph{DDI-RDF Discovery Vocabulary (DDI-RDF)} is a vocabulary to represent metadata on unit-record data in RDF.
%The series (\emph{DDI-RDF:StudyGroup}) \emph{EU-SILC} contains one study (\emph{DDI-RDF:Study}) for each year (\emph{dcterms:temporal}) of data collection.   
%\emph{dcterms:spatial} points to the countries for which the data has been collected.
%The study \emph{EU-SILC 2011} contains eight unit-record data sets (\emph{DDI-RDF:LogicalDataSet})
%including unit-record variables (\emph{DDI-RDF:Variable}) like the six ones needed to calculate the aggregated variable \emph{formal childcare}.
%
%The \emph{Simple Knowledge Organization System (SKOS)} is reused multiple times to build SBE vocabularies.
%The codes of the variable \emph{Education at pre-school} (number of education hours per week) are modeled as \emph{skos:Concepts} and 
%a \emph{skos:OrderedCollection} organizes them in a particular order within a \emph{skos:member} \emph{List}.
%A variable may be associated with a theoretical concept (\emph{skos:Concept}). 
%\emph{skos:narrower} builds the hierarchy of theoretical concepts within a \emph{skos:Concept} \emph{Scheme} of a series.
%The variable \emph{Education at pre-school} is assigned to the theoretical concept \emph{Child Care} which is the narrower concept of \emph{Education} - one of the top concepts of the series \emph{EU-SILC}.
%Controlled vocabularies (\emph{skos:ConceptScheme}), serving as extension and reuse mechanism,
%organize types (\emph{skos:Concept}) of descriptive statistics (\emph{DDI-RDF:SummaryStatistics}) like minimum, maximum, and arithmetic mean.
%\emph{XKOS}\footnote{\url{https://github.com/linked-statistics/xkos}} is a SKOS extension to describe formal statistical classifications like the International Standard Classification of Occupations (\emph{ISCO}). 
%and the Statistical Classification of Economic Activities in the European Community \emph{NACE}.
%\emph{DCAT} enables to represent data sets inside of data collections like portals, repositories, catalogs, and archives
%which serve as typical entry points when searching for data.





% Backup of 'Common Vocabularies in SBE Sciences'
% -----

\section{Related Work}
\label{related-work}

%\textbf{XML vs. RDF Validation.}
For data archives, research institutes, and data libraries,
RDF validation according to predefined constraints is a much sought-after feature, 
particularly as this is taken for granted in the XML world.
DDI-XML documents, e.g., are validated against diverse XML Schemas.
%\footnote{\url{http://www.ddialliance.org/Specification/}}.
As certain constraints cannot be formulated and validated by XML Schemas, 
so-called secondary-level validation tools like \emph{Schematron}\footnote{\url{https://msdn.microsoft.com/en-us/library/aa468554.aspx}} 
have been introduced to overcome the limitations of XML validation.
Schematron generates validation rules and validates XML documents according to them.
With RDF validation, one can overcome the drawbacks when validating XML documents.\footnote{\url{http://www.xmlmind.com/xmleditor/_distrib/doc/xmltool/xsd_structure_limitations.html}}
It cannot be validated, e.g., if each code of a variable's code list is associated with a category and that if an element has a specific value then certain child elements must be present.  

A well-formed \emph{RDF Data Cube} is an RDF graph describing one or more instances of \emph{qb:DataSet} for which each of the 22 integrity constraints,\footnote{http://www.w3.org/TR/vocab-data-cube/\#wf} defined within the QB specification, passes.
Each integrity constraint is expressed as narrative prose and, where possible, as a SPARQL ASK query or query template. 
If the ASK query is applied to an RDF graph then it will return true if that graph contains one or more QB instances which violate the corresponding constraint.

\cite{MaderHaslhoferIsaac2012} investigated how to support
taxonomists in improving SKOS vocabularies by pointing out quality
issues that go beyond the integrity constraints defined in the SKOS specification.
 
\emph{Stardog ICV} and \emph{Pellet ICV} use OWL 2 constructs to formulate constraints. \emph{OWL} in its current version 2 is an expressive language which is based on formal logic and on the subject-predicate-object triples from RDF. OWL offers knowledge representation and reasoning services in combination with \emph{SWRL}. Validation, however, is not the primary purpose of its design which has lead to claims that OWL cannot be used for validation. \cite{Motik-2007} and \cite{Motik-2009}, e.g., discuss the differences between constraints and RDFS/OWL axioms. In practice, however, OWL is well-spread and RDFS/OWL constructs are widely used to tell people and applications about how valid instances should look like. In general, RDF documents follow the semantics of RDFS/OWL ontologies which could therefore not only be used for reasoning but also for validation. 

The semantics which is applied for RDF validation is CWA/UNA.
RDF validation requires that different names represent different objects ({\em unique name assumption (UNA)}), whereas
OWL is based on the {\em non-unique name assumption (nUNA)}.  
%A data property is \emph{functional} (\emph{R-65}) if for each individual \emph{x}, there can be at most one distinct literal \emph{y} such that \emph{x} is connected by the data property to \emph{y}. (See \cite{resource} for this terminology.)
%When assuming UNA, the functional property {\em title} (\ms{funct (title)}) causes a clash
%in case the book {\em Huckleberry-Finn} has more than one \emph{title}
%(\ms{title(Huckleberry-Finn, The-Adventures-of-Huckleberry-Finn)}, \ms{title(Huckleberry-Finn, Die-Abenteuer-des-Huckleberry-Finn)}).
%Assuming nUNA, however, reasoning concludes that the titles {\em The-Adventures-of-Huckleberry-Finn} and {\em Die-Abenteuer-des-Huckleberry-Finn} are the same (\ms{\{The-Adventures-of-Huckleberry-Finn\}} = \ms{\{Die-Abenteuer-des-Huckle} \ms{berry-}\ms{Finn\}}).  
Reasoning in OWL is based on the {\em open-world assumption (OWA)}, i.e., a statement cannot be inferred to be false if it cannot be proved to be true. 
%which fits its primary design purpose to represent knowledge on the World Wide Web. 
%Most of the constraint languages except OWL, have a major shortcoming of redundancy.
%With its open-world semantics, used as a constraint language, OWL solves the redundancy problem. 
%\tb{the following example does not show the redundancy problem!}
%\tb{2. benefit: reasoning solves redundancy problem}
%As each \emph{book} must have a \emph{title} (\ms{Book $\sqsubseteq \exists$ title.$\top$}) and {\em Hamlet} is a \emph{book} (\ms{Book(Hamlet)}),
%{\em Hamlet} must have at least one \emph{title}.
%In an OWA setting, this axiom does not cause a constraint violation (even if there is no explicitly defined \emph{title}), since there must be a \emph{title} for this \emph{book} which we may not know. 
On the other hand, RDF validation scenarios require the {\em closed-world assumption (CWA)}, i.e., a statement is inferred to be false if it cannot be proved to be true. 
This ambiguity in semantics is one of the main reasons why OWL has not been adopted as a standard constraint language for RDF validation in the past.  
\cite{Tao-2010} propose an alternative semantics for OWL using CWA/UNA so that it could be used to validate integrity constraints.
\cite{Patel-Schneider-2015} claims that DL and therefore OWL axioms can be interpreted in a closed-world setting and used for constraint checking.
When using OWL axioms in terms of constraints, we adopt the same semantics that is used for RDF validation.

\section{Classification of Constraint Types and Constraints}
\label{classification}

To gain better insights into the role that certain types of constraints play for the quality of RDF data, we use two simple classifications: on the one hand, we classify RDF constraint types whether they are expressible by different types of constraint languages and on the other hand, we classify constraints formulated for a given vocabulary according to the perceived severity of their violation. 

Within the working groups, we identified by today 81 requirements to formulate RDF constraints (e.g., \emph{R-75: minimum qualified cardinality restrictions}); each of them corresponding to an RDF constraint type.\footnote{Constraint types are uniquely identified by alphanumeric technical identifiers like \emph{R-71-CONDITIONAL-PROPERTIES}} 
Within a technical report, we explain each requirement/constraint type in detail and give examples for each expressed by different constraint languages \cite{BoschNolleAcarEckert2015}. We provide mappings to representations in Description Logics (DL) \cite{Baader-2003} to logically underpin each requirement and to determine which DL constructs are needed to express each constraint type.
%In total, we distinguish 81 RDF constraint types as we identified 6 RDF constraint types which are only necessary for validating particular vocabularies.
%Constraints are instantiated from constraint types in order to validate both metadata and data represented by any vocabulary; thus, the proposed classification system is vocabulary-independent and therefore applicable generically.
For the three vocabularies, several SBE domain experts determined the default severity level of the 115 concrete constraints, which we published in a technical report \cite{BoschZapilkoWackerowEckert2015}.
In the following, we summarize the classifications of constraint types and constraints for the purpose of our evaluation.

\subsection{Classification of Constraint Types according to the Expressivity of Constraint Languages}

According to the expressivity of constraint languages, the complete set of constraint types encompasses three not disjoint sets of constraint types:
\begin{enumerate}
	\item \emph{RDFS/OWL Based}
	\item \emph{Constraint Language Based}
	\item \emph{SPARQL Based}
\end{enumerate}

\textbf{\emph{RDFS/OWL Based}.}
The modeling languages RDFS and OWL are typically used to formally specify vocabularies. \emph{RDFS/OWL Based} denotes the set of constraint types which can be formulated with RDFS/OWL axioms which we use in terms of constraints with CWA/UNA semantics and without reasoning.\footnote{The entailment regime is to be decided by the implementers. It is our point that reasoning affects validation and that a proper definition of the reasoning to be applied is needed.} RDFS/OWL axioms are commonly found within formal specifications of vocabularies. \emph{RDFS/OWL Based} constraints generally can be seen as a basic level of constraints ensuring that the data is consistent with the formally and explicitly specified intended semantics as well as the integrity of vocabularies' conceptual models about data.

%The {\em existential quantification} (\emph{R-86}) 
%\ms{Study $\equiv$ $\exists$ product.LogicalDataSet} (DL), e.g., restricts studies to contain at least one data set which is expressible in OWL 2:
Constraints of the type \emph{minimum qualified cardinality restrictions} (\emph{R-75}), e.g., guarantee that individuals of given classes are connected by particular properties to at least n different individuals/literals of certain classes or data ranges. For \emph{DDI-RDF}, a \emph{minimum qualified cardinality restriction} can be obtained from a respective OWL axiom which ensures that a \emph{disco:Questionnaire} has (\emph{disco:question}) at least one \emph{disco:Question}:
\begin{ex}
[   a owl:Restriction ; rdfs:subClassOf Questionnaire ;
    owl:minQualifiedCardinality 1 ;
    owl:onProperty question ;
    owl:onClass Question ] .
\end{ex}

%In \emph{PHDD}, a \emph{minimum qualified cardinality restriction} can be obtained from a respective OWL axiom which ensures that a \emph{phdd:TableStructure} has (\emph{phdd:column}) at least one \emph{phdd:Column}:
%\begin{ex}
%[   a owl:Restriction ; rdfs:subClassOf TableStructure ;
    %owl:minQualifiedCardinality 1 ;
    %owl:onProperty column ;
    %owl:onClass Column ] .
%\end{ex}

\emph{Constraint Language Based} and \emph{SPARQL Based} are in contrast to \emph{RDFS/OWL Based} constraints usually not (yet) explicitly defined within formal specifications of vocabularies. Instead, they are often defined within textual descriptions of vocabularies. Additionally, we let our domain and data experts define constraints when they agreed that violating the constraint would affect the usefulness of the data.

\textbf{\emph{Constraint Language Based}.}
We further distinguish \emph{Constraint Language Based} as the set of constraint types that can be expressed by common classical declarative high-level constraint languages like ShEx, ReSh, and DSP. 
%RDFS, and OWL as we consider RDFS and OWL as constraint languages when assuming CWA/UNA semantics without reasoning. 
There is a strong overlap between \emph{RDFS/OWL} and \emph{Constraint Language Based} constraint types as in many cases constraint types are expressible by both classical constraint languages and RDFS/OWL. SPARQL, however, is considered as a low-level implementation language in this context. In contrast to SPARQL, high-level constraint languages are comparatively easy to understand and constraints can be formulated more concisely. Declarative languages may be placed on top of SPARQL when using it as an implementation language. For these \emph{Constraint Language Based} constraint types, we expect a straight-forward support in future constraint languages.

\emph{Context-specific exclusive or of property groups} (\emph{R-11}) is a constraint type which can be formulated by a high-level constraint language. Constraints of this type restrict individuals of given classes to have properties defined within exactly one of multiple mutually exclusive property groups. Within the context of DDI-RDF, \emph{skos:Concept}s can have either \emph{skos:definition} (when interpreted as theo\-reti\-cal concepts) or \emph{skos:notation} and \emph{skos:prefLabel} properties (when interpreted as codes), but not both:
%(\emph{error}).

\begin{ex}[commandchars=\\\{\}]
\textit{ShEx:} Concept \{ 
    ( definition string ) |
    ( notation string , prefLabel string ) \}
\end{ex}

%We further distinguished \emph{simple constraint types} as the set of constraint types whose constraints can be easily defined without much effort in addition to the already defined \emph{vocabulary constraints}.
%\emph{Data property facets} (\emph{R-46}) is an example of a \emph{simple constraint type} which enables to declare frequently needed facets for data properties in order to validate input against simple conditions including min/max values, regular expressions, and string length.
%The abstract of series/studies, e.g., should have a minimum length which is easily and concisely expressible by SPARQL: 
%\begin{ex}
%SELECT ?study WHERE {
    %?study dcterms:abstract ?abstract . 
    %FILTER ( STRLEN ( ?abstract ) = 10 ) . }
%\end{ex}

%\emph{Complex constraint types} encompass constraint types for which the definition of constraints is rather complex and cannot be derived from vocabulary definitions.
\textbf{\emph{SPARQL Based}.}
The set \emph{SPARQL Based} encompasses constraint types that are not expressible by RDFS/OWL or common high-level constraint languages but by plain SPARQL. For assessing the quality of thesauri, e.g., we concentrate on the graph-based structure and apply graph- and network-analysis techniques.
An example of such constraints of the constraint type \emph{structure} is that 
a thesaurus should not contain many orphan concepts, i.e., concepts without any associative or hierarchical relations, lacking context information valuable for search. As the complexity of this constraint is relatively high, it is only expressible by SPARQL and not directly understandable:
\begin{ex}
SELECT ?concept WHERE {
    ?concept a [rdfs:subClassOf* skos:Concept] .
    FILTER NOT EXISTS { ?concept ?p ?o . 
        FILTER ( ?p IN ( skos:related, skos:relatedMatch, 
                         skos:broader, ... ) ) . } }
\end{ex}
\emph{SPARQL Based} constraint types are today only expressible by plain SPARQL. Depending on their usefulness, a support in constraint languages should be considered.

\subsection{Classification of Constraints according to the Severity of Constraint Violations}

A concrete constraint is instantiated from one of the 81 constraint types and is defined for a specific vocabulary.
It does not make sense to determine the severity of constraint violations of an entire constraint type,
as the severity depends on the individual context and vocabulary.
%as the violation of one constraint may be severe,
%whereas, the violation of another constraint of the same type may be minor.
SBE experts determined the default \emph{severity level}\footnote{The possibility to define severity levels in vocabularies is in itself a requirement (\emph{R-158}).} for each of the 115 constraints to indicate how serious the violation of the constraint is. We use the classification system of log messages in software development like \emph{Apache Log4j 2} \cite{Apache-2015}, the \emph{Java Logging API},\footnote{\url{http://docs.oracle.com/javase/7/docs/api/java/util/logging/Level.html}} and the \emph{Apache Commons Logging API}\footnote{\url{http://commons.apache.org/proper/commons-logging/}} as many data practitioners also have experience in software development and software developers intuitively understand these levels. We simplify this commonly accepted classification system and distinguish the three severity levels (1) \emph{informational}, (2) \emph{warning}, and (3) \emph{error}.
%According to the default severity level of constraints, the complete set of constraints encompasses three disjoint \emph{sets of constraints}:\ke{auch dieser satz macht keinen sinn für mich, ebensowenig wie die extrem erhellende auflistung, die gleich folgt... mein vorschlag von eben kann das komplett ersetzen.}
%\begin{itemize}
	%\item \textbf{\emph{informational constraints}}: constraints with severity level \emph{informational}
	%\item \textbf{\emph{warning constraints}}: constraints with severity level \emph{warning}
	%\item \textbf{\emph{error constraints}}: constraints with severity level \emph{error}
%\end{itemize}
Violations of \emph{informational} constraints point to desirable but not necessary data improvements to achieve RDF representations which are ideal in terms of syntax and semantics of used vocabularies. 
%They are not seen as serious errors.
\emph{Warnings} are syntactic or semantic problems which typically should not lead to an abortion of data processing.
\emph{Errors}, in contrast, are syntactic or semantic errors which should cause the abortion of data processing. 
%Data not conforming to \emph{warning} and \emph{error constraints} is syntactically and/or semantically not correctly represented.
%Data not conforming to \emph{warning constraints} but to \emph{error constraints} could be processed further.
%Data not corresponding to \emph{error constraints}, however, cannot be processed further after validation. 

Note that there is a correlation between the severity of a constraint and the classification according to the expressivity of constraint languages of its type: \emph{RDFS/OWL Based} constraints are in many cases classified with an \emph{error} level as they typically represent basic constraints; there is a reason why they have been included in the vocabulary specification. Although we provide default severity levels for each constraint, validation environments should enable users to adapt the severity levels of constraints according to their individual needs.
% Validation environments should enable users to select which constraints to validate against depending on their individual use cases.
% For some use cases, 
% validating \emph{vocabulary constraints} may be more important than validating \emph{simple} or \emph{complex constraints}.
% For other use cases,
% validating \emph{error constraints} may be sufficient without taking \emph{warning} and \emph{informational constraints} into account.
% We evaluated the (meta)data quality of large real world data sets represented by multiple and different vocabularies to get an understanding  
% (1) which sets of constraint types (on different levels of complexity) and 
% (2) which sets of constraints (associated with particular severity levels) encompass the constraints causing the most/fewest constraint violations (see section \ref{evaluation}).

%
%- different sets of constraints for different use cases / use case specific constraints
%
%- validation against minimal set of requirements / constraints
%
%-----

%further ideas:
%
%RDF validation scenarios require the closed-world assumption (CWA) (i.e., a statement is inferred to be false if it cannot be proved to be true).

%use-case specific constraints, e.g., DCAT: searching for metadata
%\tb{Thomas: ToDo}

\subsection{Examples}
%\ke{die examples finde ich sehr schwer zu lesen, es fehlt auch eine einleitung aus der hervorgeht, wieso die beispiele hier eigentlich aufgeführt werden.}
\label{vocabulary-constraint-types}

To get an overview on the sets of constraint types, we delineate concrete constraints on the three vocabularies for each set of constraint type and classify them according to their severity.

%The constraint type \emph{vocabulary} guarantees that users do not invent new or use deprecated terms of vocabularies.
%Out-dated classes and properties of previous vocabulary versions can be marked as deprecated.
%The constraint types \emph{context-specific valid classes and properties} (\emph{R-209; R-210}) can be used to specify which classes and properties are valid in which context - here a given vocabulary version.
%\emph{Value is valid for datatype} (\emph{R-223}) constraints serve to make sure that all literal values are valid with regard to their datatypes - as stated in the vocabularies.
%Thus, it is checked that all date values (e.g., {{\em dcterms:date}) are actually of the datatype \emph{xsd:date} and that \emph{xsd:nonNegativeInteger} values (e.g. \emph{DDI-RDF:frequency}) are not negative\footnote{For simplicity reasons, we only assign severity levels to \emph{vocabulary constraints} if they differ from \emph{error}.}.
%Depending on property datatypes, two different literal values have
%a specific ordering with respect to operators like \textless  (\emph{R-43: literal value comparison}).
%Start dates (\emph{DDI-RDF:startDate}), e.g., must be before (\emph{\textless}) end dates (\emph{DDI-RDF:endDate}).

%All properties, not having the same domain and range types, are defined to be pairwise disjoint
%(\emph{R-9: disjoint properties}), i.e., no individual \emph{x} can be connected to an individual/literal \emph{y} by disjoint properties (e.g., \emph{phdd:isStructuredBy} and \emph{phdd:column}).
%All \emph{PHDD} classes (e.g., \emph{phdd:TableDescription}, \emph{phdd:ColumnDescription}) are pairwise disjoint (\emph{R-7: disjoint classes}),
%i.e., individuals cannot be instances of multiple disjoint classes.
\textbf{\emph{RDFS/OWL Based}.}
It is a common requirement to narrow down the value space of properties by an exhaustive enumeration of valid values (\emph{R-30/37: allowed values}):
%\emph{Allowed values} (\emph{R-30, R-37}) for properties can be IRIs (matching one or multiple patterns), any literals, allowed literals (e.g. 'red' 'blue' 'green'), and typed literals of one or multiple type(s) (e.g. \emph{xsd:string}). 
\emph{disco:CategoryStatistics}, e.g., can only have \emph{disco:computationBase} relationships to the values \emph{valid} and \emph{invalid} of the datatype \emph{rdf:langString} (default severity level: \emph{error}).
%Validation should \emph{exploit sub-super relations} in vocabularies (\emph{R-224}).
%If \emph{dcterms:coverage} and one of its sub properties (\emph{dcterms:spatial}, \emph{dcterms:temporal}) are given,
%it is checked that \emph{dcterms:coverage} is not redundant with its sub-properties 
%which may indicate when the data is verbose/redundant or expressed at a too general level.
Consider the following \emph{DL knowledge base} $\mathcal{K}$:\footnote{A \emph{DL knowledge base} is a collection of formal statements which correspond to \emph{facts} or what is known explicitly. }
\begin{center}
\begin{DL} 
$\mathcal{K}=\{$ 
	%&\ms{isStructuredBy $\sqsubseteq$ $\neg$ column}, \\
	%&\ms{TableDescription $\sqcap$ ColumnDescription $\sqsubseteq$ $\perp$}, \\
&{\small\ms{CategoryStatistics $\equiv$ $\forall$ computationBase.}} \\
	&{\small\ms{\{valid,invalid\} $\sqcap$ langString,}} \\
  &{\small\ms{Variable $\equiv$ $\exists$ concept.Concept,}} \\
	&{\small\ms{DataSet $\sqsubseteq$ $\forall$ structure.DataStructureDef,}} \\
  &{\small\ms{$\exists$ hasTopConcept.$\top$ $\sqsubseteq$ ConceptScheme,}} \\
	&{\small\ms{$\top$ $\sqsubseteq$ $\forall$ variable.Variable}}
\}\\ 
\end{DL}
\end{center}

%We extend $\mathcal{K}$ by 
\emph{Existential quantifications} (\emph{R-86}) enforce that instances of given classes must have some property relation to individuals/literals of certain types.
Variables, e.g., should have a relation to a theoretical concept (\emph{informational}).
%The variables \emph{number of school years} and \emph{highest educational degree}, e.g., are associated with the theoretical concept \emph{education}. 
The variable \emph{Education at pre-school} is associated with the theoretical concept \emph{Child Care}. 
The default severity level of this constraint is weak, as in most cases research can be continued without having information about the theoretical concept of a variable.
%
A \emph{universal quantification} (\emph{R-91}) contains all those individuals that are connected by a property only to individuals/literals of particular classes  or data ranges.
%Only \emph{dcat:Catalogs}, e.g., can have \emph{dcat:dataset} relationships to \emph{dcat:Datasets}:
%\begin{DL}
%Catalog $\sqsubseteq$ $\forall$ dataset.Dataset
%\end{DL}
%Only \emph{\textcolor{red}{dcat}:Catalogs}, e.g., can have \emph{dcat:dataset} relationships to \emph{dcat:Datasets} (\emph{error}).
Only \emph{qb:DataSets}, e.g., can have \emph{qb:structure} relationships to \emph{qb:DataStructureDefinitions} (\emph{error}).
\emph{Property domains} (\emph{R-25, R-26}) and \emph{property ranges} (\emph{R-28, R-35}) constraints restrict domains and ranges of properties:
%Only \emph{phdd:Tables}, e.g., can have \emph{\textcolor{red}{phdd}:isStructuredBy} relationships (\emph{error}) and \emph{\textcolor{red}{xkos}:belongsTo} relations can only point to \emph{skos:Concepts} (\emph{error}).
only \emph{skos:ConceptSchemes}, e.g., can have \emph{skos:hasTopConcept} relationships (\emph{error}) and \emph{disco:variable} relations can only point to \emph{disco:Variables} (\emph{error}).

It is often useful to declare a given (data) property as the \emph{primary key} (\emph{R-226}) of a class, so that a system can enforce uniqueness and build URIs from user inputs and imported data. 
In DDI-RDF, resources are uniquely identified by the property \emph{adms:identifier},
which is therefore inverse-functional $({\small\ms{funct identifier}\sp{\overline{\ }}})$,
i.e., for each \emph{rdfs:Resource x}, there can be at most one distinct resource \emph{y} such that \emph{y} is connected by \emph{adms:identifier$\sp{\overline{\ }}$} to \emph{x} (\emph{error}).
%\begin{DL}
%$(\ms{funct identifier}\sp{\overline{\ }})$
%\end{DL} 
Keys, however, are even more general than \emph{inverse-functional properties} (\emph{R-58}),
as a key can be a data property, an object property, or a chain of properties \cite{Schneider2009}.
For this reason, as there are different sorts of key, and as keys can lead to undecidability, 
DL is extended with the construct \emph{keyfor} ({\small\ms{identifier \ms{keyfor} Resource}}) \cite{Lutz2005} which is implemented by the OWL 2 \emph{hasKey} construct.

%\emph{Existential quantifications} (\emph{R-86}) enforce that instances of given classes must have some property relation to individuals/literals of certain types.
%Variables, e.g., should have a relation to a theoretical concept ($\mathcal{SL}_{0}$).
%%The variables \emph{number of school years} and \emph{highest educational degree}, e.g., are associated with the theoretical concept \emph{education}. 
%The variable \emph{Education at pre-school} is associated with the theoretical concept \emph{Child Care}. 
%The default severity level of the constraint is weak, as in most cases research can be continued without having information about the theoretical concept of a variable.
%If a study, e.g., does not contain any data set ($\mathcal{SL}_{2}$), the actual description of the data is missing which may indicate that it is very hard or unlikely to get access to the data.
%If metadata on data files, including the actual data, is missing (especially case and variable quantities; $\mathcal{SL}_{1}$), the description of the data sets and the study is not sufficient.
%Case quantity measures how many cases are collected for a study.
%High case and variable quantities are indicators for high statistical quality and comprehensiveness of the underlying study ($\mathcal{SL}_{1}$).

%\textbf{Cardinality Restrictions on Properties.}
%An \emph{existential quantification} (\emph{R-86}) contains all those individuals that are connected by a property to individuals/literals of given classes or data ranges.
%Every \emph{qb:SliceKey}, e.g., must be associated with (\emph{qb:sliceKey}) a \emph{qb:DataStructureDefinition} (\ms{SliceKey $\sqsubseteq$ $\exists$ sliceKey$^{-}$.DataStructureDefinition}).
%\emph{Minimum/maximum/exact qualified cardinality restrictions} (\emph{R-74, R-75, R-76}) contain all those individuals that are connected by a property to at least/at most/exactly n different individuals/literals of particular classes or data ranges.
%A \emph{phdd:TableStructure} has (\emph{phdd:column}) at least one \emph{phdd:Column},
%a \emph{DDI-RDF:Variable} has at most one \emph{DDI-RDF:concept} relationship to a theoretical concept (\emph{skos:Concept}), and a \emph{qb:DataSet} is structured by (\emph{qb:structure}) exactly one \emph{qb:DataStructureDefinition}.
%\begin{DL}
%TableStructure $\sqsubseteq$ $\geq$1 column.Column \\
%Variable $\sqsubseteq$ $\leq$1 concept.Concept \\
%DataSet $\sqsubseteq$ $\geq$1 structure.DSD $\sqcap$
%$\leq$1 structure.DSD
%\end{DL}

%%\textbf{Validation and Reasoning.}
%Some constraint types enable performing reasoning prior to validation which may resolve or cause constraint violations.
%With \emph{subsumption} (\emph{R-100}), one can state that \emph{xkos:ClassificationLevel} is a sub-class of \emph{skos:Collection}, i.e., each \emph{xkos:ClassificationLevel} must also be part of the \emph{skos:Collection} class extension.
%With \emph{sub properties} (\emph{R-54, R-64}), one can state that \emph{DDI-RDF:fundedBy} is a sub-property of \emph{dcterms:contributor} - i.e., if a study is funded by an organization, then this organization contributed to this study.
%\begin{DL}
%ClassificationLevel $\sqsubseteq$ Collection \\
%fundedBy $\sqsubseteq$ contributor
%\end{DL} 

%\section{Basic Constraint Types}
%\label{vocabulary-constraint-types}

%As RDFS and OWL are typically used to define vocabularies, RDFS and OWL reasoning may be performed prior to validation. 
%Reasoning and validation are indeed very closely related. 
%\emph{Reasoning} is the process of determining what follows from what has been stated.
%%Both should be possible: (1) validation with reasoning and (2) validation without reasoning. 
%We divide the whole set of \emph{basic constraint types} ($\mathcal{CT}_{B}$) into two disjoint sets to investigate the affect of reasoning to the validation process: 
%\begin{enumerate}
	%\item $\mathcal{C}_B ^{\mathcal{R}}$ corresponds to axioms in \emph{OWL 2} and denotes the set of constraint types which enable performing reasoning prior to validation, especially when not all the knowledge is explicit (section \ref{basic-constraint-types-with-reasoning}).  
  %\item $\overline{\mathcal{C}_B ^{\mathcal{R}}}$  denotes the set of constraint types for which reasoning cannot be done or does not improve the result in any obvious sense (section \ref{basic-constraint-types-without-reasoning}).
%\end{enumerate}

%\subsection{Basic Constraint Types with Reasoning}
%\label{basic-constraint-types-with-reasoning}

%%Validation environments should enable users to decide if they wish to perform reasoning prior to validation.
%%Reasoning as an optional pre validation step is beneficial for RDF validation as 
%%(1) it may resolve constraint violations and  
%%(2) it may cause useful constraint violations.
%A \emph{universal quantification} (\emph{R-91}) contains all those individuals that are connected by a property only to individuals/literals of particular classes  or data ranges.
%%Only \emph{dcat:Catalogs}, e.g., can have \emph{dcat:dataset} relationships to \emph{dcat:Datasets}:
%%\begin{DL}
%%Catalog $\sqsubseteq$ $\forall$ dataset.Dataset
%%\end{DL}
%Consider the following DL knowledge base $\mathcal{K}$\footnote{A knowledge base is a collection of formal statements which corresponds to \emph{facts} or what is known explicitly. For simplicity reasons, we only write namespace prefixes in DL statements to avoid ambiguities.}:
%
%\begin{center}
%\begin{DL} 
%$\mathcal{K}=\{$ &\ms{LogicalDataSet $\sqsubseteq$ dcat:DataSet},\\
 %&\ms{LogicalDataSet $\sqsubseteq \forall$ aggregation.qb:DataSet},\\
 %%&\ms{dcat:DataSet $\equiv$ $\exists$ dcat:distribution . dcat:Distribution},\\
 %&\ms{LogicalDataSet( logical-data-set )},\\
 %&\ms{aggregation( logical-data-set, aggregated-data-set )}
 %\}\\ 
%\end{DL}
%\end{center}
%
%As we know that only unit-record data sets (\emph{DDI-RDF:LogicalDataSet}) can derive (\emph{DDI-RDF:aggregation}) aggregated data sets (\emph{qb:DataSet}), 
%{\em logical-data-set} is a \emph{DDI-RDF:LogicalDataSet}, 
%and \emph{aggregated-data-set} is derived from \emph{logical-data-set},
%we conclude that \emph{aggregated-data-set} must be a \emph{qb:DataSet}.
%As \emph{aggregated-data-set} is not explicitly defined to be a \emph{qb:DataSet}, however, a constraint violation is raised.
%If we perform reasoning prior to validation, the constraint violation is resolved, as the implicit triple \ms{qb:DataSet(aggregated-data-set)} is inferred. 
%
%Reasoning may also cause constraint violations which are needed to enhance data quality.
%With \emph{subsumption} (\emph{R-100}), one can state that \emph{DDI-RDF:LogicalDataSet} is a sub-class of \emph{dcat:DataSet}, 
%i.e., each unit-record data set is also a catalog data set.
%Thus, constraints on catalog data sets are also validated for unit-record data sets;
%e.g., the \emph{existential quantification} below restricting that unit-record data sets must have a distribution: 

%\begin{center}
%\begin{DL} 
%$\mathcal{K}=\{$ 
 %&\ms{DataSet $\equiv$ $\exists$ distribution.Distribution},\\
%&\ms{Variable $\equiv$ $\exists$ concept.Concept}
 %\}\\ 
%\end{DL}
%\end{center}

%We extend $\mathcal{K}$ by \emph{existential quantifications} (\emph{R-86}) enforcing that instances of given classes must have some property relation to individuals/literals of certain types.
%Variables, e.g., should have a relation to a theoretical concept ($\mathcal{SL}_{0}$).
%%The variables \emph{number of school years} and \emph{highest educational degree}, e.g., are associated with the theoretical concept \emph{education}. 
%The variable \emph{Education at pre-school} is associated with the theoretical concept \emph{Child Care}. 
%The default severity level of the constraint is weak, as in most cases research can be continued without having information about the theoretical concept of a variable.
%If a study, e.g., does not contain any data set ($\mathcal{SL}_{2}$), the actual description of the data is missing which may indicate that it is very hard or unlikely to get access to the data.
%If metadata on data files, including the actual data, is missing (especially case and variable quantities; $\mathcal{SL}_{1}$), the description of the data sets and the study is not sufficient.
%Case quantity measures how many cases are collected for a study.
%High case and variable quantities are indicators for high statistical quality and comprehensiveness of the underlying study ($\mathcal{SL}_{1}$).

%\begin{center}
%\begin{DL} 
%$\mathcal{K}=\{$ 
  %&\ms{fundedBy $\sqsubseteq$ contributor}
 %\}\\ 
%\end{DL}
%\end{center}
%
%By stating that \emph{DDI-RDF:fundedBy} is a sub property of \emph{dcterms:contributor},
%the {\em sub property} (\emph{R-54, R-64}) above assures that if a series is funded by an organization, then the organization must also contribute to the series.
%In case the \emph{sub-property} is applied without reasoning and $\mathcal{K}$ contains the triple \ms{DDI-RDF:fundedBy} \ms{(EU-SILC,} \ms{organization)},
%a constraint violation is thrown if $\mathcal{K}$ does not explicitly include the triple \ms{dcterms:contributor} \ms{(EU-SILC,} \ms{organization)}.
%If the \emph{sub property} is applied with reasoning, on the other side, the latter triple is derived which resolves the constraint violation.

%\begin{DL}
%fundedBy $\sqsubseteq$ contributor
%\end{DL} 

%\emph{Asymmetric object properties} (\emph{R-62}) restrict that if individual \emph{x} is connected by the object property \emph{OP} to individual \emph{y}, then \emph{y} cannot be connected by \emph{OP} to \emph{x}. 
%Such constraints are defined for each object property for which a semantically equivalent object property pointing from the other direction would also be possible but is not defined within the vocabulary.
%A \emph{DDI-RDF:Variable}, e.g., may be based on (\emph{DDI-RDF:basedOn}) a \emph{DDI-RDF:RepresentedVariable}.
%A \emph{DDI-RDF:RepresentedVariable}, however, cannot be based on a \emph{DDI-RDF:Variable} (\ms{$DDI-RDF:basedOn \sqcap DDI-RDF:basedOn^{-} \sqsubseteq \bot$}).

%\emph{Property domain} (\emph{R-25, R-26}) and \emph{range} (\emph{R-28, R-35}) constraints restrict domains and ranges of properties.
%Only \emph{phdd:Tables}, e.g., can have \emph{phdd:isStructuredBy} relationships and
%\emph{xkos:belongsTo} relationships can only point to \emph{skos:Concepts}:
%\begin{DL}
%$\exists$ isStructuredBy.$\top$ $\sqsubseteq$ Table \\
%$\top$ $\sqsubseteq$ $\forall$ belongsTo.Concept
%\end{DL}

%\begin{center}
%\begin{DL} 
%$\mathcal{K}=\{$ 
  %&\ms{TableStructure $\sqsubseteq$ $\geq$1 column.Column}, \\
	%&\ms{Variable $\sqsubseteq$ $\leq$1 concept.Concept}, \\
	%&\ms{DataSet $\sqsubseteq$ $\geq$1 structure.DataStructureDefinition} \\ 
	%&\ms{$\sqcap$ $\leq$1 structure.DataStructureDefinition}
 %\}\\ 
%\end{DL}
%\end{center}

%\textbf{Cardinality Restrictions on Properties.}
%An \emph{existential quantification} (\emph{R-86}) contains all those individuals that are connected by a property to individuals/literals of given classes or data ranges.
%Every \emph{qb:SliceKey}, e.g., must be associated with (\emph{qb:sliceKey}) a \emph{qb:DataStructureDefinition} (\ms{SliceKey $\sqsubseteq$ $\exists$ sliceKey$^{-}$.DataStructureDefinition}).
%\emph{Minimum/maximum/exact qualified cardinality restrictions} (\emph{R-74, R-75, R-76}) contain all those individuals that are connected by a property to at least/at most/exactly n different individuals/literals of particular classes or data ranges.
%A \emph{phdd:TableStructure} has (\emph{phdd:column}) at least one \emph{phdd:Column},
%a \emph{DDI-RDF:Variable} has at most one \emph{DDI-RDF:concept} relationship to a theoretical concept (\emph{skos:Concept}), and a \emph{qb:DataSet} is structured by (\emph{qb:structure}) exactly one \emph{qb:DataStructureDefinition}.
%DATA-CUBE-C-MINIMUM-QUALIFIED-CARDINALITY-RESTRICTIONS-
%02: Unique data set (IC-1 [3]) - Every qb:Observation has (qb:dataSet) ex-
%actly one associated qb:DataSet (Observation  ¥1 dataSet.DataSet [
%¤1 dataSet.DataSet).
%Severity level: ERROR
%
%(\emph{R-75: minimum qualified cardinality restrictions})
%
%DATA-CUBE-C-EXACT-QUALIFIED-CARDINALITY-RESTRICTIONS-
%02: Unique DSD (IC-2 [3]) - Every qb:DataSet has (qb:structure) exactly one
%associated qb:DataStructureDefinition (DataSet  ¥1 structure.DataStructureDefinition
%[ ¤1 structure.DataStructureDefinition).
%Severity level: ERROR
%
%(\emph{R-74: exact qualified cardinality restrictions})
%\footnote{\emph{DATA-CUBE-C-EXACT-QUALIFIED-CARDINALITY-RESTRICTIONS-02}}.
%\textbf{Language Tag Cardinality.}

%\subsection{Basic Constraint Types without Reasoning}
%\label{basic-constraint-types-without-reasoning}
%
%$\overline{\mathcal{C}_B ^{\mathcal{R}}}$  denotes the set of constraint types for which reasoning cannot be done or does not improve the result in any obvious sense.
%The constraint type \emph{vocabulary} guarantees that users do not invent new or use deprecated terms of vocabularies.
%%Out-dated classes and properties of previous vocabulary versions can be marked as deprecated.
%%The constraint types \emph{context-specific valid classes and properties} (\emph{R-209; R-210}) can be used to specify which classes and properties are valid in which context - here a given vocabulary version.
%\emph{Value is valid for datatype} (\emph{R-223}) constraints serve to make sure that all literal values are valid with regard to their datatypes - as stated in the vocabularies.
%Thus, it is checked that all date values (e.g., {{\em dcterms:date}, \em DDI-RDF:startDate}, {\em DDI-RDF:endDate}) are actually of the datatype \emph{xsd:date} and that \emph{xsd:nonNegativeInteger} values (e.g. \emph{DDI-RDF:frequency}) are not negative.
%Depending on property datatypes, two different literal values have
%a specific ordering with respect to operators like \textless  (\emph{R-43: literal value comparison}).
%Start dates (\emph{DDI-RDF:startDate}), e.g., must be before (\emph{\textless}) end dates (\emph{DDI-RDF:endDate}).
%
%\begin{center}
%\begin{DL} 
%$\mathcal{K}=\{$ 
	%&\ms{isStructuredBy $\sqsubseteq$ $\neg$ column}, \\
	%&\ms{TableDescription $\sqcap$ ColumnDescription $\sqsubseteq$ $\perp$}, \\
	%&\ms{CategoryStatistics $\equiv$} \\
	%&\ms{$\forall$ computationBase.\{valid,invalid\} $\sqcap$ langString}
 %\}\\ 
%\end{DL}
%\end{center}
%
%All properties, not having the same domain and range types, are defined to be pairwise disjoint
%(\emph{R-9: disjoint properties}), i.e., no individual \emph{x} can be connected to an individual/literal \emph{y} by disjoint properties (e.g., \emph{phdd:isStructuredBy} and \emph{phdd:column}).
%All \emph{PHDD} classes (e.g., \emph{phdd:TableDescription}, \emph{phdd:ColumnDescription}) are pairwise disjoint (\emph{R-7: disjoint classes}),
%i.e., individuals cannot be instances of multiple disjoint classes.
%It is a common requirement to narrow down the value space of properties by an exhaustive enumeration of valid values (\emph{R-30/37: allowed values}). 
%%\emph{Allowed values} (\emph{R-30, R-37}) for properties can be IRIs (matching one or multiple patterns), any literals, allowed literals (e.g. 'red' 'blue' 'green'), and typed literals of one or multiple type(s) (e.g. \emph{xsd:string}). 
%\emph{DDI-RDF:CategoryStatistics}, e.g., can only have \emph{DDI-RDF:computationBase} relationships to the values \emph{valid} and \emph{invalid} of the datatype \emph{rdf:langString}.
%Validation should \emph{exploit sub-super relations} in vocabularies (\emph{R-224}).
%If \emph{dcterms:coverage} and one of its sub-properties (\emph{dcterms:spatial}, \emph{dcterms:temporal}) are given,
%it is checked that \emph{dcterms:coverage} is not redundant with its sub-properties 
%which may indicate when the data is verbose/redundant or expressed at a too general level.

\textbf{\emph{Constraint Language Based}.}
%{Simple constraint types} is the set of constraint types whose constraints can be easily defined without much effort in addition to $\mathcal{CT}_{B}$ constraints.
%\textbf{Validation and Reasoning.}
Depending on property datatypes, two different literal values have
a specific ordering with respect to operators like \textless  (\emph{R-43: literal value comparison}).
Start dates (\emph{disco:startDate}), e.g., must be before (\emph{\textless}) end dates (\emph{disco:endDate}).

%Percentage values are only valid when they are within the literal range of 0 and 100 (\emph{R-45: literal ranges}; \emph{error})
%which is checked for \emph{DDI-RDF:percentage} standing for the number of cases of a given code in relation to the total number of cases for a particular variable.

In many cases, resources must be \emph{members of controlled vocabularies} (\emph{R-32}).
If a QB dimension property, e.g., has a \emph{qb:codeList},
then the value of the dimension property on every \emph{qb:Observation} must be in the code list (\emph{error}).

\emph{Default values} for objects (\emph{R-31}) or literals (\emph{R-38}) of given properties are inferred automatically when properties are not present in the data.
The value \emph{true} for the property {\em disco:isPublic} indicates that a {\em disco:LogicalDataSet} can be accessed by anyone.
Per default, however, access to data sets should be restricted (\emph{false}) (\emph{informational}).

%Many properties are not necessarily required but \emph{recommended} within a particular context (\emph{R-72}).
%The property {\em skos:notation}, e.g., is not mandatory for {\em disco:Variable}s, but recommended to represent variable names (\emph{informational}).

%\begin{center}
%\begin{DL} 
%$\mathcal{K}=\{$ 
	%&\ms{(funct identifier$\sp{\overline{\ }})$, identifier keyfor Resource}
 %\}\\ 
%\end{DL}
%\end{center}

%\emph{OWL 2} \emph{hasKey} implements \emph{keyfor} ($\mathcal{SL}_{2}$) and thus can be used to identify resources uniquely, to merge resources with identical key property values, and to recognize constraint violations.
%\begin{DL}
%identifier \ms{keyfor} Resource
%\end{DL} 

\textbf{\emph{SPARQL Based}.}
%\textcolor{red}{
%In this sub-chapter, we assign default severity levels to and describe constraints of diverse $\mathcal{CT}_{C}$ constraint types 
%to ensure that the data is consistent with the intended syntax, semantics, and integrity of vocabularies' data models.
%}
%\textbf{Observations of Aggregated Data Sets.}
%- for each dimension there should be a description and code lists.
%- for each code list there should be a description.
%- there should be a relationship to the underlying unit-record data.
%
%$\mathcal{CT}_{C}$ denotes the set of constraint types for which the definition of constraints is rather complex and cannot be derived from vocabulary definitions.
The purpose of \emph{data model consistency} constraints is to ensure the integrity of
the data according to the intended semantics of vocabularies.
Every \emph{qb:Observation}, e.g., must have a value for each dimension
declared in its \emph{qb:DataStructureDefinition} (\emph{error})
and no two \emph{qb:Observations} in the same \emph{qb:DataSet}
can have the same value for all dimensions (\emph{warning}).
If a \emph{qb:DataSet} \emph{D} has a \emph{qb:Slice} \emph{S}, and \emph{S} has an
\emph{qb:Observation} \emph{O}, then the \emph{qb:DataSet} corresponding to \emph{O} must be \emph{D} (\emph{warning}).
%Relative frequencies of variable codes are calculated correctly, if the cumulative percentage (\emph{DDI-RDF:cumulativePercentage}) of a given code exactly matches the cumulative percentage of the previous code
%plus the percentage value (\emph{DDI-RDF:percentage}) of the current code ($\mathcal{SL}_{2}$).
%{\em Mathematical Operations} (\emph{R-41, R-42}; e.g. date calculations and statistical computations like average, mean, and sum) are performed to ensure the integrity of data models.
%The sum of percentage values of all variable codes, e.g., must exactly be 100 ($\mathcal{SL}_{2}$)
%and the minimum absolute frequency of all variable codes do not have to be greater than the maximum ($\mathcal{SL}_{2}$).
%\textbf{Hierarchies and Ordering.}
%\textbf{Structure and Ordering.}
%For assessing the quality of SKOS vocabularies, we concentrate on graph-based structures and apply graph- and network-analysis techniques (\emph{structure} constraints) like 
%(1) a thesaurus should provide entry points (top concepts) to the data to provide efficient access and guidance for human users,
%(2) concepts, internal to the tree, should not be indicated as top concepts, and
%(3) a thesaurus should not contain many orphan concepts 
%(concepts without any associative or hierarchical relations) lacking valuable context information for retrieval.
%, as, e.g., no hierarchical query expansion can be performed on search terms to find documents with more general content.) \cite{MaderHaslhoferIsaac2012}. 
%A very common research question is to compare variables of multiple studies or countries (constraint type: \emph{comparison}).
%To compare variables 
%(1) their code lists must be structured properly and
%(2) their code lists must either be identical or at least similar.
%If a researcher only wants to get a first overview over comparable variables (use case 1), 
%covering the first constraint may be sufficient.
%Thus, the severity level of the first constraint is stronger ($\mathcal{SL}_{2}$) than the one for the second constraint ($\mathcal{SL}_{0}$).
%If the intention of the researcher is to perform more detailed comparisons (use case 2), however, the violation of the second constraint is getting more serious
%and the user may raise its severity level.
%\textbf{Comparison.}
%A very common research question is to compare variables of multiple studies or countries (constraint type: \emph{comparison}).
%To compare variables, 
%(1) variables and (2) variable definitions must be present,
%(3) code lists must be structured properly,
%(4) for each code an associated category must be specified, and
%(5) code lists must either be identical or at least similar.
%If a researcher wants to get a first overview over comparable variables (use case 1), 
%covering the first three constraints may be sufficient.
%Thus, the severity level of the first three constraints is stronger ($\mathcal{SL}_{2}$) than for the last two constraints ($\mathcal{SL}_{1}$ and $\mathcal{SL}_{0}$).
%If the intention of the researcher is to perform more detailed comparisons (use case 2), however, the violation of the remaining two constraints is getting more serious.
%\textbf{Membership in Controlled Vocabularies.}

%Summary statistics types like minimum, maximum, and arithmetic mean are maintained within a controlled vocabulary.  
%Thus, summary statistics can only have \emph{DDI-RDF:summaryStatisticType} relationships to \emph{skos:Concept}s which must be members of the controlled vocabulary \emph{ddicv:SummaryStatisticType}, a \emph{skos:ConceptScheme} ($\mathcal{SL}_{2}$).
Objects/literals can be declared to be ordered for given properties (\emph{R-121/217: ordering}).
Variables, questions, and codes, e.g., are typically organized in a particular order. 
If codes (\emph{skos:Concept}) should be ordered, they must be members (\emph{skos:memberList}) in an ordered collection (\emph{skos:OrderedCollection}), the variable's code list (\emph{informational}).

%\textbf{Constraints on Properties.}
It is useful to declare properties to be \emph{conditional} (\emph{R-71}), i.e., if particular properties exist (or do not exist), then other properties must also be present (or absent).
To get an overview on a study either an abstract, a title, an alternative title, or links to external descriptions should be provided. 
If an abstract and an external description are absent, however,  
a title or an alternative title should be given (\emph{warning}).
In case a variable is represented in form of a code list, codes may be associated with categories, i.e., human-readable labels (\emph{informational}).
The variable \emph{Education at pre-school}, e.g., is represented as an ordered code list without any categories.

For data properties, it may be desirable to restrict that values of predefined languages must be present for determined number of times (\emph{R-48/49: language tag cardinality}):
(1) It is checked if literal language tags are set. Some controlled vocabularies, e.g., contain literals in natural language, but without information what language has actually been used (\emph{warning}). 
(2) Language tags must conform to language standards (\emph{error}). 
(3) Some thesaurus concepts are labeled in only one, others in multiple languages. 
It may be desirable to have each concept labeled in each of the languages that are also used on the other concepts,
as language coverage incompleteness for some concepts may indicate shortcomings of thesauri (\emph{informational})
\cite{MaderHaslhoferIsaac2012}.

%If a {\em skos:Concept} represents a code (having {\em skos:notation} and {\em skos:prefLabel} properties), 
%then the property {\em DDI-RDF:isValid} has to be stated indicating if the code stands for valid (\emph{true}) or missing (\emph{false}) cases ($\mathcal{SL}_{2}$).

%\textbf{Constraints on Literals.}

%\begin{center}
%\begin{DL}
%Concept $\sqsubseteq$ ($\neg$ D $\sqcap$ C) $\sqcup$ (D $\sqcap$ $\neg$ C), D $\equiv$ A $\sqcap$ B \\
%A $\sqsubseteq$ $\geq$ 1 notation.string $\sqcap$ $\leq$ 1 notation.string \\
%B $\sqsubseteq$ $\geq$ 1 prefLabel.string $\sqcap$ $\leq$ 1 prefLabel.string \\
%C $\sqsubseteq$ $\geq$ 1 definition.string $\sqcap$ $\leq$ 1 definition.string \\
%\end{DL}
%\end{center}

%\textbf{Searching for (Meta)data.}
%
%- DCAT
%
%\textbf{Data Integration.}
%use RDF validation for data integration

%\textbf{Series, Studies, Data Sets, and Data Files.}
%It is useful to declare properties to be \emph{conditional} (\emph{R-71}), i.e., if particular properties exist (or do not exist), then other properties must also be present (or absent).
%To get an overview over a series/study either an abstract, a title, an alternative title, or links to external descriptions should be provided. 
%If an abstract and an external description are absent, however,  
%a title or an alternative title should be given ($\mathcal{SL}_{1}$).
%For datatype properties, it should be possible to declare frequently needed \emph{facets} ({\emph{R-46}) to validate input against simple conditions including min/max values, regular expressions, and string length.
%The abstract of series/studies, e.g., should have a minimum length ($\mathcal{SL}_{1}$).
%\emph{Existential quantifications} (\emph{R-86}) enforce that instances of given classes must have some property relation to individuals of certain types.
%If a study, e.g., does not contain any data set ($\mathcal{SL}_{2}$), the actual description of the data is missing which may indicate that it is very hard or unlikely to get access to the data.
%If metadata on data files, including the actual data, is missing (especially case and variable quantities; $\mathcal{SL}_{1}$), the description of the data sets and the study is not sufficient.
%Case quantity measures how many cases are collected for a study.
%High case and variable quantities are indicators for high statistical quality and comprehensiveness of the underlying study ($\mathcal{SL}_{1}$).
%
%\textbf{Variables and Variable Comparison.}
%Variables should be represented (\emph{R-86}; $\mathcal{SL}_{1}$) either as (un)ordered code lists or as unions of datatypes.
%In case of a code list, associated categories (human-readable labels) may be stated (\emph{R-71}; $\mathcal{SL}_{0}$).
%The variable \emph{Education at pre-school}, e.g., is represented as ordered code list without any categories.
%If a {\em skos:Concept} represents a code (having {\em skos:notation} and {\em skos:prefLabel} properties), 
%then the property {\em DDI-RDF:isValid} has to be stated indicating if the code stands for valid (\emph{true}) or missing (\emph{false}) cases (\emph{R-71}; $\mathcal{SL}_{2}$).
%Variables may have at least one relationship to a theoretical concept ({\emph{R-86}; $\mathcal{SL}_{0}$).
%%The variables \emph{number of school years} and \emph{highest educational degree}, e.g., are associated with the theoretical concept \emph{education}. 
%The variable \emph{Education at pre-school},.e.g, is associated with the theoretical concept \emph{Child Care}. 
%The default severity level of this constraint is weak, as in most cases research can be continued without associated theoretical concepts.
%A very common research question is to compare variables of multiple studies or countries (\emph{comparison}).
%To compare variables, 
%(1) variables and (2) variable definitions must be present,
%(3) code lists must be structured properly,
%(4) for each code an associated category (human-readable label) must be specified, and
%(5) code lists must either be identical or at least similar.
%If a researcher wants to get a first overview over comparable variables (use case 1), 
%covering the first three constraints may be sufficient for this purpose.
%Thus, the severity level of the first three constraints is stronger ($\mathcal{SL}_{2}$) than the severity level of the next two constraints ($\mathcal{SL}_{1}$ and $\mathcal{SL}_{0}$).
%If the intention of the researcher is to perform more detailed comparisons (use case 2), however, the violation of the remaining two constraints is getting more serious.
%
%\textbf{Descriptive Statistics.}
%The property \emph{DDI-RDF:percentage} stands for the number of cases of a given code in relation to the total number of cases for a particular variable within a data set.
%Percentage values are only valid when they are within the \emph{literal range} of 0 and 100 (\emph{R-45}; $\mathcal{SL}_{2}$).
%{\em Mathematical Operations} (\emph{R-41, R-42}; e.g. date calculations and statistical computations like average, mean, and sum) are performed to ensure the integrity of data models.
%The sum of percentage values of all codes of a variable code list, e.g., must exactly be 100 ($\mathcal{SL}_{2}$)
%and the minimum of all variable codes do not have to be greater than the maximum ($\mathcal{SL}_{2}$).
%Codes (\emph{skos:Concept}) are ordered and therefore have fixed positions in an ordered collection (\emph{skos:OrderedCollection}) within variable representations.
%In order to check the correctness of relative frequencies' calculations, the cumulative percentage (\emph{DDI-RDF:cumulativePercentage}) of the current code must exactly be the cumulative percentage of the previous code
%plus the percentage value (\emph{DDI-RDF:percentage}) of the current code (\emph{data model consistency}; $\mathcal{SL}_{2}$).
%
%\textbf{Unique Identification.}
%It is often useful to declare a given (data) property as the \emph{primary key} (\emph{R-226}) of a class, so that a system can enforce uniqueness and also automatically build URIs from user inputs and imported data. 
%In DDI-RDF, resources are uniquely identified by the property \emph{adms:identifier},
%which is therefore inverse-functional
%$(\ms{funct identifier}\sp{\overline{\ }})$,
%i.e. for each \emph{rdfs:Resource x}, there can be at most one distinct \emph{rdfs:Resource y} such that \emph{y} is connected by \emph{adms:identifier$\sp{\overline{\ }}$} to \emph{x} ($\mathcal{SL}_{2}$).
%Keys, however, are even more general than inverse-functional properties (\emph{R-58}),
%as a key can be a data, an object property, or a chain of properties \cite{Schneider2009}.
%For this generalization purposes, as there are different sorts of key, and as keys can lead to undecidability, 
%DL is extended with \emph{key boxes} and a special \emph{keyfor} construct (\ms{identifier \ms{keyfor} Resource}) \cite{Lutz2005}.
%OWL 2 \emph{hasKey} implements \emph{keyfor} ($\mathcal{SL}_{2}$) and thus can be used to identify resources uniquely, to merge resources with identical key property values, and to recognize constraint violations.
%%OWL 2 hasKey can be used to identify resources uniquely
%%
%%We used Protégé 5.
%%
%%example: owl:Thing owl:hasKey ( :hasSSN ) . :Peter :hasSSN "123-45-6789" .
%%:Peter_Griffin :hasSSN "123-45-6789" .
%%
%%We use the predefined Reasoner HermiT.
%%
%%:Peter and :Peter_Griffin are derived as identical from the reasoner as they have the same value for :hasSSN.
%%
%%hasKey can be used to merge resources and to recognize constraint violations.
%%
%%Alternative to OWL 2 hasKey: concise bounded description (proposal of Dan)
%%
%%action: explore CBD document solution might be using both approaches
%%
%%benefit: additional identification to URI / IRI benefit: compound properties can be used as key
%
%
%\textbf{Membership in Controlled Vocabularies.}
%In many cases, resources must be members of controlled vocabularies (\emph{R-32}).
%If a dimension property, e.g., has a \emph{qb:codeList},
%then the value of the dimension property on every \emph{qb:Observation} must be in the code list ($\mathcal{SL}_{2}$).
%Summary statistics types like minimum, maximum, and arithmetic mean are maintained within a controlled vocabulary.  
%Summary statistics can only have \emph{DDI-RDF:summaryStatisticType} relationships to \emph{skos:Concept}s which must be members of the controlled vocabulary \emph{ddicv:SummaryStatisticType}, a \emph{skos:ConceptScheme} ($\mathcal{SL}_{2}$).
%
%%\begin{center}
%%\begin{DL}
%%SummaryStatistics $\sqsubseteq$ $\forall summaryStatisticType.A$ \\
%%$A \equiv Concept \sqcap \forall inScheme . B$ \\
%%$B \equiv ConceptScheme \sqcap \{SummaryStatisticType\}$
%%\end{DL}
%%\end{center}
%
%\textbf{Coverage.}
%Information about the temporal (\emph{dcterms:temporal}), the spatial (\emph{dcterms:spatial}), and the topical coverage (\emph{dcterms:subject}) of series, studies, data sets, and data files (\emph{R-86}; $\mathcal{SL}_{1}$) is of interest when performing frequently formulated queries 
	%(e.g. to search for all data sets of given years (temporal coverage) in which data is collected in certain countries (spatial coverage) about particular topics (topical coverage)).
%Depending on property datatypes,
%two different literal values have
%a specific ordering with respect to an operator like \textless (\emph{R-43: literal value comparison}).
%Start dates (\emph{DDI-RDF:startDate}), e.g., must be before (\emph{\textless}) end dates (\emph{DDI-RDF:endDate}) ($\mathcal{SL}_{2}$).
%
%%\textbf{Organizations, Hierarchies, Classifications, and Ordering.}
%\textbf{Hierarchies and Ordering.}
%SKOS is based on RDF, which is a graph-based data model. Therefore, we can concentrate on the vocabulary's graph-based structure for assessing the quality of SKOS vocabularies and apply graph- and network-analysis techniques (\emph{structure}) like 
%(1) a vocabulary should provide entry points (top concepts) to the data to provide efficient access and guidance for human users,
%(2) concepts, internal to the tree, should not be indicated as top concepts, and
%(3) a vocabulary should not contain many orphan concepts 
%(concepts without any associative or hierarchical relations) lacking valuable context information. A controlled vocabulary that contains many orphan concepts is less usable for search and retrieval use cases.
%%, as, e.g., no hierarchical query expansion can be performed on search terms to find documents with more general content.) \cite{MaderHaslhoferIsaac2012}. 
%Objects and literals can be \emph{ordered} (\emph{R-121, R-217}) for given properties.
%\emph{DDI-RDF }variables, questions, and codes/categories are typically organized in a particular order. 
%If a variable code list should be ordered, the variable representation should be of the type \emph{skos:OrderedCollection} containing multiple codes/categories (each represented as \emph{skos:Concept}) in a \emph{skos:memberList}. 
%
%\textbf{Reusability.}
%Within the context of DDI-RDF, \emph{skos:Concept}s can have either \emph{skos:definition} (when interpreted as theoretical concepts) or \emph{skos:notation} and \emph{skos:prefLabel} properties (when interpreted as codes/categories), but not both ($\mathcal{SL}_{2}$).
%The constraint type \emph{context-specific exclusive or of property groups} (\emph{R-11})
%restricts individuals of given classes to have exactly one of multiple property groups.


\section{Evaluation}
\label{evaluation}

In this section, we describe our findings based on an automatic constraint checking of a large data set. Despite the large volume of the data sets in general, this study only uses data for three vocabularies. As described in Section~\ref{sbe-vocabularies}, for other vocabularies there is often not (yet) enough data openly available to draw general conclusions. The three vocabularies, however, are representative, cover different aspects of SBE data, and are also a mixture of widely adopted and accepted well-established vocabularies (QB, SKOS) and a vocabulary under development (DDI-RDF\footnote{Expected publication end of 2015.}). 
%If our findings hold generally is still to be determined, nevertheless they provide valuable insights for future developments of constraint languages.
As the evaluation is based on three vocabularies, 
we cannot make valid general statements for all vocabularies,
but we can formulate several findings to direct the further development of constraint languages.
As these findings cannot be proved yet, 
they still have to be verified or falsified
by evaluating the quality of data represented by further well-established and newly developed vocabularies.

\subsection{Experimental Setup}
\label{implementation}

%\ke{Überarbeiten und mit nächstem teil mergen. Nur das erzählen, was für die Auswertung auf den Datensets relevant ist. 115 constraints, 3 vokabulare, wie wurden die datasets daraufhin validiert.}

	%\item First, we assigned each constraint type to exactly one of the disjoint sets of constraint types 
%in order to get an overview how many constraint types in relation to the total amount of constraint types are extractable from vocabularies ($\mathcal{CT}_{B}$), are easily definable ($\mathcal{CT}_{S}$), and are rather difficult to specify ($\mathcal{CT}_{C}$). 
On the three vocabularies (DDI-RDF, QB, SKOS), we identified and classified 115 constraints\footnote{All 115 implemented constraints are online available at: \url{https://github.com/boschthomas/rdf-validation/tree/master/constraints}} which we implemented for data validation. We ensured that the implementation of the constraints is equally distributed over the classes and vocabularies we have. We then evaluated the data quality of 15,694 data sets (4.26 billion triples) of SBE research data using these 115 constraints, obtained from 33 SPARQL endpoints.
%\ke{Das ist eine Wiederholung, ggf. streichen, falls Platz knapp}

%QB and SKOS are well-established vocabularies which are widely adopted and accepted. DDI-RDF is a newly developed vocabulary which will be published in 2015.

%It is likely that such a vocabulary is still subject of constant change, that published data may not be consistent with its current version,
%and that early adopters did not properly understand its formal specification.
%\ke{ergebnisse, dann hypothesen, dann kritische auseinandersetzung}


Table~\ref{tab:datasets} lists the number of validated data sets and the overall sizes in terms of triples for each of the vocabularies. We validated, i.a., 
(1) QB data sets published by the \emph{Australian Bureau of Statistics},
the \emph{European Central Bank}, and the
\emph{Organisation for Economic Co-operation and Development},
(2) SKOS thesauri like the \emph{AGROVOC Multilingual agricultural thesaurus},
the \emph{STW Thesaurus for Economics}, and the
\emph{Thesaurus for the Social Sciences}, and
(3) DDI-RDF data sets provided by the \emph{Microdata Information System}, 
the \emph{Data Without Boundaries Discovery Portal}, the
\emph{Danish Data Archive}, and the
\emph{Swedish National Data Service}.
In a technical report, we describe the evaluation in further detail \cite{BoschZapilkoWackerowEckert2015-2}. Furthermore, we published the evaluation results for each QB data set in form of one document per SPARQL endpoint.\footnote{Online available at: \url{https://github.com/boschthomas/rdf-validation/tree/master/evaluation/data-sets/data-cube}}

\begin{table}[H]
		\scriptsize
    \begin{center}
		\caption{Validated Data Sets for each Vocabulary}
		\label{tab:datasets}
    \begin{tabular}{lrr}
%           & \multicolumn{6}{c}{\textbf{Vocabularies}}
%    \\  \cmidrule{2-7}
           \textbf{Vocabulary}
           & \textbf{Data Sets}
           & \textbf{Triples}
					 
    \\ \midrule
		%\emph{Triples} & 9,673,055 & 3,775,983,610 & 477,737,281 & 4,263,393,946 \\
		%\emph{Data Sets} & 1,526 & 9,990 & 4,178 & 15,694 \\
		%\hline
		\textbf{QB} & $9,990$   & $3,775,983,610$  \\
		\textbf{SKOS} & $4,178$  & $477,737,281$ \\
		\textbf{DDI-RDF} & $1,526$  & $9,673,055$  \\
    \bottomrule
    \end{tabular}
    %\\ \emph{C (constraints), CT (constraint types)}
    \end{center}
\end{table}

Since the validation of each of the 81 constraint types can be implemented using SPARQL, we use \emph{SPIN}, a SPARQL-based way to formulate and check constraints, as basis to develop a
validation environment to validate RDF data according to constraints expressed by arbitrary constraint languages\footnote{Constraint language implementations online available at: \url{https://github.com/boschthomas/rdf-validation/tree/master/SPIN}} \cite{BoschEckert2014-2}.
The \emph{RDF Validator}\footnote{Online demo available at: \url{http://purl.org/net/rdfval-demo}, source code online available at: \url{https://github.com/boschthomas/rdf-validator}} can directly be used to validate arbitrary RDF data for the three vocabularies. Additionally, own constraints on any vocabulary can be defined using several constraint languages.
The SPIN engine checks for each resource if it satisfies all constraints, which are associated with its assigned classes, and generates a result RDF graph containing information about all constraint violations.
There is one SPIN construct template for each constraint type.
A SPIN construct template contains a SPARQL CONSTRUCT query which generates constraint violation triples indicating the subject and the properties causing constraint violations and the reason why constraint violations have been raised.
A SPIN construct template creates constraint violation triples if all triple patterns within the SPARQL WHERE clause match.
%\emph{Missy}\footnote{\url{http://www.gesis.org/missy/eu/missy-home}} provides comprehensive Linked Data services like diverse RDF exports of unit-record metadata conforming to the DDI-RDF vocabulary in form of multiple concrete syntaxes. 

\subsection{Evaluation Results and Formulation of Findings}

%DCAT: 11 constraints defined
%PHDD: 12
%XKOS: 10

%The majority (48 $\equiv$ 58.5\%) of the overall 81 $\mathcal{CT}$ constraint types are $\mathcal{CT}_{B}$ whose constraints can therefore be derived from vocabularies without any effort.
%Among $\mathcal{CT}_{B}$, two-thirds (34 $\equiv$ 70.8\%) are $\mathcal{C}_B ^{\mathcal{R}}$, i.e., constraint types for which reasoning may be performed prior to validation, and one third (14 $\equiv$ 29.2\%) are $\overline{\mathcal{C}_B ^{\mathcal{R}}}$, i.e., constraint types for which reasoning does not make any sense.
%A quarter (20 $\equiv$ 24.4\%) of all constraint types are $\mathcal{CT}_{S}$ and a sixth (14 $\equiv$ 17.1\%) are $\mathcal{CT}_{C}$.
%
%\begin{table}[H]
		%\scriptsize
    %\begin{center}
    %\begin{tabular}{@{}lcccc@{}}
%%           & \multicolumn{6}{c}{\textbf{Vocabularies}}
%%    \\  \cmidrule{2-7}
    %\\       \textbf{Criteria}
           %& \textbf{DDI-RDF}
           %& \textbf{QB}
					 %& \textbf{SKOS}
					 %& \textbf{Total}
    %\\ \midrule
		%%\emph{Triples} & 9,673,055 & 3,775,983,610 & 477,737,281 & 4,263,393,946 \\
		%%\emph{Data Sets} & 1,526 & 9,990 & 4,178 & 15,694 \\
		%%\hline
		%\emph{CT} & 52 & 20 & 15 & 53 \\
		%\hline
		%$\mathcal{CT}_{C}$ & 9 (17.3\%) & 2 (10\%) & 2 (13.3\%) & 9 (17\%) \\
		%$\mathcal{CT}_{S}$ & 16 (30.8\%) & 3 (15\%) & 4 (26,7\%) & 16 (30.2\%) \\
		%$\mathcal{CT}_{B}$ & 27 (\textbf{51.9\%}) & 15 (\textbf{75\%}) & 9 (\textbf{60\%}) & 28 (\textbf{52.8\%}) \\
		%\hline
		%$\mathcal{C}_B ^{\mathcal{R}}$ & 18 (34.6\%) & 9 (45\%) & 4 (26.7\%) & 19 (35.9\%) \\
		%$\overline{\mathcal{C}_B ^{\mathcal{R}}}$ & 9 (17.3\%) & 6 (30\%) & 5 (33.3\%) & 9 (17\%) \\
    %\bottomrule
    %\end{tabular}
    %%\\ \emph{C (constraints), CT (constraint types)}
    %\caption{Evaluation - Constraint Types}
		%\label{tab:evaluation-constraint-types}
    %\end{center}
%\end{table}

%Table \ref{tab:evaluation-constraint-types} displays the evaluation of the metadata quality on real world data sets regarding constraint types.
Tables \ref{tab:evaluation-constraint-violations-1} and \ref{tab:evaluation-constraint-violations-2} show the results of the evaluation, more specifically the constraints and the constraint violations, which are caused by these constraints, in percent; whereas the numbers in the first line indicate the absolute amount of constraints and violations. The constraints and their raised violations are grouped by vocabulary, which type of language the constraint types are formulated with, and their severity level.
The numbers of validated triples and data sets differ between the vocabularies
as we validated 3.8 billion QB, 480 million SKOS, and 10 million DDI-RDF triples.
To be able to formulate findings which apply for all vocabularies, 
we only use normalized relative values representing the percentage of constraints and violations belonging to the respective sets. 

There is a strong overlap between \emph{RDFS/OWL} and \emph{Constraint Language Based} constraint types as in many cases constraint types are expressible by RDFS/OWL and classical constraint languages. This is the reason why the percentage values of constraints and violations grouped by the classification of constraint types according to the expressivity of constraint languages do not accumulate to 100\%. 

%\ke{da kam noch mal der limitations disclaimer, ich denke, einmal reicht... habs auskommentiert}
%As the evaluation is based on three vocabularies, 
%we cannot make valid general statements for all vocabularies,
%but we can formulate several findings to direct the further development of constraint languages.
%As these findings cannot be proved yet, 
%they still have to be verified or falsified
%by evaluating the quality of data represented by further well-established and newly developed vocabularies.

%by building the arithmetic mean over the individual vocabularies' percentage values. 
%\tb{More than 80\% of the overall approx. 55 million constraint violations are caused by QB constraints.}

%\begin{table}[H]
		%\scriptsize
    %\begin{center}
		%\caption{Constraints and Constraint Violations}
		%\label{tab:evaluation-constraint-violations}
    %\begin{tabular}{@{}lcccccccc@{}}
    %%       & \multicolumn{2}{c}{\textbf{Vocabularies}}
    %%\\  \cmidrule{2-3}
		%\hline
    %\multirow{2}{*}{} &
      %\multicolumn{2}{c}{\textbf{DDI-RDF}} &
      %\multicolumn{2}{c}{\textbf{QB}} &
      %\multicolumn{2}{c}{\textbf{SKOS}} &
      %\multicolumn{2}{c}{\textbf{\emph{Total}}} \\
    %\textbf{} & C & CV & C & CV & C & CV & C & CV \\
    %\hline
    %%\\ \midrule
		 %& 143 & 3,575,002 & 35 & 45,635,861 & 35 & 5,540,988 & 213 & 54,751,851 \\
		%\hline
		%\textbf{\emph{complex}} & 25.9\% & 18.3\% & 37.1\% & \textbf{100.0\%} & \textbf{37.1\%} & 21.4\% & 33.4\% & \textbf{46.6\%} \\
		%\textbf{\emph{simple}} & 19.6\% & 15.7\% & 8.6\% & 0.0\% & \textbf{34.3\%} & \textbf{78.6\%} & 20.8\% & 31.4\% \\
		%\textbf{\emph{vocabulary}} & \textbf{54.6\%} & \textbf{66.1\%} & \textbf{54.3\%} & 0.0\% & \textbf{28.6\%} & 0.0\% & \textbf{45.8\%} & 22.0\% \\
		%%\hline
		%%\emph{CV (}$\mathcal{C}_B ^{\mathcal{R}}$\emph{)} & 2,333,365 (65.3\%) & 1,777 (0\%) & 0 (0\%) & 2,335,142 (4.3\%) \\
		%%\emph{CV (}$\overline{\mathcal{C}_B ^{\mathcal{R}}}$\emph{)} & 28,000 (0.8\%) & 0 (0\%) & 0 (0\%) & 28,000 (0.1\%) \\
		%\hline
		%\textbf{\emph{info}} & \textbf{52.5\%} & \textbf{52.6\%} & 11.4\% & 0.0\% & \textbf{60.0\%} & 41.2\% & \textbf{41.3\%} & 31.3\% \\
		%\textbf{\emph{warning}} & 7.0\% & 29.4\% & 8.6\% & \textbf{99.8\%} & 14.3\% & \textbf{58.8\%} & 10.0\% & \textbf{62.7\%} \\
		%\textbf{\emph{error}} & 40.6\% & 18.0\% & \textbf{80.0\%} & 0.3\% & 25.7\% & 0.0\% & \textbf{48.8\%} & 6.1\%\\
    %\bottomrule
    %\end{tabular}
    %\\ \emph{C (constraints), CV (constraint violations)}
    %\end{center}
%\end{table}

%\begin{table}[H]
		%\scriptsize
    %\begin{center}
		%\caption{OLD}
		%\label{tab:evaluation-constraint-violations-1}
    %\begin{tabular}{@{}lcccc@{}}
    %%       & \multicolumn{2}{c}{\textbf{Vocabularies}}
    %%\\  \cmidrule{2-3}
		%\hline
    %\multirow{2}{*}{} &
      %\multicolumn{2}{c}{\textbf{DDI-RDF}} &
      %\multicolumn{2}{c}{\textbf{QB}} \\
    %\textbf{} & C & CV & C & CV \\
    %\hline
    %%\\ \midrule
		 %& 143 & 3,575,002 & 35 & 45,635,861 \\
		%\hline
		%\textbf{\emph{complex}} & 25.9\% & 18.3\% & 37.1\% & \textbf{100.0\%} \\
		%\textbf{\emph{simple}} & 19.6\% & 15.7\% & 8.6\% & 0.0\% \\
		%\textbf{\emph{vocabulary}} & \textbf{54.6\%} & \textbf{66.1\%} & \textbf{54.3\%} & 0.0\% \\
		%\hline
		%\textbf{\emph{info}} & \textbf{52.5\%} & \textbf{52.6\%} & 11.4\% & 0.0\% \\
		%\textbf{\emph{warning}} & 7.0\% & 29.4\% & 8.6\% & \textbf{99.8\%} \\
		%\textbf{\emph{error}} & 40.6\% & 18.0\% & \textbf{80.0\%} & 0.3\% \\
    %\bottomrule
    %\end{tabular}
    %\\ \emph{C (constraints), CV (constraint violations)}
    %\end{center}
%\end{table}
%
%\begin{table}[H]
		%\scriptsize
    %\begin{center}
		%\caption{OLD}
		%\label{tab:evaluation-constraint-violations-2}
    %\begin{tabular}{@{}lcccc@{}}
    %%       & \multicolumn{2}{c}{\textbf{Vocabularies}}
    %%\\  \cmidrule{2-3}
		%\hline
    %\multirow{2}{*}{} &
      %\multicolumn{2}{c}{\textbf{SKOS}} &
      %\multicolumn{2}{c}{\textbf{\emph{Total}}} \\
    %\textbf{} & C & CV & C & CV \\
    %\hline
    %%\\ \midrule
		 %& 35 & 5,540,988 & 213 & 54,751,851 \\
		%\hline
		%\textbf{\emph{complex}} & \textbf{37.1\%} & 21.4\% & 33.4\% & \textbf{46.6\%} \\
		%\textbf{\emph{simple}} & \textbf{34.3\%} & \textbf{78.6\%} & 20.8\% & 31.4\% \\
		%\textbf{\emph{vocabulary}} & \textbf{28.6\%} & 0.0\% & \textbf{45.8\%} & 22.0\% \\
		%\hline
		%\textbf{\emph{info}} & \textbf{60.0\%} & 41.2\% & \textbf{41.3\%} & 31.3\% \\
		%\textbf{\emph{warning}} & 14.3\% & \textbf{58.8\%} & 10.0\% & \textbf{62.7\%} \\
		%\textbf{\emph{error}} & 25.7\% & 0.0\% & \textbf{48.8\%} & 6.1\%\\
    %\bottomrule
    %\end{tabular}
    %\\ \emph{C (constraints), CV (constraint violations)}
    %\end{center}
%\end{table}

%\begin{table}[H]
		%\scriptsize
    %\begin{center}
		%\caption{Constraints and Constraint Violations (1)}
		%\label{tab:evaluation-constraint-violations-1}
    %\begin{tabular}{@{}lcccc@{}}
    %%       & \multicolumn{2}{c}{\textbf{Vocabularies}}
    %%\\  \cmidrule{2-3}
		%\hline
    %\multirow{2}{*}{} &
      %\multicolumn{2}{c}{\textbf{DDI-RDF}} &
      %\multicolumn{2}{c}{\textbf{QB}} \\
    %\textbf{} & C & CV & C & CV \\
    %\hline
    %%\\ \midrule
		 %& 143 & 3,575,002 & 35 & 45,635,861 \\
		%\hline
		%\textbf{\emph{SPARQL}} & 44.8\% & 34.7\% & \textbf{54.3\%} & \textbf{100.0\%} \\
		%\textbf{\emph{CL}} & 42.0\% & \textbf{65.3\%} & 28.6\% & 0.0\% \\
		%\textbf{\emph{RDFS/OWL}} & \textbf{51.8\%} & \textbf{65.3\%} & 42.9\% & 0.0\% \\
		%\hline
		%\textbf{\emph{info}} & \textbf{52.5\%} & \textbf{52.6\%} & 11.4\% & 0.0\% \\
		%\textbf{\emph{warning}} & 7.0\% & 29.4\% & 8.6\% & \textbf{99.8\%} \\
		%\textbf{\emph{error}} & 40.6\% & 18.0\% & \textbf{80.0\%} & 0.3\% \\
    %\bottomrule
    %\end{tabular}
    %\\ \emph{C (constraints), CV (constraint violations)}
    %\end{center}
%\end{table}
%
%\begin{table}[H]
		%\scriptsize
    %\begin{center}
		%\caption{Constraints and Constraint Violations (2)}
		%\label{tab:evaluation-constraint-violations-2}
    %\begin{tabular}{@{}lcccc@{}}
    %%       & \multicolumn{2}{c}{\textbf{Vocabularies}}
    %%\\  \cmidrule{2-3}
		%\hline
    %\multirow{2}{*}{} &
      %\multicolumn{2}{c}{\textbf{SKOS}} &
      %\multicolumn{2}{c}{\textbf{\emph{Total}}} \\
    %\textbf{} & C & CV & C & CV \\
    %\hline
    %%\\ \midrule
		 %& 35 & 5,540,988 & 213 & 54,751,851 \\
		%\hline
		%\textbf{\emph{SPARQL}} & \textbf{80.0\%} & \textbf{100.0\%} & \textbf{59.7\%} & \textbf{78.2\%} \\
		%\textbf{\emph{CL}} & 0.0\% & 0.0\% & 23.5\% & 21.8\% \\
		%\textbf{\emph{RDFS/OWL}} & 20.0\% & 0.0\% & 38.2\% & 21.8\% \\
		%\hline
		%\textbf{\emph{info}} & \textbf{60.0\%} & 41.2\% & \textbf{41.3\%} & 31.3\% \\
		%\textbf{\emph{warning}} & 14.3\% & \textbf{58.8\%} & 10.0\% & \textbf{62.7\%} \\
		%\textbf{\emph{error}} & 25.7\% & 0.0\% & \textbf{48.8\%} & 6.1\%\\
    %\bottomrule
    %\end{tabular}
    %\\ \emph{C (constraints), CV (constraint violations)}
    %\end{center}
%\end{table}

\begin{table}[H]
		\scriptsize
    \begin{center}
		\caption{Constraints and Constraint Violations (1)}
		\label{tab:evaluation-constraint-violations-1}
    \begin{tabular}{@{}lcc|cc@{}}
    %       & \multicolumn{2}{c}{\textbf{Vocabularies}}
    %\\  \cmidrule{2-3}
		%\hline
    \multirow{2}{*}{} &
      \multicolumn{2}{c}{\textbf{DDI-RDF}} &
      \multicolumn{2}{c}{\textbf{QB}} \\
    \textbf{} & C & CV & C & CV \\
    \hline
    %\\ \midrule
		 & 78 & 3,575,002 & 20 & 45,635,861 \\
		\hline
		\textbf{\emph{SPARQL}} & 29.5 & 34.7 & \textbf{60.0} & \textbf{100.0} \\
		\textbf{\emph{CL}} & \textbf{64.1} & \textbf{65.3} & 40.0 & 0.0 \\
		\textbf{\emph{RDFS/OWL}} & \textbf{66.7} & \textbf{65.3} & 40.0 & 0.0 \\
		\hline
		\textbf{\emph{info}} & \textbf{56.4} & \textbf{52.6} & 0.0 & 0.0 \\
		\textbf{\emph{warning}} & 11.5 & 29.4 & 15.0 & \textbf{99.8} \\
		\textbf{\emph{error}} & 32.1 & 18.0 & \textbf{85.0} & 0.3 \\
    \bottomrule
    \end{tabular}
    \\ \emph{C (constraints), CV (constraint violations)}
    \end{center}
\end{table}

\begin{table}[H]
		\scriptsize
    \begin{center}
		\caption{Constraints and Constraint Violations (2)}
		\label{tab:evaluation-constraint-violations-2}
    \begin{tabular}{@{}lcc|cc@{}}
    %       & \multicolumn{2}{c}{\textbf{Vocabularies}}
    %\\  \cmidrule{2-3}
		%\hline
    \multirow{2}{*}{} &
      \multicolumn{2}{c}{\textbf{SKOS}} &
      \multicolumn{2}{c}{\textbf{\emph{Total}}} \\
    \textbf{} & C & CV & C & CV \\
    \hline
    %\\ \midrule
		 & 17 & 5,540,988 & 115 & 54,751,851 \\
		\hline
		\textbf{\emph{SPARQL}} & \textbf{100.0} & \textbf{100.0} & \textbf{63.2} & \textbf{78.2} \\
		\textbf{\emph{CL}} & 0.0 & 0.0 & 34.7 & 21.8 \\
		\textbf{\emph{RDFS/OWL}} & 0.0 & 0.0 & 35.6 & 21.8 \\
		\hline
		\textbf{\emph{info}} & \textbf{70.6} & 41.2 & \textbf{42.3} & 31.3 \\
		\textbf{\emph{warning}} & 29.4 & \textbf{58.8} & 18.7 & \textbf{62.7} \\
		\textbf{\emph{error}} & 0.0 & 0.0 & \textbf{39.0} & 6.1\\
    \bottomrule
    \end{tabular}
    \\ \emph{C (constraints), CV (constraint violations)}
    \end{center}
\end{table}

%More than the half of them are $\mathcal{CT}_{B}$, nearly a third $\mathcal{CT}_{S}$, and only a sixth $\mathcal{CT}_{C}$ constraint types.
%For DDI-RDF, QB, and SKOS, more than 50\% of the instantiated constraint types are $\mathcal{CT}_{B}$ constraint types 
%(for QB even three quarters).
%\emph{Existential quantifications} (\emph{R-86}, 32.4\%, DDI-RDF), \emph{data model consistency} (31.4\%, QB), and \emph{structure} (28.6\%, SKOS) are the constraint types the most constraints are instantiated from.
%\tb{1. Fakten (diese Fakten bewirken eine Erhöhung der Datenqualität) 2. Hypothese (unsere Interpretation der Ergebnisse, unsere Behauptung)}

Almost 2/3 of all constraints, nearly 1/3 of the DDI-RDF, 60\% of the QB, and all SKOS constraints are \emph{SPARQL Based}. For well-established vocabularies, the most formulated constraints are \emph{SPARQL Based} (80\%). For newly developed vocabularies, however, the most expressed constraints are \emph{RDFS/OWL Based} (2/3). 
%
Nearly 80\% of all violations are caused by \emph{SPARQL}, 1/5 by \emph{Constraint Language}, and 1/5 by \emph{RDFS/OWL Based} constraints.
%Hence, a significant amount of 46 \% of the constraints are directly extractable from formal specifications of well-established and newly defined vocabularies. 
%\begin{hyp}
%As a significant amount of 46 \% of the constraints are vocabulary constraints which can be expressed by modeling languages like RDF, RDFS, and OWL 2,
%the further development of constraint languages should concentrate on expressing simple and especially complex constraints which up to now in most cases can only be expressed by plain SPARQL.   
%As a significant amount of 54\% of the constraints are either simple or complex constraints,
%the further development of constraint languages should concentrate on expressing simple and especially complex constraints which up to now in most cases can only be expressed by plain SPARQL.  
%\end{hyp}
\begin{hyp}
The facts that 80\% of all violations are raised by SPARQL Based constraints and that 2/3 of all constraints are SPARQL Based, increases the importance to formulate constraints, which up to now can only be expressed in SPARQL, using high-level constraint languages. Data quality can be significantly improved when suitable constraint languages are developed which enable to define SPARQL Based constraints in an easy, concise, and intuitive way. Thereby, the more elaborate a vocabulary is, the more sophisticated and complex constraints are specified using SPARQL. 
\end{hyp} 

These constraints are of such complexity that up to now in most cases they can only be expressed by plain SPARQL. It should be an incentive for language designers to devise languages which are more intuitive than SPARQL in a way that also domain experts, which are not familiar with SPARQL, can formulate respective constraints.

%Nearly 1/3 of all constraints are either \emph{RDFS/OWL} or \emph{Constraint Language Based}.

%The fact that only 1/5 of all violations result from \emph{RDFS/OWL Based} constraints, even though 1/3 of all constraints are \emph{RDFS/OWL Based}, indicates good data quality for all vocabularies with regard to their formal specifications.

\begin{hyp}
The fact that only 1/5 of all violations result from RDFS/OWL Based constraints, even though 1/3 of all constraints are RDFS/OWL Based, indicates good data quality for all vocabularies with regard to their formal specifications.
%Data corresponds to formal specifications of vocabularies the data is conforming to. 
%This demonstrates that constraint formulation in general works. 
\end{hyp}

\begin{hyp}
As 1/3 of all constraints are RDFS/OWL Based, the first step to make progress in the further development of constraint languages is to cover the constraint types which can already be formulated using RDFS and OWL.
\end{hyp} 

%\tb{je nach hypothese spezifische aufgaben um die hypothese zu beweisen}
While 2/3 of the DDI-RDF violations result from \emph{RDFS/OWL Based} constraints,
QB and SKOS violations are only raised by \emph{SPARQL Based} constraints.

\begin{hyp}
For well-established vocabularies, RDFS/OWL Based constraints are almost completely satisfied which generally indicates very impressive data quality, at least in the SBE domain and for the basic requirements. For newly developed vocabularies, however, data quality is poor as RDFS/OWL Based constraints are not fulfilled. 
\end{hyp}

For DDI-RDF, data providers still have to understand the vocabulary and of course data cannot have high quality if the specification is not yet stable.
It is likely that a newly developed vocabulary is still subject of constant change
and that early adopters did not properly understand its formal specification.
Thus, published data may not be consistent with the current draft of its conforming vocabulary.
In case newly developed vocabularies turn into well-established ones,
data providers are experienced in publishing their data in conformance with these vocabularies
and formal specifications are more elaborated. 
As a consequence, \emph{RDFS/OWL Based} constraints are satisfied to a greater extend which leads to better data quality.  
%
The reason why we only defined \emph{SPARQL Based} constraints for assessing the quality of thesauri is that literature and practice especially concentrate on evaluating graph-based structures of thesauri by applying graph- and network-analysis techniques which can only be implemented by SPARQL.

%Therefore, existing constraint languages have to be extended or new constraint languages have to be developed.

%\begin{table}[H]
		%\scriptsize
    %\begin{center}
		%\caption{Constraints and Constraint Violations (1)}
		%\label{tab:evaluation-constraint-violations-1}
    %\begin{tabular}{@{}lcccc@{}}
    %%       & \multicolumn{2}{c}{\textbf{Vocabularies}}
    %%\\  \cmidrule{2-3}
		%\hline
    %\multirow{2}{*}{} &
      %\multicolumn{2}{c}{\textbf{DDI-RDF}} &
      %\multicolumn{2}{c}{\textbf{QB}} \\
    %\textbf{} & C & CV & C & CV \\
    %\hline
    %%\\ \midrule
		 %& 143 & 3,575,002 & 35 & 45,635,861 \\
		%\hline
		%\textbf{\emph{SPARQL}} & 44.8\% & 34.7\% & \textbf{54.3\%} & \textbf{100.0\%} \\
		%\textbf{\emph{CL}} & 42.0\% & \textbf{65.3\%} & 28.6\% & 0.0\% \\
		%\textbf{\emph{RDFS/OWL}} & \textbf{51.8\%} & \textbf{65.3\%} & 42.9\% & 0.0\% \\
		%\hline
		%\textbf{\emph{info}} & \textbf{52.5\%} & \textbf{52.6\%} & 11.4\% & 0.0\% \\
		%\textbf{\emph{warning}} & 7.0\% & 29.4\% & 8.6\% & \textbf{99.8\%} \\
		%\textbf{\emph{error}} & 40.6\% & 18.0\% & \textbf{80.0\%} & 0.3\% \\
    %\bottomrule
    %\end{tabular}
    %\\ \emph{C (constraints), CV (constraint violations)}
    %\end{center}
%\end{table}
%
%\begin{table}[H]
		%\scriptsize
    %\begin{center}
		%\caption{Constraints and Constraint Violations (2)}
		%\label{tab:evaluation-constraint-violations-2}
    %\begin{tabular}{@{}lcccc@{}}
    %%       & \multicolumn{2}{c}{\textbf{Vocabularies}}
    %%\\  \cmidrule{2-3}
		%\hline
    %\multirow{2}{*}{} &
      %\multicolumn{2}{c}{\textbf{SKOS}} &
      %\multicolumn{2}{c}{\textbf{\emph{Total}}} \\
    %\textbf{} & C & CV & C & CV \\
    %\hline
    %%\\ \midrule
		 %& 35 & 5,540,988 & 213 & 54,751,851 \\
		%\hline
		%\textbf{\emph{SPARQL}} & \textbf{80.0\%} & \textbf{100.0\%} & \textbf{59.7\%} & \textbf{78.2\%} \\
		%\textbf{\emph{CL}} & 0.0\% & 0.0\% & 23.5\% & 21.8\% \\
		%\textbf{\emph{RDFS/OWL}} & 20.0\% & 0.0\% & 38.2\% & 21.8\% \\
		%\hline
		%\textbf{\emph{info}} & \textbf{60.0\%} & 41.2\% & \textbf{41.3\%} & 31.3\% \\
		%\textbf{\emph{warning}} & 14.3\% & \textbf{58.8\%} & 10.0\% & \textbf{62.7\%} \\
		%\textbf{\emph{error}} & 25.7\% & 0.0\% & \textbf{48.8\%} & 6.1\%\\
    %\bottomrule
    %\end{tabular}
    %\\ \emph{C (constraints), CV (constraint violations)}
    %\end{center}
%\end{table}

Almost 40\% of all constraints are \emph{error}, more than 40\% are \emph{informational}, and nearly 20\% are \emph{warning} constraints.
\emph{Informational} constraints caused almost 1/3 and
\emph{warning} constraints narrowly 2/3 of all violations.
%As the percentage of severe violations is very low for all vocabularies (6.1\%),
%data quality is high with regard to the severity level of constraints.

\begin{hyp}
Although 40\% of all constraints are error constraints, the percentage of severe violations is very low, compared to about 2/3 of warning and 1/3 of informational violations. This implies that data quality is high with regard to the severity level of constraints and that proper constraint languages can significantly improve data quality beyond fundamental requirements.
\end{hyp} 

We did not detect violations of \emph{error} constraints for well-established vocabularies, even though 85\% of the QB constraints are \emph{error} constraints. 
%85\% of the QB constraints are \emph{error} constraints.
More than 50\% of the DDI-RDF and SKOS constraints are \emph{informational} constraints.
1/6 of the DDI-RDF violations are caused by \emph{error} constraints and
almost all QB and 59\% of the SKOS violations are caused by \emph{warning} constraints. 
%For well-established vocabularies, we detected no violations of \emph{error} constraints.  
%Constraints are serious when they violate the integrity of vocabularies.
%For well-established vocabularies, it is well documented and the experience is high how to guarantee this integrity
%which is not the case or newly-developed vocabularies.

\begin{hyp}
%\ke{das ist doch das gleiche wie 4, oder?}\bz{Stimme Kai zu. Finding 4 und 6 koennte man zusammenfassen oder falls es doch Unterschiede gibt, muessen die staerker hervorgehoben werden.}
For well-established vocabularies, data quality is high as serious violations rarely appear (0.3\% for QB). For newly developed vocabularies, however, data quality is worse as serious violations occur partially (1/6 for DDI-RDF).   
\end{hyp} 

Especially for newly developed vocabularies, constraint languages should be used to a larger extend in addition to RDFS/OWL in order to define appropriate constraints to detect and solve severe violations.
%\ke{aber das wird doch gemacht, halt mit RDFS und OWL, weswegen die etablierten ja besser sind.}
%to reduce severe violations by checking and solving them.

%For the vocabularies DDI-RDF, QB, and SKOS, we defined 213 constraints of 53 constraint types .

%\begin{table}[H]
		%\scriptsize
    %\begin{center}
    %\begin{tabular}{@{}lcccc@{}}
%%           & \multicolumn{6}{c}{\textbf{Vocabularies}}
%%    \\  \cmidrule{2-7}
    %\\       \textbf{Criteria}
           %& \textbf{DDI-RDF}
           %& \textbf{QB}
					 %& \textbf{SKOS}
					 %& \textbf{Total}
    %\\ \midrule
		%%\emph{Triples} & 9,673,055 & 3,775,983,610 & 477,737,281 & 4,263,393,946 \\
		%%\emph{Data Sets} & 1,526 & 9,990 & 4,178 & 15,694 \\
		%%\hline
		%\emph{C} & 143 (67.1\%) & 35 (16.4\%) & 35 (16.4\%) & 213 \\
		%\hline
		%\emph{C (}$\mathcal{CT}_{C}$\emph{)} & 37 (25.9\%) & 13 (37.1\%) & 13 (\textbf{37.1\%}) & 63 (29.6\%) \\
		%\emph{C (}$\mathcal{CT}_{S}$\emph{)} & 28 (19.6\%) & 3 (8.6\%) & 12 (\textbf{34.3\%}) & 43 (20.2\%) \\
		%\emph{C (}$\mathcal{CT}_{B}$\emph{)} & 78 (\textbf{54.6\%}) & 19 (\textbf{54.3\%}) & 10 (\textbf{28.6\%}) & 107 (\textbf{50.2\%}) \\
		%%\hline
		%%\emph{C (}$\mathcal{C}_B ^{\mathcal{R}}$\emph{)} & 67 (46.9\%) & 13 (37.1\%) & 4 (11.4\%) & 84 (39.4\%) \\
		%%\emph{C (}$\overline{\mathcal{C}_B ^{\mathcal{R}}}$\emph{)} & 11 (7.7\%) & 6 (17.1\%) & 6 (17.1\%) & 23 (10.8\%) \\
		%\hline
		%\emph{C ($\mathcal{SL}_{0}$)} & 75 (\textbf{52.5\%}) & 4 (11.4\%) & 21 (\textbf{60\%}) & 100 (\textbf{46.9\%}) \\
		%\emph{C ($\mathcal{SL}_{1}$)} & 10 (7\%) & 3 (8.6\%) & 5 (14.3\%) & 18 (8.5\%) \\
		%\emph{C ($\mathcal{SL}_{2}$)} & 58 (40.6\%) & 28 (\textbf{80\%}) & 9 (25.7\%) & 95 (\textbf{44.6\%}) \\
    %\bottomrule
    %\end{tabular}
    %%\\ \emph{C (constraints), CT (constraint types)}
    %\caption{Evaluation - Constraints}
		%\label{tab:evaluation-constraints}
    %\end{center}
%\end{table}





%\begin{table}[H]
		%\scriptsize
    %\begin{center}
    %\begin{tabular}{@{}lcccccccc@{}}
%%           & \multicolumn{6}{c}{\textbf{Vocabularies}}
%%    \\  \cmidrule{2-7}
    %\\       \textbf{}
           %& \textbf{DDI-RDF}
           %& \textbf{QB}
					 %& \textbf{SKOS}
					 %& \textbf{Total}
					 %& \textbf{DDI-RDF}
           %& \textbf{QB}
					 %& \textbf{SKOS}
					 %& \textbf{Total}
    %\\ \midrule
		 %& 3,575,002 & 45,635,861 & 5,540,988 & 54,751,851 & & & &  \\
		%\hline
		%\textbf{\emph{complex}} & 18.3\% & \textbf{100\%} & 21.4\% & \textbf{86.7\%} & & & & \\
		%\textbf{\emph{simple}} & 15.7\% & 0.0\% & \textbf{78.6\%} & 9\% & & & & \\
		%\textbf{\emph{vocabulary}} & \textbf{66.1\%} & 0.0\% & 0\% & 4.3\% & & & & \\
		%%\hline
		%%\emph{CV (}$\mathcal{C}_B ^{\mathcal{R}}$\emph{)} & 2,333,365 (65.3\%) & 1,777 (0\%) & 0 (0\%) & 2,335,142 (4.3\%) \\
		%%\emph{CV (}$\overline{\mathcal{C}_B ^{\mathcal{R}}}$\emph{)} & 28,000 (0.8\%) & 0 (0\%) & 0 (0\%) & 28,000 (0.1\%) \\
		%\hline
		%\textbf{\emph{info}} & \textbf{52.6\%} & 0.0\% & 41.2\% & 7.6\% & & & & \\
		%\textbf{\emph{warning}} & 29.4\% & \textbf{99,8\%} & \textbf{58.8\%} & \textbf{91\%} & & & & \\
		%\textbf{\emph{error}} & 18\% & 0.3\% & 0.0\% & 1.4\% & & & & \\
    %\bottomrule
    %\end{tabular}
    %%\\ \emph{CV (constraint violations)}
    %\caption{Constraint Violations}
		%\label{tab:evaluation-constraint-violations}
    %\end{center}
%\end{table}

%The constraints responsible for the largest amounts of constraint violations are \emph{DDI-RDF-C-LABELING-AND-DOCUMENTATION-06} and \emph{DDI-RDF-C-COMPARISON-VARIABLES-02} (both 547,916; DDI-RDF), \emph{DATA-CUBE-C-DATA-MODEL-CONSISTENCY-05} (45,514,102; QB), and \emph{SKOS-C-LANGUAGE-TAG-CARDINALITY-01} (2,508,903; SKOS).

%Table \ref{tab:evaluation-expressivity-severity} shows the relations between the severity level of the violations caused by all 115 constraints and the classification according to the expressivity of constraint languages of the constraint types of the violations in percent. 
80\% of the violations which are raised by either \emph{RDFS/OWL} or \emph{Constraint Language Based} constraints are caused by constraints with the severity level \emph{informational} (see Table \ref{tab:evaluation-expressivity-severity}) and almost all (94\%) of the violations which are caused by \emph{SPARQL Based} constraints are raised by \emph{warning} constraints. Approx. 1/2 of all constraints are \emph{informational} constraints regardless how their types are classified according to the expressivity of constraint languages.

%\begin{table}[H]
		%\scriptsize
    %\begin{center}
		%\caption{OLD}
		%\label{tab:evaluation-complexity-severity}
    %\begin{tabular}{@{}lcccc@{}}
%%           & \multicolumn{6}{c}{\textbf{Vocabularies}}
%%    \\  \cmidrule{2-7}
    %\\       \textbf{}
           %& \textbf{\emph{RDFS/OWL}}
           %& \textbf{\emph{Constraint Language}}
					 %& \textbf{\emph{SPARQL}}
    %\\ \midrule
		%%\emph{Triples} & 9,673,055 & 3,775,983,610 & 477,737,281 & 4,263,393,946 \\
		%%\emph{Data Sets} & 1,526 & 9,990 & 4,178 & 15,694 \\
		%%\hline
		%\textbf{\emph{info}} & 38.7 \% & \textbf{76.2 \%} & \textbf{42.3 \%} \\
		%\textbf{\emph{warning}} & 6.7 \% & 7.1 \% & 13.5 \% \\
		%\textbf{\emph{error}} & \textbf{54.6 \%} & 16.7 \% & \textbf{44.2 \%} \\
    %\bottomrule
    %\end{tabular}
    %%\\ \emph{C (constraints), CT (constraint types)}
    %\end{center}
%\end{table}

%\begin{table}[H]
		%\scriptsize
    %\begin{center}
		%\caption{Constraint Language Expressivity and Severity Level of Violations}
		%\label{tab:evaluation-violations-expressivity-severity}
    %\begin{tabular}{@{}lcccc@{}}
%%           & \multicolumn{6}{c}{\textbf{Vocabularies}}
%%    \\  \cmidrule{2-7}
    %\\       \textbf{}
           %& \textbf{\emph{RDFS/OWL}}
           %& \textbf{\emph{Constraint Language}}
					 %& \textbf{\emph{SPARQL}}
    %\\ \midrule
		%%\emph{Triples} & 9,673,055 & 3,775,983,610 & 477,737,281 & 4,263,393,946 \\
		%%\emph{Data Sets} & 1,526 & 9,990 & 4,178 & 15,694 \\
		%%\hline
		%\textbf{\emph{info}} & \textbf{79.64} & \textbf{79.60} & 4.39 \\
		%\textbf{\emph{warning}} & 20.28 & 20.27 & \textbf{94.17} \\
		%\textbf{\emph{error}} & 0.08 & 0.13 & 1.43 \\
    %\bottomrule
    %\end{tabular}
    %%\\ \emph{C (constraints), CT (constraint types)}
    %\end{center}
%\end{table}

%\begin{table}[H]
		%\scriptsize
    %\begin{center}
		%\caption{Constraint Language Expressivity and Severity Level of Constraints}
		%\label{tab:evaluation-constraints-expressivity-severity}
    %\begin{tabular}{@{}lcccc@{}}
%%           & \multicolumn{6}{c}{\textbf{Vocabularies}}
%%    \\  \cmidrule{2-7}
    %\\       \textbf{}
           %& \textbf{\emph{RDFS/OWL}}
           %& \textbf{\emph{Constraint Language}}
					 %& \textbf{\emph{SPARQL}}
    %\\ \midrule
		%%\emph{Triples} & 9,673,055 & 3,775,983,610 & 477,737,281 & 4,263,393,946 \\
		%%\emph{Data Sets} & 1,526 & 9,990 & 4,178 & 15,694 \\
		%%\hline
		%\textbf{\emph{info}} & \textbf{79.64} & \textbf{79.60} & 4.39 \\
		%\textbf{\emph{warning}} & 20.28 & 20.27 & \textbf{94.17} \\
		%\textbf{\emph{error}} & 0.08 & 0.13 & 1.43 \\
    %\bottomrule
    %\end{tabular}
    %%\\ \emph{C (constraints), CT (constraint types)}
    %\end{center}
%\end{table}

\begin{table}[H]
		\scriptsize
    \begin{center}
		\caption{Language Expressivity and Severity Level}
		\label{tab:evaluation-expressivity-severity}
    \begin{tabular}{@{}lcc|cc|cc@{}}
    %       & \multicolumn{2}{c}{\textbf{Vocabularies}}
    %\\  \cmidrule{2-3}
		%\hline
    \multirow{3}{*}{} &
      \multicolumn{2}{c}{\textbf{RDFS/OWL}} &
      \multicolumn{2}{c}{\textbf{\emph{CL}}} &
      \multicolumn{2}{c}{\textbf{\emph{SPARQL}}} \\
    \textbf{} & C & CV & C & CV & C & CV \\
    \hline
		\textbf{\emph{info}} & \textbf{52.5} & \textbf{79.64} & \textbf{55.2} & \textbf{79.60} & \textbf{45.1} & 4.39 \\
		\textbf{\emph{warning}} & 18.0 & 20.28 & 15.5 & 20.27 & 19.6 & \textbf{94.17} \\
		\textbf{\emph{error}} & 29.5 & 0.08 & 29.3 & 0.13 & 35.3 & 1.43 \\
    \bottomrule
    \end{tabular}
		\\ \emph{C (constraints), CV (constraint violations)}
    \end{center}
\end{table}

\begin{hyp}
Whatever language is used to formulate constraints, 1/2 of all constraints are informational, 1/3 are error, and 1/5 are warning constraints. 
%Thereby, it does not matter which language is used to formulate constraints.
\end{hyp}

The fact that regardless of the language 1/2 of all constraints are \emph{informational} 
%indicates that the purpose of constraints for users is mostly just to point to desirable but not necessary data improvements in terms of syntax and semantics of vocabularies. 
indicates the importance that constraint languages support
constraints on several levels. Constraints are by far not only to
prevent certain usages of a vocabulary, they are rather needed to
provide better guidance for improved interoperability.

\begin{hyp}
Regardless of the type of the used language, there are only a few violations raised by \emph{error} constraints which stands for good data quality in general.
In contrast, constraints of low severity, expressed by RDFS/OWL or high-level constraint languages, are violated to a large extent (80\%), whereas more serious constraints, expressed by SPARQL, are violated to an even larger extend (94\%).
%Violations caused by constraints expressed by RDFS/OWL or high-level constraint languages are of low severity (as the important constraints are satisfied), whereas the violation of constraints formulated by SPARQL is more serious. There is a significant demand for languages that support the expression of SPARQL Based constraints.
\end{hyp}

%The reason why there is a significant demand for languages supporting \emph{SPARQL Based} constraints is that 94\% of all violations, which are caused by \emph{SPARQL Based} constraints, are also raised by \emph{warning} constraints. This means that data quality may be improved in case these \emph{warning} constraints are adequately tackled which is more likely when these constraints are expressible not only by SPARQL but also by high-level constraint languages enabling to formulate constraints more intuitively and concisely. 

94\% of the violations of SPARQL-based constraints are warnings. This
means that data quality could be improved significantly, if these
constraints are met. This is more likely when these constraints
are expressible not only by SPARQL but also by high-level
constraint languages enabling to formulate constraints more
intuitively and concisely.

%SPARQL seems to be used to formulate constraints whose violation is of higher severity than the violation of constraints expressed by either RDFS/OWL or high-level constraint languages. This is a surprising finding as the violation of \emph{RDFS/OWL Based} constraints should be of higher severity than \emph{informational} constraints since \emph{RDFS/OWL Based} constraints are expected to be basic constraints as they are in many cases part of vocabularies' formal specifications. 

%\textcolor{red}{
%das letzte finding und die erläuterung finde ich konfus. violations gibts doch bei rdfs/owl hauptsächlich informational, weil die% wirklich wichtigen eingehalten werden. sparql based kann man ja bis jetzt nicht anders formulieren, insofern ist zu erwarten, dass da% auch warning constraints weniger eingehalten werden, wenn man welche auf warning level formuliert. Ist das nicht eh ein Artefakt %basierend auf letztlich 1 oder 2 constraints die ihr formuliert habt?
%}

%and (2)
%the purpose of high-level constraint languages should be to enable to define constraints of higher severity than \emph{informational}.


\section{Conclusion and Future Work}

%In this paper, we showed in form of a complete real world running example how to represent metadata on unit-record data (DDI-RDF), metadata on aggregated data (QB), and data on both aggregation levels in a rectangular format (\emph{PHDD}) in RDF and how therefore used vocabularies are interrelated (\textbf{contribution 1}, section \ref{rdf-representation}).
%We explained why RDF validation is important in this context and how metadata on unit-record data, aggregated data, thesauri, and statistical classifications as well as data on both aggregation levels is validated against constraints to ensure high (meta)data quality\footnote{The first appendix of this paper describing each constraint in detail is available at: \url{http://arxiv.org/abs/1504.04479} \cite{BoschZapilkoWackerowEckert2015}} (\textbf{contribution 2}, section \ref{rdf-validation}). 
%We distinguish two validation types:
%(1) \emph{Content-Driven Validation} $\mathcal{C}_{C}$ contains the set of constraints ensuring that the data is consistent with the intended syntax, semantics, and integrity of data models (section \ref{SPARQL-based-constraint-types}).
%(2) \emph{Technology-Driven Validation} $\mathcal{C}_{T}$ includes the set of constraints which can be generated automatically out of data models, such as cardinality restrictions, universal and existential quantifications, domains, and ranges (section \ref{vocabulary-constraint-types}).
%We determined the default \emph{severity level} for each constraint to indicate how serious the violation of the constraint is
%and propose an extensible metric to measure the continuum of severity levels.

%We implemented a validation environment (available at \url{http://purl.org/net/rdfval-demo}) to validate RDF data according to constraints expressed my arbitrary constraint languages and to ensure correct syntax and semantics of diverse vocabularies such as DDI-RDF, QB, SKOS, and \emph{PHDD} (section \ref{implementation}).
%We exhaustively evaluated the metadata quality of large real world aggregated (QB), unit-record (DDI-RDF), and thesauri (SKOS) data sets (more than 4.2 billion triples and 15 thousand data sets) by means of 213  constraints of the majority of the constraint types\footnote{The second appendix of this paper describing the evaluation in detail is available at: \url{http://arxiv.org/abs/1504.04478} \cite{BoschZapilkoWackerowEckert2015-2}.} (section \ref{evaluation}).

We published by today 81 constraint types that are required by various stakeholders for data applications.
In close collaboration with several domain experts for the social, behavioral, and economic (SBE) sciences, we formulated 115 constraints on three different vocabularies (DDI-RDF, QB, SKOS) and classified them according to their severity level and  whether their type is expressible by different types of constraint languages - RDFS/OWL, high-level constraint languages, and SPARQL. 
Using these constraints, we evaluated  the data quality of 15,694 data sets (4.26 billion triples) of research data for the SBE sciences obtained from 33 SPARQL endpoints.

Based on the evaluation results, we formulated several findings to direct the further development of constraint languages. The general applicability of these findings, however, is still to be confirmed beyond the examined vocabularies and for other domains.
The main findings are: 

\begin{enumerate}
\item
\emph{Data quality can be significantly improved when suitable constraint languages are developed enabling to define 
constraints, which up to now can only be expressed by plain SPARQL, in an easy, concise, and intuitive way. Thereby, the more elaborate a vocabulary is, the more sophisticated and complex constraints are necessary which can up to now only be specified by SPARQL.}
\item
\emph{As only 1/5 of all violations result from RDFS/OWL Based constraints, even though 1/3 of all constraints are RDFS/OWL Based, data quality is high for all vocabularies with regard to their formal specifications.}  
\item
\emph{Although 40\% of all constraints are error constraints, the percentage of severe violations is very low, compared to about 2/3 of warning and 1/3 of informational violations. This implies that data quality is high with regard to the severity level of constraints and that proper constraint languages can significantly improve data quality beyond fundamental requirements.}
\item
\emph{Whatever language is used to formulate constraints, 1/2 of all constraints are informational, 1/3 are error, and 1/5 are warning constraints. 
Violations caused by constraints expressed by RDFS/OWL or high-level constraint languages are of low severity, whereas the violation of constraints formulated by SPARQL is more serious.
There is a significant demand for languages that support the expression of SPARQL Based constraints causing 94\% of all violations.}
\end{enumerate}

%\begin{enumerate}
	%\item The percentage of severe violations is very low which implies that proper constraint languages can significantly improve the data quality beyond fundamental requirements.
  %\item The further development of constraint languages should concentrate on how to express \emph{complex constraints} as 44\% of all \emph{complex constraints} are also serious ones.
  %\item 47\% of the violations refer to complex constraints that are in most cases not expressible by existing constraint languages which confirms the necessity to provide suitable constraint languages.
  %\item As 54\% of the constraints are either \emph{simple} or \emph{complex constraints},
%the further development of constraint languages should concentrate on expressing \emph{simple} and especially \emph{complex constraints} which up to now in most cases can only be expressed by plain SPARQL. 
%\end{enumerate}

We have been really impressed by the high quality of the QB and SKOS data. This is in contrast to the sometimes heard rumor that Linked Open Data lacks quality. We are actively involved in the further development and implementation of constraint languages and will use the results presented in the paper to set priorities on features where we expect the highest impact on the data quality of real-life data in the SBE domain.
As the use of constraint languages per se enhances data quality, it must be continued working intensively on their further development.

%\ke{in die conclusion sollte noch rein, dass constraint languages per se funktionieren zur erhöhung der datenqualität und deswegen mit hochdruck an der weiterentwicklung gearbeitet werden muss.}
% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation Results}
%\label{fig_sim}
%\end{figure}

% Note that IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command, the
% \label for the overall figure must come after \caption.
% \hfil must be used as a separator to get equal spacing.
% The subfigure.sty package works much the same way, except \subfigure is
% used instead of \subfloat.
%
%\begin{figure*}[!t]
%\centerline{\subfloat[Case I]\includegraphics[width=2.5in]{subfigcase1}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{subfigcase2}%
%\label{fig_second_case}}}
%\caption{Simulation results}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.


% An example of a floating table. Note that, for IEEE style tables, the 
% \caption command should come BEFORE the table. Table text will default to
% \footnotesize as IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that IEEE does not put floats in the very first column - or typically
% anywhere on the first page for that matter. Also, in-text middle ("here")
% positioning is not used. Most IEEE journals/conferences use top floats
% exclusively. Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the \fnbelowfloat
% command of the stfloats package.







% conference papers do not normally have an appendix


% use section* for acknowledgement
%\section*{Acknowledgment}







% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
\bibliography{../../literature/literature}{}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
%\begin{thebibliography}{1}
%
%\bibitem{IEEEhowto:kopka}
%H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
  %0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.
%
%\end{thebibliography}



% that's all folks
\end{document}


